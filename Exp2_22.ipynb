{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exp1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHgLZpLpo3Wj"
      },
      "source": [
        "# from google.colab import drive\r\n",
        "# drive.mount('/content/drive/')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP4BT5QEo7Ot",
        "outputId": "d81ae44a-9256-4c69-f05c-c6d234063372"
      },
      "source": [
        "%cd /content/drive/MyDrive/topcoder"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/topcoder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pglu-X_4paow"
      },
      "source": [
        "# Новый раздел"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jp6My88p8on"
      },
      "source": [
        "import pickle\r\n",
        "import yaml\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "\r\n",
        "import os\r\n",
        "import random\r\n",
        "import time\r\n",
        "\r\n",
        "import torch\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from torch import nn\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch.utils.data import Dataset\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.optim.lr_scheduler import _LRScheduler"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dbn0AJYuq_2Q"
      },
      "source": [
        "__all__ = ['mobilenetv3_large', 'mobilenetv3_small']\r\n",
        "\r\n",
        "\r\n",
        "def _make_divisible(v, divisor, min_value=None):\r\n",
        "    \"\"\"\r\n",
        "    This function is taken from the original tf repo.\r\n",
        "    It ensures that all layers have a channel number that is divisible by 8\r\n",
        "    It can be seen here:\r\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\r\n",
        "    :param v:\r\n",
        "    :param divisor:\r\n",
        "    :param min_value:\r\n",
        "    :return:\r\n",
        "    \"\"\"\r\n",
        "    if min_value is None:\r\n",
        "        min_value = divisor\r\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\r\n",
        "    # Make sure that round down does not go down by more than 10%.\r\n",
        "    if new_v < 0.9 * v:\r\n",
        "        new_v += divisor\r\n",
        "    return new_v\r\n",
        "\r\n",
        "\r\n",
        "class h_sigmoid(nn.Module):\r\n",
        "    def __init__(self, inplace=True):\r\n",
        "        super(h_sigmoid, self).__init__()\r\n",
        "        self.relu = nn.ReLU6(inplace=inplace)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return self.relu(x + 3) / 6\r\n",
        "\r\n",
        "\r\n",
        "class h_swish(nn.Module):\r\n",
        "    def __init__(self, inplace=True):\r\n",
        "        super(h_swish, self).__init__()\r\n",
        "        self.sigmoid = h_sigmoid(inplace=inplace)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return x * self.sigmoid(x)\r\n",
        "\r\n",
        "\r\n",
        "class SELayer(nn.Module):\r\n",
        "    def __init__(self, channel, reduction=4):\r\n",
        "        super(SELayer, self).__init__()\r\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\r\n",
        "        self.fc = nn.Sequential(\r\n",
        "                nn.Linear(channel, _make_divisible(channel // reduction, 8)),\r\n",
        "                nn.ReLU(inplace=True),\r\n",
        "                nn.Linear(_make_divisible(channel // reduction, 8), channel),\r\n",
        "                h_sigmoid()\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        b, c, _, _ = x.size()\r\n",
        "        y = self.avg_pool(x).view(b, c)\r\n",
        "        y = self.fc(y).view(b, c, 1, 1)\r\n",
        "        return x * y\r\n",
        "\r\n",
        "\r\n",
        "def conv_3x3_bn(inp, oup, stride):\r\n",
        "    return nn.Sequential(\r\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\r\n",
        "        nn.BatchNorm2d(oup),\r\n",
        "        h_swish()\r\n",
        "    )\r\n",
        "\r\n",
        "\r\n",
        "def conv_1x1_bn(inp, oup):\r\n",
        "    return nn.Sequential(\r\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\r\n",
        "        nn.BatchNorm2d(oup),\r\n",
        "        h_swish()\r\n",
        "    )\r\n",
        "\r\n",
        "\r\n",
        "class InvertedResidual(nn.Module):\r\n",
        "    def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se, use_hs):\r\n",
        "        super(InvertedResidual, self).__init__()\r\n",
        "        assert stride in [1, 2]\r\n",
        "\r\n",
        "        self.identity = stride == 1 and inp == oup\r\n",
        "\r\n",
        "        if inp == hidden_dim:\r\n",
        "            self.conv = nn.Sequential(\r\n",
        "                # dw\r\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\r\n",
        "                nn.BatchNorm2d(hidden_dim),\r\n",
        "                h_swish() if use_hs else nn.ReLU(inplace=True),\r\n",
        "                # Squeeze-and-Excite\r\n",
        "                SELayer(hidden_dim) if use_se else nn.Identity(),\r\n",
        "                # pw-linear\r\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\r\n",
        "                nn.BatchNorm2d(oup),\r\n",
        "            )\r\n",
        "        else:\r\n",
        "            self.conv = nn.Sequential(\r\n",
        "                # pw\r\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\r\n",
        "                nn.BatchNorm2d(hidden_dim),\r\n",
        "                h_swish() if use_hs else nn.ReLU(inplace=True),\r\n",
        "                # dw\r\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\r\n",
        "                nn.BatchNorm2d(hidden_dim),\r\n",
        "                # Squeeze-and-Excite\r\n",
        "                SELayer(hidden_dim) if use_se else nn.Identity(),\r\n",
        "                h_swish() if use_hs else nn.ReLU(inplace=True),\r\n",
        "                # pw-linear\r\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\r\n",
        "                nn.BatchNorm2d(oup),\r\n",
        "            )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        if self.identity:\r\n",
        "            return x + self.conv(x)\r\n",
        "        else:\r\n",
        "            return self.conv(x)\r\n",
        "\r\n",
        "\r\n",
        "class MobileNetV3(nn.Module):\r\n",
        "    def __init__(self, cfgs, mode, num_classes=1000, width_mult=1.):\r\n",
        "        super(MobileNetV3, self).__init__()\r\n",
        "        # setting of inverted residual blocks\r\n",
        "        self.cfgs = cfgs\r\n",
        "        assert mode in ['large', 'small']\r\n",
        "\r\n",
        "        # building first layer\r\n",
        "        input_channel = _make_divisible(16 * width_mult, 8)\r\n",
        "        layers = [conv_3x3_bn(3, input_channel, 2)]\r\n",
        "        # building inverted residual blocks\r\n",
        "        block = InvertedResidual\r\n",
        "        for k, t, c, use_se, use_hs, s in self.cfgs:\r\n",
        "            output_channel = _make_divisible(c * width_mult, 8)\r\n",
        "            exp_size = _make_divisible(input_channel * t, 8)\r\n",
        "            layers.append(block(input_channel, exp_size, output_channel, k, s, use_se, use_hs))\r\n",
        "            input_channel = output_channel\r\n",
        "        self.features = nn.Sequential(*layers)\r\n",
        "        # building last several layers\r\n",
        "        self.conv = conv_1x1_bn(input_channel, exp_size)\r\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\r\n",
        "        output_channel = {'large': 1280, 'small': 1024}\r\n",
        "        output_channel = _make_divisible(output_channel[mode] * width_mult, 8) if width_mult > 1.0 else output_channel[mode]\r\n",
        "        self.classifier = nn.Sequential(\r\n",
        "            nn.Linear(exp_size, output_channel),\r\n",
        "            h_swish(),\r\n",
        "            nn.Dropout(0.2),\r\n",
        "            nn.Linear(output_channel, num_classes),\r\n",
        "        )\r\n",
        "\r\n",
        "        self._initialize_weights()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.features(x)\r\n",
        "        x = self.conv(x)\r\n",
        "        x = self.avgpool(x)\r\n",
        "        x = x.view(x.size(0), -1)\r\n",
        "        x = self.classifier(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "    def _initialize_weights(self):\r\n",
        "        for m in self.modules():\r\n",
        "            if isinstance(m, nn.Conv2d):\r\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\r\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\r\n",
        "                if m.bias is not None:\r\n",
        "                    m.bias.data.zero_()\r\n",
        "            elif isinstance(m, nn.BatchNorm2d):\r\n",
        "                m.weight.data.fill_(1)\r\n",
        "                m.bias.data.zero_()\r\n",
        "            elif isinstance(m, nn.Linear):\r\n",
        "                n = m.weight.size(1)\r\n",
        "                m.weight.data.normal_(0, 0.01)\r\n",
        "                m.bias.data.zero_()\r\n",
        "\r\n",
        "\r\n",
        "def mobilenetv3_large(**kwargs):\r\n",
        "    \"\"\"\r\n",
        "    Constructs a MobileNetV3-Large model\r\n",
        "    \"\"\"\r\n",
        "    cfgs = [\r\n",
        "        # k, t, c, SE, HS, s \r\n",
        "        [3,   1,  16, 0, 0, 1],\r\n",
        "        [3,   4,  24, 0, 0, 2],\r\n",
        "        [3,   3,  24, 0, 0, 1],\r\n",
        "        [5,   3,  40, 1, 0, 2],\r\n",
        "        [5,   3,  40, 1, 0, 1],\r\n",
        "        [5,   3,  40, 1, 0, 1],\r\n",
        "        [3,   6,  80, 0, 1, 2],\r\n",
        "        [3, 2.5,  80, 0, 1, 1],\r\n",
        "        [3, 2.3,  80, 0, 1, 1],\r\n",
        "        [3, 2.3,  80, 0, 1, 1],\r\n",
        "        [3,   6, 112, 1, 1, 1],\r\n",
        "        [3,   6, 112, 1, 1, 1],\r\n",
        "        [5,   6, 160, 1, 1, 2],\r\n",
        "        [5,   6, 160, 1, 1, 1],\r\n",
        "        [5,   6, 160, 1, 1, 1]\r\n",
        "    ]\r\n",
        "    return MobileNetV3(cfgs, mode='large', **kwargs)\r\n",
        "\r\n",
        "\r\n",
        "def mobilenetv3_small(**kwargs):\r\n",
        "    \"\"\"\r\n",
        "    Constructs a MobileNetV3-Small model\r\n",
        "    \"\"\"\r\n",
        "    cfgs = [\r\n",
        "        # k, t, c, SE, HS, s \r\n",
        "        [3,    1,  16, 1, 0, 2],\r\n",
        "        [3,  4.5,  24, 0, 0, 2],\r\n",
        "        [3, 3.67,  24, 0, 0, 1],\r\n",
        "        [5,    4,  40, 1, 1, 2],\r\n",
        "        [5,    6,  40, 1, 1, 1],\r\n",
        "        [5,    6,  40, 1, 1, 1],\r\n",
        "        [5,    3,  48, 1, 1, 1],\r\n",
        "        [5,    3,  48, 1, 1, 1],\r\n",
        "        [5,    6,  96, 1, 1, 2],\r\n",
        "        [5,    6,  96, 1, 1, 1],\r\n",
        "        [5,    6,  96, 1, 1, 1],\r\n",
        "    ]\r\n",
        "\r\n",
        "    return MobileNetV3(cfgs, mode='small', **kwargs)\r\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13rtbK2h8sCE"
      },
      "source": [
        "class CosineAnnealingWarmupRestarts(_LRScheduler):\r\n",
        "    \"\"\"\r\n",
        "        optimizer (Optimizer): Wrapped optimizer.\r\n",
        "        first_cycle_steps (int): First cycle step size.\r\n",
        "        cycle_mult(float): Cycle steps magnification. Default: -1.\r\n",
        "        max_lr(float): First cycle's max learning rate. Default: 0.1.\r\n",
        "        min_lr(float): Min learning rate. Default: 0.001.\r\n",
        "        warmup_steps(int): Linear warmup step size. Default: 0.\r\n",
        "        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\r\n",
        "        last_epoch (int): The index of last epoch. Default: -1.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    def __init__(self,\r\n",
        "                 optimizer : torch.optim.Optimizer,\r\n",
        "                 first_cycle_steps : int,\r\n",
        "                 cycle_mult : float = 1.,\r\n",
        "                 max_lr : float = 0.1,\r\n",
        "                 min_lr : float = 0.001,\r\n",
        "                 warmup_steps : int = 0,\r\n",
        "                 gamma : float = 1.,\r\n",
        "                 last_epoch : int = -1\r\n",
        "        ):\r\n",
        "        assert warmup_steps < first_cycle_steps\r\n",
        "        \r\n",
        "        self.first_cycle_steps = first_cycle_steps # first cycle step size\r\n",
        "        self.cycle_mult = cycle_mult # cycle steps magnification\r\n",
        "        self.base_max_lr = max_lr # first max learning rate\r\n",
        "        self.max_lr = max_lr # max learning rate in the current cycle\r\n",
        "        self.min_lr = min_lr # min learning rate\r\n",
        "        self.warmup_steps = warmup_steps # warmup step size\r\n",
        "        self.gamma = gamma # decrease rate of max learning rate by cycle\r\n",
        "        \r\n",
        "        self.cur_cycle_steps = first_cycle_steps # first cycle step size\r\n",
        "        self.cycle = 0 # cycle count\r\n",
        "        self.step_in_cycle = last_epoch # step size of the current cycle\r\n",
        "        \r\n",
        "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\r\n",
        "        \r\n",
        "        # set learning rate min_lr\r\n",
        "        self.init_lr()\r\n",
        "    \r\n",
        "    def init_lr(self):\r\n",
        "        self.base_lrs = []\r\n",
        "        for param_group in self.optimizer.param_groups:\r\n",
        "            param_group['lr'] = self.min_lr\r\n",
        "            self.base_lrs.append(self.min_lr)\r\n",
        "    \r\n",
        "    def get_lr(self):\r\n",
        "        if self.step_in_cycle == -1:\r\n",
        "            return self.base_lrs\r\n",
        "        elif self.step_in_cycle < self.warmup_steps:\r\n",
        "            return [(self.max_lr - base_lr)*self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\r\n",
        "        else:\r\n",
        "            return [base_lr + (self.max_lr - base_lr) \\\r\n",
        "                    * (1 + math.cos(math.pi * (self.step_in_cycle-self.warmup_steps) \\\r\n",
        "                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\r\n",
        "                    for base_lr in self.base_lrs]\r\n",
        "\r\n",
        "    def step(self, epoch=None):\r\n",
        "        if epoch is None:\r\n",
        "            epoch = self.last_epoch + 1\r\n",
        "            self.step_in_cycle = self.step_in_cycle + 1\r\n",
        "            if self.step_in_cycle >= self.cur_cycle_steps:\r\n",
        "                self.cycle += 1\r\n",
        "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\r\n",
        "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\r\n",
        "        else:\r\n",
        "            if epoch >= self.first_cycle_steps:\r\n",
        "                if self.cycle_mult == 1.:\r\n",
        "                    self.step_in_cycle = epoch % self.first_cycle_steps\r\n",
        "                    self.cycle = epoch // self.first_cycle_steps\r\n",
        "                else:\r\n",
        "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\r\n",
        "                    self.cycle = n\r\n",
        "                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\r\n",
        "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\r\n",
        "            else:\r\n",
        "                self.cur_cycle_steps = self.first_cycle_steps\r\n",
        "                self.step_in_cycle = epoch\r\n",
        "                \r\n",
        "        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\r\n",
        "        self.last_epoch = math.floor(epoch)\r\n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\r\n",
        "            param_group['lr'] = lr"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4ElywIvqA6K"
      },
      "source": [
        "CONFIG_PATH = \"proj_config.yaml\"\r\n",
        "with open(CONFIG_PATH, 'r') as stream:\r\n",
        "    CONFIG = yaml.safe_load(stream)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP7AkAj8MAsZ"
      },
      "source": [
        "def spec_augment(spec: np.ndarray, num_mask=2, freq_masking_max_percentage=0.05, time_masking_max_percentage=0.1):\r\n",
        "    spec = spec.copy()\r\n",
        "    for i in range(num_mask):\r\n",
        "        num_freqs, num_frames = spec.shape\r\n",
        "        freq_percentage = random.uniform(0.0, freq_masking_max_percentage)\r\n",
        "        time_percentage = random.uniform(0.0, time_masking_max_percentage)\r\n",
        "        \r\n",
        "        num_freqs_to_mask = int(freq_percentage * num_freqs)\r\n",
        "        num_frames_to_mask = int(time_percentage * num_frames)\r\n",
        "        \r\n",
        "        t0 = int(np.random.uniform(low=0.0, high=num_frames - num_frames_to_mask))\r\n",
        "        f0 = int(np.random.uniform(low=0.0, high=num_freqs - num_freqs_to_mask))\r\n",
        "        \r\n",
        "        spec[:, t0:t0 + num_frames_to_mask] = 0      \r\n",
        "        spec[f0:f0 + num_freqs_to_mask, :] = 0 \r\n",
        "        \r\n",
        "    return spec"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G43KN85lp4D0"
      },
      "source": [
        "def uniform_len(mel, input_len):\r\n",
        "    mel_len = mel.shape[-1]\r\n",
        "    if mel_len > input_len:\r\n",
        "        diff = mel_len - input_len\r\n",
        "        start = np.random.randint(diff)\r\n",
        "        end = start + input_len\r\n",
        "        mel = mel[:, start: end]\r\n",
        "    elif mel_len < input_len:\r\n",
        "        diff = input_len - mel_len\r\n",
        "        offset = np.random.randint(diff)\r\n",
        "        offset_right = diff - offset\r\n",
        "        mel = np.pad(\r\n",
        "            mel,\r\n",
        "            ((0, 0), (offset, offset_right)),\r\n",
        "            \"symmetric\",  # constant\r\n",
        "        )\r\n",
        "    return mel\r\n",
        "\r\n",
        "\r\n",
        "class TorqueDataset(Dataset):\r\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, data, mel_logs, labels=None, transform=None):\r\n",
        "        \"\"\"Init Dataset\"\"\"\r\n",
        "        self.mel_logs = mel_logs\r\n",
        "        self.data = data\r\n",
        "        self.labels = labels\r\n",
        "        self.transform = transform\r\n",
        "        self.input_len = CONFIG['mel']['mel_len']\r\n",
        "        self.mode = 'test' if self.labels is None else 'train'\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        \"\"\"Length\"\"\"\r\n",
        "        return len(self.mel_logs)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        \"\"\"Generates one sample of data\"\"\"\r\n",
        "        table_data = self.data[index]\r\n",
        "\r\n",
        "        label = None\r\n",
        "        if self.mode == 'train':\r\n",
        "            label = self.labels[[index]]\r\n",
        "\r\n",
        "        mel_data = uniform_len(self.mel_logs[index], self.input_len)\r\n",
        "        if self.transform and self.mode == 'train':\r\n",
        "            mel_data = self.transform(mel_data)\r\n",
        "\r\n",
        "        mel_data = np.expand_dims(mel_data, axis=0)\r\n",
        "        return mel_data, table_data, label"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdOpMc7Rp-AZ"
      },
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "torch.backends.cudnn.benchmark= True\r\n",
        "\r\n",
        "def seed_everything(seed=1234):\r\n",
        "    \"\"\"Fix random seeds\"\"\"\r\n",
        "    random.seed(seed)\r\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\r\n",
        "    np.random.seed(seed)\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    torch.cuda.manual_seed(seed)\r\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vJdztTNqiv0"
      },
      "source": [
        "def get_mobilenet_model(out_features, pretrained_mn3_path=\"\", pretrained_path=\"\"):\r\n",
        "    \"\"\"Load MobilenetV3 model with specified in and out channels\"\"\"\r\n",
        "    # model = mobilenetv3_small().to(DEVICE)\r\n",
        "    model = mobilenetv3_large() #.to(DEVICE)\r\n",
        "    if pretrained_mn3_path and not pretrained_path:\r\n",
        "        model.load_state_dict(torch.load(pretrained_mn3_path))\r\n",
        "\r\n",
        "    model.features[0][0].weight.data = torch.sum(\r\n",
        "        model.features[0][0].weight.data, dim=1, keepdim=True\r\n",
        "    )\r\n",
        "    model.features[0][0].in_channels = 1\r\n",
        "\r\n",
        "    if pretrained_path:\r\n",
        "        model.load_state_dict(torch.load(pretrained_path))\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "class TorqueModel(nn.Module):\r\n",
        "    def __init__(self, out_features_conv, out_features_dence, mid_features, pretrained_mn3_path=\"\", pretrained_path=\"\"):\r\n",
        "        super(TorqueModel, self).__init__()\r\n",
        "        self.mnet = get_mobilenet_model(out_features_conv, pretrained_mn3_path, pretrained_path)\r\n",
        "        self.fc1 = nn.Linear(out_features_conv + out_features_dence, mid_features)\r\n",
        "        self.fc2 = nn.Linear(mid_features, mid_features)\r\n",
        "        self.fc3 = nn.Linear(mid_features, 1)\r\n",
        "\r\n",
        "    def forward(self, image, data):\r\n",
        "        x1 = self.mnet(image)\r\n",
        "        x2 = data\r\n",
        "        x = torch.cat((x1, x2), dim=1)\r\n",
        "        x = F.relu(self.fc1(x))\r\n",
        "        x = F.relu(self.fc2(x))\r\n",
        "        x = self.fc3(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "def process_epoch(model, criterion, optimizer, loader):\r\n",
        "    \"\"\"Calc one epoch\"\"\"\r\n",
        "    losses = []\r\n",
        "    y_true = []\r\n",
        "    y_pred = []\r\n",
        "    with torch.set_grad_enabled(model.training):\r\n",
        "        for local_batch, local_data, local_labels in loader:\r\n",
        "            local_batch, local_data, local_labels = \\\r\n",
        "                local_batch.to(DEVICE), local_data.to(DEVICE), local_labels.to(DEVICE)\r\n",
        "\r\n",
        "            # optimizer.zero_grad()\r\n",
        "            for param in model.parameters():\r\n",
        "                param.grad = None\r\n",
        "            outputs = model(local_batch, local_data)\r\n",
        "\r\n",
        "            loss = criterion(outputs, local_labels)\r\n",
        "            if model.training:\r\n",
        "                loss.backward()\r\n",
        "                optimizer.step()\r\n",
        "\r\n",
        "            losses.append(loss)\r\n",
        "            y_true.append(local_labels.detach().cpu().numpy())\r\n",
        "            y_pred.append(outputs.data.detach().cpu().numpy())\r\n",
        "    loss_train = np.array(losses).astype(np.float32).mean()\r\n",
        "    y_true = np.concatenate(y_true)\r\n",
        "    y_pred = np.concatenate(y_pred)\r\n",
        "    rmse_train = mean_squared_error(y_true, y_pred, squared=False)\r\n",
        "    return loss_train, rmse_train, y_true, y_pred"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT95qJpQqlu9"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, train_loader, test_loader, n_fold, n_freeze=10):\r\n",
        "    \"\"\"Training loop\"\"\"\r\n",
        "    logs = {'loss_train': [], 'loss_val': [], 'mse_train': [], 'mse_val': []}\r\n",
        "    best_true = None\r\n",
        "    best_pred = None\r\n",
        "    for epoch in range(CONFIG['num_epochs']):\r\n",
        "        start_time = time.time()\r\n",
        "\r\n",
        "        # More accurate work with new layers\r\n",
        "        if epoch == 0:\r\n",
        "            for i, child in enumerate(model.children()):\r\n",
        "                if i == 0:\r\n",
        "                    for param in child.parameters():\r\n",
        "                        param.requires_grad = False\r\n",
        "                    for param_group in optimizer.param_groups:\r\n",
        "                        old_lr = param_group['lr']\r\n",
        "                        param_group['lr'] = CONFIG['lr']\r\n",
        "        elif epoch == n_freeze:\r\n",
        "            for i, child in enumerate(model.children()):\r\n",
        "                if i == 0:\r\n",
        "                    for param in child.parameters():\r\n",
        "                        param.requires_grad = True\r\n",
        "                    for param_group in optimizer.param_groups:\r\n",
        "                        param_group['lr'] = old_lr\r\n",
        "        if epoch >= n_freeze:\r\n",
        "            scheduler.step()\r\n",
        "\r\n",
        "        # Training\r\n",
        "        model.train()\r\n",
        "        loss_train, mse_train, _, _ = \\\r\n",
        "            process_epoch(model, criterion, optimizer, train_loader)\r\n",
        "        logs['loss_train'].append(loss_train)\r\n",
        "        logs['mse_train'].append(mse_train)\r\n",
        "\r\n",
        "        # Validation\r\n",
        "        model.eval()\r\n",
        "        loss_val, mse_val, y_true, y_pred = \\\r\n",
        "            process_epoch(model, criterion, optimizer, test_loader)\r\n",
        "        logs['loss_val'].append(loss_val)\r\n",
        "        logs['mse_val'].append(mse_val)\r\n",
        "        print(\r\n",
        "            f\"Epoch #{epoch + 1}. \"\r\n",
        "            f\"Time: {(time.time() - start_time):.1f}s. \"\r\n",
        "            f\"Train loss: {loss_train:.3f}, train mse: {mse_train:.5f}. \"\r\n",
        "            f\"Val loss: {loss_val:.3f}, val mse: {mse_val:.5f}\"\r\n",
        "        )\r\n",
        "        if mse_val <= np.min(logs['mse_val']):\r\n",
        "            if CONFIG['save_model']:\r\n",
        "                torch.save(\r\n",
        "                    model.state_dict(),\r\n",
        "                    os.path.join(\r\n",
        "                        CONFIG['model_dir'],\r\n",
        "                        f\"work_{CONFIG['experiment_name']}_fold{n_fold}.pt\"\r\n",
        "                    )\r\n",
        "                )\r\n",
        "            best_true = y_true\r\n",
        "            best_pred = y_pred\r\n",
        "    return best_true, best_pred\r\n",
        "\r\n",
        "\r\n",
        "def run_training():\r\n",
        "    start_time = time.time()\r\n",
        "\r\n",
        "    with open(CONFIG['data_path'], 'rb') as f:\r\n",
        "        (data, mel_logs, target) = pickle.load(f)\r\n",
        "\r\n",
        "    folds = KFold(\r\n",
        "        n_splits=CONFIG['n_folds'],\r\n",
        "        shuffle=True,\r\n",
        "        random_state=CONFIG['fold_seed']\r\n",
        "    )\r\n",
        "    splits = list(folds.split(mel_logs))\r\n",
        "\r\n",
        "    total_rmse = list()\r\n",
        "\r\n",
        "    for n_fold, (train_idx, val_idx) in enumerate(splits):\r\n",
        "        print(f\"Start #{n_fold + 1} fold\")\r\n",
        "        train_dataset = TorqueDataset(\r\n",
        "            data[train_idx],\r\n",
        "            [mel_logs[i] for i in train_idx],\r\n",
        "            target[train_idx],\r\n",
        "            transform=spec_augment\r\n",
        "        )\r\n",
        "        val_dataset = TorqueDataset(\r\n",
        "            data[val_idx],\r\n",
        "            [mel_logs[i] for i in val_idx],\r\n",
        "            target[val_idx]\r\n",
        "        )\r\n",
        "        train_loader = DataLoader(train_dataset, **CONFIG['loader_params'])\r\n",
        "        val_loader = DataLoader(val_dataset, **CONFIG['loader_params'])\r\n",
        "\r\n",
        "        model = TorqueModel(\r\n",
        "            CONFIG['model_params']['out_features_conv'],\r\n",
        "            CONFIG['model_params']['out_features_dence'],\r\n",
        "            CONFIG['model_params']['mid_features'],\r\n",
        "            CONFIG['pretrained_path']\r\n",
        "        )\r\n",
        "        model = model.to(DEVICE)\r\n",
        "        criterion = nn.MSELoss()\r\n",
        "        optimizer = torch.optim.Adam(model.parameters(), CONFIG['lr'])\r\n",
        "\r\n",
        "        # CONFIG['scheduler_params']['max_lr'] *= CONFIG['lr']\r\n",
        "        # CONFIG['scheduler_params']['min_lr'] *= CONFIG['lr']\r\n",
        "        scheduler = CosineAnnealingWarmupRestarts(optimizer, **CONFIG['scheduler_params'])\r\n",
        "\r\n",
        "        best_true, best_pred = \\\r\n",
        "            train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, n_fold)\r\n",
        "\r\n",
        "        rmse = mean_squared_error(best_true, best_pred, squared=False)\r\n",
        "        print(f\"Training done. Best rmse: {rmse}\")\r\n",
        "        total_rmse.append(rmse)\r\n",
        "    print(f\"Total time: {(time.time() - start_time) / 60}m\")\r\n",
        "    print(f\"Total rmse: {np.mean(total_rmse)} +- {np.std(total_rmse)}\")\r\n",
        "    print(total_rmse)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1aNynqjtHPj"
      },
      "source": [
        "seed_everything()"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiLbhqtFtEMt"
      },
      "source": [
        "CONFIG['loader_params'] = {'batch_size': 16, 'shuffle': True, \r\n",
        "                           'num_workers': 4, 'pin_memory':False}\r\n",
        "CONFIG['lr'] = 0.00005\r\n",
        "\r\n",
        "# CONFIG['slow_epochs']\r\n",
        "CONFIG['num_epochs'] = 50\r\n",
        "\r\n",
        "CONFIG['pretrained_path'] = './pretrained/mobilenetv3-large-1cd25616.pth'\r\n",
        "\r\n",
        "CONFIG['scheduler_params'] = {'first_cycle_steps':20,\r\n",
        "                            'cycle_mult':1.0,\r\n",
        "                            'max_lr':CONFIG['lr'] * 6,\r\n",
        "                            'min_lr':CONFIG['lr'] / 8,\r\n",
        "                            'warmup_steps':5,\r\n",
        "                            'gamma':0.5}\r\n",
        "\r\n",
        "CONFIG['experiment_name'] = 'add_table'"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jM8j3o0mMK7",
        "outputId": "5ae80648-048c-4259-f581-924a7a207033"
      },
      "source": [
        "run_training() \r\n",
        "\r\n",
        "# Total rmse: 21.617450714111328 +- 1.5673015117645264\r\n",
        "# [22.85315, 20.501451, 19.603146, 22.78535, 21.317307, 21.486523, 20.497011, 19.476, 24.399702, 23.254879]"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start #1 fold\n",
            "Epoch #1. Time: 2.7s. Train loss: 6645.824, train mse: 81.48435. Val loss: 6618.590, val mse: 81.29009\n",
            "Epoch #2. Time: 2.7s. Train loss: 5185.472, train mse: 72.10763. Val loss: 4753.931, val mse: 69.14619\n",
            "Epoch #3. Time: 2.7s. Train loss: 2296.314, train mse: 47.99612. Val loss: 2189.055, val mse: 47.31985\n",
            "Epoch #4. Time: 2.7s. Train loss: 1347.742, train mse: 36.73559. Val loss: 1659.536, val mse: 40.44096\n",
            "Epoch #5. Time: 2.7s. Train loss: 1172.975, train mse: 34.26660. Val loss: 1436.203, val mse: 37.75899\n",
            "Epoch #6. Time: 2.8s. Train loss: 1167.098, train mse: 34.13404. Val loss: 1452.121, val mse: 38.09715\n",
            "Epoch #7. Time: 2.7s. Train loss: 1119.812, train mse: 33.51500. Val loss: 1487.449, val mse: 37.92863\n",
            "Epoch #8. Time: 2.7s. Train loss: 1056.181, train mse: 32.43797. Val loss: 1308.952, val mse: 36.16893\n",
            "Epoch #9. Time: 2.7s. Train loss: 1094.150, train mse: 33.01291. Val loss: 1253.416, val mse: 35.58585\n",
            "Epoch #10. Time: 2.7s. Train loss: 1028.914, train mse: 31.99344. Val loss: 1330.975, val mse: 36.25534\n",
            "Epoch #11. Time: 6.6s. Train loss: 1042.955, train mse: 32.36685. Val loss: 912.487, val mse: 30.23689\n",
            "Epoch #12. Time: 6.5s. Train loss: 1006.579, train mse: 31.74898. Val loss: 844.359, val mse: 29.15119\n",
            "Epoch #13. Time: 6.5s. Train loss: 1010.632, train mse: 31.83260. Val loss: 1178.083, val mse: 34.64097\n",
            "Epoch #14. Time: 6.5s. Train loss: 1006.169, train mse: 31.73515. Val loss: 870.783, val mse: 29.25704\n",
            "Epoch #15. Time: 6.5s. Train loss: 940.783, train mse: 30.62256. Val loss: 967.327, val mse: 30.96652\n",
            "Epoch #16. Time: 6.5s. Train loss: 851.922, train mse: 29.25055. Val loss: 771.365, val mse: 28.05455\n",
            "Epoch #17. Time: 6.5s. Train loss: 854.563, train mse: 29.27055. Val loss: 778.966, val mse: 27.55106\n",
            "Epoch #18. Time: 6.5s. Train loss: 784.425, train mse: 28.00008. Val loss: 838.420, val mse: 28.64308\n",
            "Epoch #19. Time: 6.5s. Train loss: 733.873, train mse: 27.03875. Val loss: 674.948, val mse: 26.12642\n",
            "Epoch #20. Time: 6.4s. Train loss: 669.604, train mse: 25.90128. Val loss: 777.892, val mse: 28.22019\n",
            "Epoch #21. Time: 6.5s. Train loss: 690.772, train mse: 26.19776. Val loss: 859.415, val mse: 29.61111\n",
            "Epoch #22. Time: 6.5s. Train loss: 615.130, train mse: 24.85720. Val loss: 760.457, val mse: 27.62074\n",
            "Epoch #23. Time: 6.5s. Train loss: 599.361, train mse: 24.44131. Val loss: 716.015, val mse: 27.21165\n",
            "Epoch #24. Time: 6.5s. Train loss: 550.055, train mse: 23.45598. Val loss: 720.729, val mse: 26.82950\n",
            "Epoch #25. Time: 6.5s. Train loss: 521.828, train mse: 22.87181. Val loss: 646.620, val mse: 25.60868\n",
            "Epoch #26. Time: 6.4s. Train loss: 476.464, train mse: 21.79759. Val loss: 758.551, val mse: 27.28251\n",
            "Epoch #27. Time: 6.5s. Train loss: 469.706, train mse: 21.65335. Val loss: 625.789, val mse: 25.44058\n",
            "Epoch #28. Time: 6.5s. Train loss: 469.365, train mse: 21.60231. Val loss: 666.816, val mse: 25.89654\n",
            "Epoch #29. Time: 6.5s. Train loss: 463.428, train mse: 21.57813. Val loss: 646.759, val mse: 25.53362\n",
            "Epoch #30. Time: 6.5s. Train loss: 432.200, train mse: 20.75661. Val loss: 646.088, val mse: 25.87763\n",
            "Epoch #31. Time: 6.4s. Train loss: 462.023, train mse: 21.49146. Val loss: 665.988, val mse: 25.95466\n",
            "Epoch #32. Time: 6.5s. Train loss: 470.030, train mse: 21.57426. Val loss: 636.478, val mse: 25.49101\n",
            "Epoch #33. Time: 6.5s. Train loss: 483.860, train mse: 22.02868. Val loss: 764.857, val mse: 27.98375\n",
            "Epoch #34. Time: 6.5s. Train loss: 451.595, train mse: 21.13810. Val loss: 634.938, val mse: 24.79386\n",
            "Epoch #35. Time: 6.5s. Train loss: 488.736, train mse: 22.10123. Val loss: 687.921, val mse: 26.25947\n",
            "Epoch #36. Time: 6.5s. Train loss: 441.779, train mse: 21.05298. Val loss: 725.625, val mse: 27.22845\n",
            "Epoch #37. Time: 6.5s. Train loss: 438.034, train mse: 20.95455. Val loss: 866.597, val mse: 28.67130\n",
            "Epoch #38. Time: 6.5s. Train loss: 417.599, train mse: 20.44230. Val loss: 660.383, val mse: 24.86121\n",
            "Epoch #39. Time: 6.4s. Train loss: 383.302, train mse: 19.53554. Val loss: 577.848, val mse: 24.24661\n",
            "Epoch #40. Time: 6.5s. Train loss: 374.560, train mse: 19.35941. Val loss: 567.845, val mse: 24.10100\n",
            "Epoch #41. Time: 6.5s. Train loss: 369.577, train mse: 19.15325. Val loss: 628.476, val mse: 24.34391\n",
            "Epoch #42. Time: 6.5s. Train loss: 307.754, train mse: 17.57596. Val loss: 652.149, val mse: 25.82319\n",
            "Epoch #43. Time: 6.4s. Train loss: 289.385, train mse: 17.03398. Val loss: 605.835, val mse: 24.68008\n",
            "Epoch #44. Time: 6.6s. Train loss: 295.170, train mse: 17.20524. Val loss: 710.208, val mse: 26.12022\n",
            "Epoch #45. Time: 6.5s. Train loss: 266.235, train mse: 16.33221. Val loss: 618.691, val mse: 25.02394\n",
            "Epoch #46. Time: 6.5s. Train loss: 258.843, train mse: 16.09157. Val loss: 603.546, val mse: 24.68135\n",
            "Epoch #47. Time: 6.4s. Train loss: 254.390, train mse: 15.97546. Val loss: 581.461, val mse: 24.34765\n",
            "Epoch #48. Time: 6.5s. Train loss: 238.530, train mse: 15.43810. Val loss: 599.284, val mse: 24.86600\n",
            "Epoch #49. Time: 6.4s. Train loss: 233.648, train mse: 15.26656. Val loss: 614.838, val mse: 25.19818\n",
            "Epoch #50. Time: 6.5s. Train loss: 243.342, train mse: 15.58699. Val loss: 611.995, val mse: 24.74458\n",
            "Training done. Best rmse: 24.100997924804688\n",
            "Start #2 fold\n",
            "Epoch #1. Time: 2.7s. Train loss: 6675.986, train mse: 81.68925. Val loss: 6364.253, val mse: 80.01699\n",
            "Epoch #2. Time: 2.7s. Train loss: 5258.141, train mse: 72.55414. Val loss: 4447.062, val mse: 66.46416\n",
            "Epoch #3. Time: 2.7s. Train loss: 2350.857, train mse: 48.52711. Val loss: 1963.680, val mse: 44.45725\n",
            "Epoch #4. Time: 2.7s. Train loss: 1319.116, train mse: 36.33579. Val loss: 1394.027, val mse: 37.67887\n",
            "Epoch #5. Time: 2.7s. Train loss: 1155.509, train mse: 33.97367. Val loss: 1339.757, val mse: 36.48819\n",
            "Epoch #6. Time: 2.7s. Train loss: 1113.310, train mse: 33.22520. Val loss: 1331.576, val mse: 36.51102\n",
            "Epoch #7. Time: 2.7s. Train loss: 1085.810, train mse: 32.90337. Val loss: 1165.629, val mse: 34.40483\n",
            "Epoch #8. Time: 2.7s. Train loss: 1094.417, train mse: 33.08727. Val loss: 1210.170, val mse: 35.11111\n",
            "Epoch #9. Time: 2.7s. Train loss: 1102.153, train mse: 33.27326. Val loss: 1097.009, val mse: 32.97408\n",
            "Epoch #10. Time: 2.7s. Train loss: 1073.945, train mse: 32.71226. Val loss: 1257.764, val mse: 35.48602\n",
            "Epoch #11. Time: 6.5s. Train loss: 1078.699, train mse: 32.83503. Val loss: 808.885, val mse: 28.72955\n",
            "Epoch #12. Time: 6.5s. Train loss: 1064.891, train mse: 32.59505. Val loss: 897.683, val mse: 29.97116\n",
            "Epoch #13. Time: 6.5s. Train loss: 981.247, train mse: 31.21777. Val loss: 1077.344, val mse: 32.41141\n",
            "Epoch #14. Time: 6.5s. Train loss: 967.047, train mse: 31.07919. Val loss: 757.624, val mse: 27.42183\n",
            "Epoch #15. Time: 6.5s. Train loss: 844.999, train mse: 29.04462. Val loss: 655.976, val mse: 25.10833\n",
            "Epoch #16. Time: 6.6s. Train loss: 854.750, train mse: 29.29957. Val loss: 893.832, val mse: 29.99881\n",
            "Epoch #17. Time: 6.5s. Train loss: 828.541, train mse: 28.78794. Val loss: 579.626, val mse: 23.76234\n",
            "Epoch #18. Time: 6.5s. Train loss: 779.083, train mse: 27.87179. Val loss: 699.040, val mse: 26.02695\n",
            "Epoch #19. Time: 6.5s. Train loss: 715.769, train mse: 26.75681. Val loss: 758.323, val mse: 28.04828\n",
            "Epoch #20. Time: 6.4s. Train loss: 687.280, train mse: 26.21415. Val loss: 628.887, val mse: 25.25524\n",
            "Epoch #21. Time: 6.5s. Train loss: 683.251, train mse: 26.10413. Val loss: 495.251, val mse: 22.37553\n",
            "Epoch #22. Time: 6.5s. Train loss: 636.911, train mse: 25.26367. Val loss: 500.102, val mse: 22.25538\n",
            "Epoch #23. Time: 6.5s. Train loss: 586.564, train mse: 24.21520. Val loss: 486.158, val mse: 22.24710\n",
            "Epoch #24. Time: 6.5s. Train loss: 555.083, train mse: 23.57322. Val loss: 476.036, val mse: 21.61072\n",
            "Epoch #25. Time: 6.5s. Train loss: 511.645, train mse: 22.66255. Val loss: 510.343, val mse: 22.43328\n",
            "Epoch #26. Time: 6.5s. Train loss: 475.123, train mse: 21.82012. Val loss: 475.419, val mse: 21.26791\n",
            "Epoch #27. Time: 6.5s. Train loss: 469.667, train mse: 21.72242. Val loss: 402.545, val mse: 20.30828\n",
            "Epoch #28. Time: 6.5s. Train loss: 453.224, train mse: 21.25403. Val loss: 411.892, val mse: 20.51089\n",
            "Epoch #29. Time: 6.5s. Train loss: 465.046, train mse: 21.59379. Val loss: 428.811, val mse: 20.64603\n",
            "Epoch #30. Time: 6.5s. Train loss: 438.105, train mse: 20.92782. Val loss: 446.148, val mse: 20.99083\n",
            "Epoch #31. Time: 6.4s. Train loss: 444.845, train mse: 21.11798. Val loss: 446.339, val mse: 21.20219\n",
            "Epoch #32. Time: 6.4s. Train loss: 457.999, train mse: 21.39591. Val loss: 472.978, val mse: 21.24601\n",
            "Epoch #33. Time: 6.5s. Train loss: 465.008, train mse: 21.55911. Val loss: 516.991, val mse: 22.42522\n",
            "Epoch #34. Time: 6.6s. Train loss: 450.446, train mse: 21.25396. Val loss: 548.694, val mse: 23.07380\n",
            "Epoch #35. Time: 6.5s. Train loss: 449.915, train mse: 21.21079. Val loss: 464.416, val mse: 21.45248\n",
            "Epoch #36. Time: 6.5s. Train loss: 407.737, train mse: 20.19465. Val loss: 423.563, val mse: 20.80852\n",
            "Epoch #37. Time: 6.5s. Train loss: 434.794, train mse: 20.81074. Val loss: 471.093, val mse: 21.89440\n",
            "Epoch #38. Time: 6.5s. Train loss: 385.373, train mse: 19.66644. Val loss: 479.767, val mse: 21.94383\n",
            "Epoch #39. Time: 6.5s. Train loss: 385.565, train mse: 19.63296. Val loss: 619.571, val mse: 24.57430\n",
            "Epoch #40. Time: 6.5s. Train loss: 384.697, train mse: 19.46826. Val loss: 441.839, val mse: 21.16067\n",
            "Epoch #41. Time: 6.5s. Train loss: 332.321, train mse: 18.27041. Val loss: 449.720, val mse: 21.42828\n",
            "Epoch #42. Time: 6.5s. Train loss: 336.923, train mse: 18.29299. Val loss: 524.318, val mse: 22.94844\n",
            "Epoch #43. Time: 6.5s. Train loss: 327.386, train mse: 18.12170. Val loss: 451.206, val mse: 21.10532\n",
            "Epoch #44. Time: 6.5s. Train loss: 308.403, train mse: 17.55999. Val loss: 457.012, val mse: 20.99645\n",
            "Epoch #45. Time: 6.5s. Train loss: 255.037, train mse: 15.97510. Val loss: 445.256, val mse: 21.29707\n",
            "Epoch #46. Time: 6.5s. Train loss: 266.441, train mse: 16.32384. Val loss: 473.849, val mse: 21.86253\n",
            "Epoch #47. Time: 6.5s. Train loss: 246.194, train mse: 15.68267. Val loss: 465.854, val mse: 21.07294\n",
            "Epoch #48. Time: 6.5s. Train loss: 264.680, train mse: 16.16020. Val loss: 448.020, val mse: 20.96661\n",
            "Epoch #49. Time: 6.5s. Train loss: 245.578, train mse: 15.64280. Val loss: 458.509, val mse: 20.98937\n",
            "Epoch #50. Time: 6.5s. Train loss: 237.115, train mse: 15.43619. Val loss: 436.720, val mse: 20.56859\n",
            "Training done. Best rmse: 20.308279037475586\n",
            "Start #3 fold\n",
            "Epoch #1. Time: 2.7s. Train loss: 6613.104, train mse: 81.32790. Val loss: 6683.421, val mse: 82.14784\n",
            "Epoch #2. Time: 2.7s. Train loss: 5131.473, train mse: 71.62160. Val loss: 4564.516, val mse: 68.11608\n",
            "Epoch #3. Time: 2.7s. Train loss: 2199.993, train mse: 46.98281. Val loss: 2024.862, val mse: 44.48560\n",
            "Epoch #4. Time: 2.7s. Train loss: 1282.719, train mse: 35.82770. Val loss: 1502.415, val mse: 39.08876\n",
            "Epoch #5. Time: 2.7s. Train loss: 1192.669, train mse: 34.58553. Val loss: 1378.172, val mse: 37.28619\n",
            "Epoch #6. Time: 2.7s. Train loss: 1151.018, train mse: 33.98182. Val loss: 1170.813, val mse: 34.65278\n",
            "Epoch #7. Time: 2.7s. Train loss: 1109.213, train mse: 33.24270. Val loss: 1235.007, val mse: 35.47561\n",
            "Epoch #8. Time: 2.7s. Train loss: 1044.340, train mse: 32.26199. Val loss: 1112.376, val mse: 32.98326\n",
            "Epoch #9. Time: 2.7s. Train loss: 1072.184, train mse: 32.75875. Val loss: 1141.935, val mse: 34.03235\n",
            "Epoch #10. Time: 2.7s. Train loss: 1051.068, train mse: 32.43216. Val loss: 1145.889, val mse: 34.10736\n",
            "Epoch #11. Time: 6.5s. Train loss: 1092.285, train mse: 33.06645. Val loss: 1090.655, val mse: 33.40487\n",
            "Epoch #12. Time: 6.5s. Train loss: 1010.275, train mse: 31.78500. Val loss: 858.421, val mse: 29.45475\n",
            "Epoch #13. Time: 6.5s. Train loss: 1006.177, train mse: 31.72948. Val loss: 740.478, val mse: 27.51684\n",
            "Epoch #14. Time: 6.6s. Train loss: 944.087, train mse: 30.65633. Val loss: 781.108, val mse: 27.65338\n",
            "Epoch #15. Time: 6.6s. Train loss: 970.839, train mse: 31.18471. Val loss: 1286.600, val mse: 35.75343\n",
            "Epoch #16. Time: 6.5s. Train loss: 843.857, train mse: 28.98435. Val loss: 668.682, val mse: 26.08828\n",
            "Epoch #17. Time: 6.5s. Train loss: 800.851, train mse: 28.28801. Val loss: 624.950, val mse: 25.23644\n",
            "Epoch #18. Time: 6.5s. Train loss: 752.676, train mse: 27.42157. Val loss: 573.865, val mse: 23.58603\n",
            "Epoch #19. Time: 6.5s. Train loss: 722.143, train mse: 26.77707. Val loss: 1171.703, val mse: 34.20769\n",
            "Epoch #20. Time: 6.5s. Train loss: 753.965, train mse: 27.46608. Val loss: 641.637, val mse: 25.65403\n",
            "Epoch #21. Time: 6.5s. Train loss: 678.522, train mse: 26.05444. Val loss: 709.285, val mse: 26.79453\n",
            "Epoch #22. Time: 6.5s. Train loss: 593.355, train mse: 24.33394. Val loss: 591.724, val mse: 24.60543\n",
            "Epoch #23. Time: 6.4s. Train loss: 530.251, train mse: 23.07067. Val loss: 850.601, val mse: 29.69129\n",
            "Epoch #24. Time: 6.5s. Train loss: 585.066, train mse: 24.11450. Val loss: 683.551, val mse: 26.25467\n",
            "Epoch #25. Time: 6.5s. Train loss: 516.376, train mse: 22.72676. Val loss: 589.639, val mse: 24.34707\n",
            "Epoch #26. Time: 6.5s. Train loss: 480.220, train mse: 21.94114. Val loss: 614.808, val mse: 24.60658\n",
            "Epoch #27. Time: 6.5s. Train loss: 468.987, train mse: 21.67831. Val loss: 591.229, val mse: 24.46183\n",
            "Epoch #28. Time: 6.5s. Train loss: 435.386, train mse: 20.88463. Val loss: 549.747, val mse: 23.80272\n",
            "Epoch #29. Time: 6.6s. Train loss: 429.928, train mse: 20.76641. Val loss: 613.297, val mse: 24.44428\n",
            "Epoch #30. Time: 6.4s. Train loss: 426.223, train mse: 20.66708. Val loss: 510.341, val mse: 22.95815\n",
            "Epoch #31. Time: 6.5s. Train loss: 443.795, train mse: 21.09763. Val loss: 543.206, val mse: 23.66087\n",
            "Epoch #32. Time: 6.5s. Train loss: 433.507, train mse: 20.84925. Val loss: 672.445, val mse: 26.25067\n",
            "Epoch #33. Time: 6.5s. Train loss: 447.511, train mse: 21.17647. Val loss: 584.562, val mse: 24.14319\n",
            "Epoch #34. Time: 6.5s. Train loss: 469.002, train mse: 21.57112. Val loss: 527.732, val mse: 23.12074\n",
            "Epoch #35. Time: 6.4s. Train loss: 433.377, train mse: 20.83692. Val loss: 531.064, val mse: 22.85145\n",
            "Epoch #36. Time: 6.5s. Train loss: 444.553, train mse: 21.11248. Val loss: 538.222, val mse: 23.25632\n",
            "Epoch #37. Time: 6.5s. Train loss: 415.659, train mse: 20.39442. Val loss: 545.600, val mse: 23.22076\n",
            "Epoch #38. Time: 6.5s. Train loss: 417.316, train mse: 20.42164. Val loss: 491.286, val mse: 22.38135\n",
            "Epoch #39. Time: 6.5s. Train loss: 372.798, train mse: 19.29468. Val loss: 550.152, val mse: 23.44998\n",
            "Epoch #40. Time: 6.5s. Train loss: 372.336, train mse: 19.29562. Val loss: 556.161, val mse: 23.67655\n",
            "Epoch #41. Time: 6.5s. Train loss: 345.634, train mse: 18.51308. Val loss: 508.160, val mse: 22.55567\n",
            "Epoch #42. Time: 6.5s. Train loss: 364.363, train mse: 19.05106. Val loss: 456.395, val mse: 21.42650\n",
            "Epoch #43. Time: 6.5s. Train loss: 291.476, train mse: 17.08963. Val loss: 452.654, val mse: 21.42442\n",
            "Epoch #44. Time: 6.5s. Train loss: 294.429, train mse: 17.08533. Val loss: 509.504, val mse: 22.73652\n",
            "Epoch #45. Time: 6.5s. Train loss: 261.857, train mse: 16.14624. Val loss: 456.036, val mse: 21.63354\n",
            "Epoch #46. Time: 6.5s. Train loss: 264.022, train mse: 16.24847. Val loss: 464.262, val mse: 21.80561\n",
            "Epoch #47. Time: 6.5s. Train loss: 234.537, train mse: 15.32664. Val loss: 457.533, val mse: 21.55852\n",
            "Epoch #48. Time: 6.5s. Train loss: 255.315, train mse: 16.00771. Val loss: 476.033, val mse: 21.91243\n",
            "Epoch #49. Time: 6.5s. Train loss: 265.692, train mse: 16.27215. Val loss: 493.288, val mse: 21.59996\n",
            "Epoch #50. Time: 6.5s. Train loss: 224.035, train mse: 14.91878. Val loss: 452.944, val mse: 21.56205\n",
            "Training done. Best rmse: 21.424421310424805\n",
            "Start #4 fold\n",
            "Epoch #1. Time: 2.7s. Train loss: 6729.929, train mse: 81.91833. Val loss: 6361.506, val mse: 80.15395\n",
            "Epoch #2. Time: 2.7s. Train loss: 5441.028, train mse: 73.86590. Val loss: 4816.213, val mse: 69.08546\n",
            "Epoch #3. Time: 2.7s. Train loss: 2593.477, train mse: 50.97569. Val loss: 2250.354, val mse: 47.85555\n",
            "Epoch #4. Time: 2.7s. Train loss: 1322.820, train mse: 36.40286. Val loss: 1429.756, val mse: 37.67915\n",
            "Epoch #5. Time: 2.7s. Train loss: 1225.115, train mse: 35.05758. Val loss: 1386.485, val mse: 37.46913\n",
            "Epoch #6. Time: 2.7s. Train loss: 1197.491, train mse: 34.63261. Val loss: 1512.678, val mse: 38.99310\n",
            "Epoch #7. Time: 2.7s. Train loss: 1115.765, train mse: 33.47533. Val loss: 1303.598, val mse: 36.16585\n",
            "Epoch #8. Time: 2.7s. Train loss: 1081.777, train mse: 32.92353. Val loss: 1389.541, val mse: 36.48162\n",
            "Epoch #9. Time: 2.7s. Train loss: 1063.792, train mse: 32.57708. Val loss: 1244.328, val mse: 35.14494\n",
            "Epoch #10. Time: 2.7s. Train loss: 1028.024, train mse: 32.09733. Val loss: 1178.778, val mse: 34.30798\n",
            "Epoch #11. Time: 6.6s. Train loss: 1026.737, train mse: 32.03748. Val loss: 1094.048, val mse: 32.52467\n",
            "Epoch #12. Time: 6.5s. Train loss: 1045.121, train mse: 32.35913. Val loss: 1068.415, val mse: 32.51407\n",
            "Epoch #13. Time: 6.5s. Train loss: 980.104, train mse: 31.35468. Val loss: 982.451, val mse: 31.26969\n",
            "Epoch #14. Time: 6.4s. Train loss: 899.130, train mse: 29.92273. Val loss: 794.599, val mse: 28.39184\n",
            "Epoch #15. Time: 6.5s. Train loss: 928.477, train mse: 30.42664. Val loss: 808.749, val mse: 28.58995\n",
            "Epoch #16. Time: 6.5s. Train loss: 861.963, train mse: 29.36275. Val loss: 819.341, val mse: 28.60057\n",
            "Epoch #17. Time: 6.5s. Train loss: 774.646, train mse: 27.87982. Val loss: 743.718, val mse: 27.43836\n",
            "Epoch #18. Time: 6.5s. Train loss: 780.142, train mse: 27.83013. Val loss: 776.211, val mse: 27.59318\n",
            "Epoch #19. Time: 6.6s. Train loss: 720.947, train mse: 26.91175. Val loss: 771.854, val mse: 27.34281\n",
            "Epoch #20. Time: 6.4s. Train loss: 663.317, train mse: 25.78425. Val loss: 789.410, val mse: 28.04852\n",
            "Epoch #21. Time: 6.5s. Train loss: 665.298, train mse: 25.84124. Val loss: 771.171, val mse: 27.63536\n",
            "Epoch #22. Time: 6.5s. Train loss: 580.728, train mse: 24.08343. Val loss: 716.229, val mse: 27.10604\n",
            "Epoch #23. Time: 6.5s. Train loss: 566.865, train mse: 23.82987. Val loss: 832.302, val mse: 28.16442\n",
            "Epoch #24. Time: 6.5s. Train loss: 549.626, train mse: 23.42902. Val loss: 671.715, val mse: 26.03006\n",
            "Epoch #25. Time: 6.5s. Train loss: 506.315, train mse: 22.53356. Val loss: 752.644, val mse: 27.06224\n",
            "Epoch #26. Time: 6.5s. Train loss: 468.194, train mse: 21.63677. Val loss: 722.979, val mse: 26.48967\n",
            "Epoch #27. Time: 6.4s. Train loss: 495.942, train mse: 22.31027. Val loss: 650.063, val mse: 25.78846\n",
            "Epoch #28. Time: 6.5s. Train loss: 430.195, train mse: 20.76926. Val loss: 736.711, val mse: 26.91556\n",
            "Epoch #29. Time: 6.5s. Train loss: 454.341, train mse: 21.17012. Val loss: 738.910, val mse: 26.81018\n",
            "Epoch #30. Time: 6.5s. Train loss: 411.271, train mse: 20.29109. Val loss: 733.718, val mse: 26.57769\n",
            "Epoch #31. Time: 6.5s. Train loss: 429.420, train mse: 20.70557. Val loss: 715.878, val mse: 26.59212\n",
            "Epoch #32. Time: 6.5s. Train loss: 463.194, train mse: 21.46734. Val loss: 695.650, val mse: 26.10719\n",
            "Epoch #33. Time: 6.5s. Train loss: 427.706, train mse: 20.68154. Val loss: 716.288, val mse: 26.76752\n",
            "Epoch #34. Time: 6.5s. Train loss: 443.803, train mse: 21.06611. Val loss: 703.757, val mse: 26.93172\n",
            "Epoch #35. Time: 6.5s. Train loss: 419.475, train mse: 20.49662. Val loss: 797.428, val mse: 27.84161\n",
            "Epoch #36. Time: 6.5s. Train loss: 472.373, train mse: 21.73199. Val loss: 839.174, val mse: 28.46047\n",
            "Epoch #37. Time: 6.4s. Train loss: 433.845, train mse: 20.83616. Val loss: 643.101, val mse: 25.38818\n",
            "Epoch #38. Time: 6.6s. Train loss: 408.264, train mse: 20.21599. Val loss: 645.919, val mse: 25.63634\n",
            "Epoch #39. Time: 6.6s. Train loss: 381.997, train mse: 19.43309. Val loss: 740.892, val mse: 27.50583\n",
            "Epoch #40. Time: 6.5s. Train loss: 362.110, train mse: 19.04245. Val loss: 604.056, val mse: 24.94902\n",
            "Epoch #41. Time: 6.5s. Train loss: 350.699, train mse: 18.72860. Val loss: 728.212, val mse: 26.89901\n",
            "Epoch #42. Time: 6.5s. Train loss: 319.582, train mse: 17.87386. Val loss: 569.652, val mse: 24.04542\n",
            "Epoch #43. Time: 6.5s. Train loss: 304.971, train mse: 17.29996. Val loss: 622.678, val mse: 25.38150\n",
            "Epoch #44. Time: 6.5s. Train loss: 267.438, train mse: 16.32792. Val loss: 635.230, val mse: 25.57146\n",
            "Epoch #45. Time: 6.5s. Train loss: 260.408, train mse: 16.17036. Val loss: 646.985, val mse: 25.64237\n",
            "Epoch #46. Time: 6.5s. Train loss: 260.233, train mse: 16.14636. Val loss: 681.369, val mse: 25.02748\n",
            "Epoch #47. Time: 6.5s. Train loss: 272.519, train mse: 16.50036. Val loss: 626.797, val mse: 24.98843\n",
            "Epoch #48. Time: 6.4s. Train loss: 231.990, train mse: 15.24710. Val loss: 600.223, val mse: 24.66279\n",
            "Epoch #49. Time: 6.5s. Train loss: 247.051, train mse: 15.73119. Val loss: 674.646, val mse: 25.82571\n",
            "Epoch #50. Time: 6.5s. Train loss: 239.668, train mse: 15.49583. Val loss: 629.187, val mse: 25.33263\n",
            "Training done. Best rmse: 24.045419692993164\n",
            "Start #5 fold\n",
            "Epoch #1. Time: 2.7s. Train loss: 6704.646, train mse: 81.78211. Val loss: 6447.045, val mse: 80.64775\n",
            "Epoch #2. Time: 2.7s. Train loss: 5328.201, train mse: 73.13251. Val loss: 4658.652, val mse: 68.71423\n",
            "Epoch #3. Time: 2.7s. Train loss: 2412.937, train mse: 49.21714. Val loss: 2150.371, val mse: 46.72697\n",
            "Epoch #4. Time: 2.7s. Train loss: 1289.865, train mse: 35.89652. Val loss: 1620.203, val mse: 39.93413\n",
            "Epoch #5. Time: 2.7s. Train loss: 1238.191, train mse: 35.20525. Val loss: 1542.297, val mse: 39.00205\n",
            "Epoch #6. Time: 2.8s. Train loss: 1173.363, train mse: 34.27055. Val loss: 1274.267, val mse: 35.73389\n",
            "Epoch #7. Time: 2.7s. Train loss: 1080.079, train mse: 32.85558. Val loss: 1305.971, val mse: 36.10439\n",
            "Epoch #8. Time: 2.7s. Train loss: 1067.875, train mse: 32.67128. Val loss: 1303.608, val mse: 36.47816\n",
            "Epoch #9. Time: 2.7s. Train loss: 1035.363, train mse: 32.20441. Val loss: 1341.604, val mse: 36.41634\n",
            "Epoch #10. Time: 2.7s. Train loss: 1023.721, train mse: 32.07371. Val loss: 1240.324, val mse: 35.51023\n",
            "Epoch #11. Time: 6.5s. Train loss: 1036.354, train mse: 32.18489. Val loss: 982.886, val mse: 31.46766\n",
            "Epoch #12. Time: 6.5s. Train loss: 1061.597, train mse: 32.49353. Val loss: 1150.024, val mse: 33.70881\n",
            "Epoch #13. Time: 6.5s. Train loss: 1014.187, train mse: 31.79369. Val loss: 874.895, val mse: 29.32221\n",
            "Epoch #14. Time: 6.5s. Train loss: 925.749, train mse: 30.47443. Val loss: 825.800, val mse: 28.61552\n",
            "Epoch #15. Time: 6.5s. Train loss: 859.103, train mse: 29.32472. Val loss: 981.643, val mse: 30.67645\n",
            "Epoch #16. Time: 6.5s. Train loss: 871.187, train mse: 29.46002. Val loss: 984.231, val mse: 31.32822\n",
            "Epoch #17. Time: 6.5s. Train loss: 825.832, train mse: 28.72582. Val loss: 722.664, val mse: 26.53292\n",
            "Epoch #18. Time: 6.5s. Train loss: 724.958, train mse: 26.93909. Val loss: 651.322, val mse: 25.56958\n",
            "Epoch #19. Time: 6.4s. Train loss: 767.371, train mse: 27.77362. Val loss: 658.489, val mse: 26.02622\n",
            "Epoch #20. Time: 6.5s. Train loss: 666.203, train mse: 25.79598. Val loss: 584.727, val mse: 24.18843\n",
            "Epoch #21. Time: 6.5s. Train loss: 626.903, train mse: 24.96136. Val loss: 562.263, val mse: 23.37545\n",
            "Epoch #22. Time: 6.5s. Train loss: 604.017, train mse: 24.42116. Val loss: 550.283, val mse: 23.80055\n",
            "Epoch #23. Time: 6.5s. Train loss: 552.856, train mse: 23.52588. Val loss: 515.359, val mse: 22.60356\n",
            "Epoch #24. Time: 6.5s. Train loss: 513.470, train mse: 22.66099. Val loss: 532.579, val mse: 23.15130\n",
            "Epoch #25. Time: 6.5s. Train loss: 495.538, train mse: 22.26732. Val loss: 462.346, val mse: 21.66577\n",
            "Epoch #26. Time: 6.5s. Train loss: 463.760, train mse: 21.57121. Val loss: 423.433, val mse: 20.26960\n",
            "Epoch #27. Time: 6.6s. Train loss: 438.601, train mse: 20.97161. Val loss: 449.298, val mse: 20.99154\n",
            "Epoch #28. Time: 6.5s. Train loss: 409.413, train mse: 20.24497. Val loss: 422.620, val mse: 20.46741\n",
            "Epoch #29. Time: 6.5s. Train loss: 420.892, train mse: 20.51968. Val loss: 523.069, val mse: 22.50132\n",
            "Epoch #30. Time: 6.5s. Train loss: 430.977, train mse: 20.80723. Val loss: 465.640, val mse: 21.13446\n",
            "Epoch #31. Time: 6.5s. Train loss: 425.264, train mse: 20.60655. Val loss: 406.165, val mse: 20.54194\n",
            "Epoch #32. Time: 6.5s. Train loss: 438.569, train mse: 20.89979. Val loss: 409.453, val mse: 20.28247\n",
            "Epoch #33. Time: 6.5s. Train loss: 449.815, train mse: 21.20867. Val loss: 530.844, val mse: 23.08075\n",
            "Epoch #34. Time: 6.5s. Train loss: 419.899, train mse: 20.51477. Val loss: 562.912, val mse: 23.36639\n",
            "Epoch #35. Time: 6.5s. Train loss: 442.137, train mse: 21.01960. Val loss: 520.827, val mse: 22.53225\n",
            "Epoch #36. Time: 6.5s. Train loss: 433.941, train mse: 20.71172. Val loss: 517.462, val mse: 22.88058\n",
            "Epoch #37. Time: 6.5s. Train loss: 462.607, train mse: 21.44563. Val loss: 530.247, val mse: 23.17879\n",
            "Epoch #38. Time: 6.5s. Train loss: 413.220, train mse: 20.33627. Val loss: 549.122, val mse: 23.08282\n",
            "Epoch #39. Time: 6.6s. Train loss: 376.678, train mse: 19.46133. Val loss: 454.017, val mse: 21.49013\n",
            "Epoch #40. Time: 6.5s. Train loss: 381.510, train mse: 19.55235. Val loss: 486.796, val mse: 22.30547\n",
            "Epoch #41. Time: 6.5s. Train loss: 323.830, train mse: 18.03118. Val loss: 614.510, val mse: 25.01942\n",
            "Epoch #42. Time: 6.5s. Train loss: 307.427, train mse: 17.45069. Val loss: 538.048, val mse: 23.05822\n",
            "Epoch #43. Time: 6.5s. Train loss: 284.834, train mse: 16.84781. Val loss: 480.799, val mse: 22.16538\n",
            "Epoch #44. Time: 6.5s. Train loss: 294.852, train mse: 17.09398. Val loss: 542.798, val mse: 23.42001\n",
            "Epoch #45. Time: 6.5s. Train loss: 240.705, train mse: 15.50538. Val loss: 498.758, val mse: 22.70716\n",
            "Epoch #46. Time: 6.5s. Train loss: 289.485, train mse: 16.97941. Val loss: 499.095, val mse: 22.50066\n",
            "Epoch #47. Time: 6.4s. Train loss: 259.694, train mse: 16.11585. Val loss: 464.546, val mse: 21.65871\n",
            "Epoch #48. Time: 6.5s. Train loss: 226.358, train mse: 15.05103. Val loss: 461.569, val mse: 21.63865\n",
            "Epoch #49. Time: 6.4s. Train loss: 246.720, train mse: 15.71995. Val loss: 469.558, val mse: 21.76865\n",
            "Epoch #50. Time: 6.5s. Train loss: 253.669, train mse: 15.91982. Val loss: 483.198, val mse: 21.65484\n",
            "Training done. Best rmse: 20.26959800720215\n",
            "Start #6 fold\n",
            "Epoch #1. Time: 2.7s. Train loss: 6704.810, train mse: 81.91682. Val loss: 6016.533, val mse: 77.33852\n",
            "Epoch #2. Time: 2.7s. Train loss: 5276.087, train mse: 72.78575. Val loss: 4091.616, val mse: 63.95430\n",
            "Epoch #3. Time: 2.7s. Train loss: 2389.455, train mse: 48.96391. Val loss: 1950.958, val mse: 43.95618\n",
            "Epoch #4. Time: 2.7s. Train loss: 1293.060, train mse: 35.93777. Val loss: 1290.662, val mse: 36.48207\n",
            "Epoch #5. Time: 2.8s. Train loss: 1209.225, train mse: 34.78242. Val loss: 1420.350, val mse: 37.18751\n",
            "Epoch #6. Time: 2.7s. Train loss: 1134.591, train mse: 33.70459. Val loss: 1432.137, val mse: 37.90791\n",
            "Epoch #7. Time: 2.8s. Train loss: 1159.076, train mse: 33.97392. Val loss: 1200.797, val mse: 34.74693\n",
            "Epoch #8. Time: 2.7s. Train loss: 1108.967, train mse: 33.32203. Val loss: 1068.830, val mse: 33.00712\n",
            "Epoch #9. Time: 2.7s. Train loss: 1065.702, train mse: 32.63982. Val loss: 1016.159, val mse: 31.96603\n",
            "Epoch #10. Time: 2.7s. Train loss: 1080.563, train mse: 32.85918. Val loss: 975.080, val mse: 31.30425\n",
            "Epoch #11. Time: 6.6s. Train loss: 1078.612, train mse: 32.82051. Val loss: 833.462, val mse: 29.13585\n",
            "Epoch #12. Time: 6.6s. Train loss: 1011.059, train mse: 31.84810. Val loss: 765.772, val mse: 27.82169\n",
            "Epoch #13. Time: 6.6s. Train loss: 938.679, train mse: 30.71092. Val loss: 840.671, val mse: 29.02255\n",
            "Epoch #14. Time: 6.6s. Train loss: 979.624, train mse: 31.23648. Val loss: 900.955, val mse: 29.56063\n",
            "Epoch #15. Time: 6.5s. Train loss: 942.414, train mse: 30.69216. Val loss: 669.205, val mse: 26.03522\n",
            "Epoch #16. Time: 6.5s. Train loss: 809.063, train mse: 28.49572. Val loss: 726.128, val mse: 27.23655\n",
            "Epoch #17. Time: 6.5s. Train loss: 814.121, train mse: 28.55543. Val loss: 650.002, val mse: 25.96643\n",
            "Epoch #18. Time: 6.5s. Train loss: 760.545, train mse: 27.57353. Val loss: 622.767, val mse: 24.90426\n",
            "Epoch #19. Time: 6.5s. Train loss: 692.449, train mse: 26.28602. Val loss: 712.153, val mse: 26.62397\n",
            "Epoch #20. Time: 6.6s. Train loss: 670.947, train mse: 25.94925. Val loss: 910.933, val mse: 29.88490\n",
            "Epoch #21. Time: 6.5s. Train loss: 670.679, train mse: 25.87388. Val loss: 622.973, val mse: 25.01871\n",
            "Epoch #22. Time: 6.5s. Train loss: 585.881, train mse: 24.18962. Val loss: 768.725, val mse: 28.04582\n",
            "Epoch #23. Time: 6.6s. Train loss: 588.267, train mse: 24.02703. Val loss: 589.824, val mse: 24.45024\n",
            "Epoch #24. Time: 6.5s. Train loss: 555.866, train mse: 23.60604. Val loss: 625.749, val mse: 24.92751\n",
            "Epoch #25. Time: 6.5s. Train loss: 486.850, train mse: 22.10366. Val loss: 637.264, val mse: 25.36090\n",
            "Epoch #26. Time: 6.5s. Train loss: 493.583, train mse: 22.21325. Val loss: 638.521, val mse: 24.81797\n",
            "Epoch #27. Time: 6.5s. Train loss: 472.581, train mse: 21.70206. Val loss: 585.078, val mse: 24.28078\n",
            "Epoch #28. Time: 6.5s. Train loss: 459.694, train mse: 21.45922. Val loss: 542.689, val mse: 23.35687\n",
            "Epoch #29. Time: 6.5s. Train loss: 442.196, train mse: 21.04527. Val loss: 600.178, val mse: 24.37410\n",
            "Epoch #30. Time: 6.6s. Train loss: 397.984, train mse: 19.99598. Val loss: 580.901, val mse: 24.22695\n",
            "Epoch #31. Time: 6.6s. Train loss: 432.168, train mse: 20.81891. Val loss: 672.548, val mse: 25.78673\n",
            "Epoch #32. Time: 6.5s. Train loss: 423.879, train mse: 20.61648. Val loss: 585.116, val mse: 24.41098\n",
            "Epoch #33. Time: 6.6s. Train loss: 409.511, train mse: 20.23331. Val loss: 538.870, val mse: 23.37280\n",
            "Epoch #34. Time: 6.5s. Train loss: 440.290, train mse: 21.00988. Val loss: 916.373, val mse: 30.23363\n",
            "Epoch #35. Time: 6.5s. Train loss: 456.963, train mse: 21.37533. Val loss: 590.230, val mse: 24.52560\n",
            "Epoch #36. Time: 6.5s. Train loss: 436.066, train mse: 20.87918. Val loss: 673.175, val mse: 26.41894\n",
            "Epoch #37. Time: 6.5s. Train loss: 439.993, train mse: 20.99788. Val loss: 614.556, val mse: 24.65931\n",
            "Epoch #38. Time: 6.5s. Train loss: 381.327, train mse: 19.54169. Val loss: 581.990, val mse: 24.48754\n",
            "Epoch #39. Time: 6.5s. Train loss: 370.355, train mse: 19.25993. Val loss: 607.241, val mse: 24.25418\n",
            "Epoch #40. Time: 6.5s. Train loss: 416.006, train mse: 20.44266. Val loss: 561.179, val mse: 23.85003\n",
            "Epoch #41. Time: 6.6s. Train loss: 337.032, train mse: 18.37070. Val loss: 597.440, val mse: 24.68252\n",
            "Epoch #42. Time: 6.5s. Train loss: 297.223, train mse: 17.19615. Val loss: 636.304, val mse: 25.36099\n",
            "Epoch #43. Time: 6.5s. Train loss: 303.590, train mse: 17.41422. Val loss: 549.068, val mse: 23.77524\n",
            "Epoch #44. Time: 6.5s. Train loss: 280.282, train mse: 16.76911. Val loss: 589.935, val mse: 24.55054\n",
            "Epoch #45. Time: 6.5s. Train loss: 269.472, train mse: 16.39448. Val loss: 588.443, val mse: 24.52031\n",
            "Epoch #46. Time: 6.5s. Train loss: 266.950, train mse: 16.33553. Val loss: 558.127, val mse: 23.83673\n",
            "Epoch #47. Time: 6.5s. Train loss: 237.889, train mse: 15.43698. Val loss: 622.849, val mse: 24.90348\n",
            "Epoch #48. Time: 6.5s. Train loss: 254.240, train mse: 15.94114. Val loss: 600.012, val mse: 24.78931\n",
            "Epoch #49. Time: 6.5s. Train loss: 232.978, train mse: 15.29551. Val loss: 587.091, val mse: 24.39835\n",
            "Epoch #50. Time: 6.5s. Train loss: 237.350, train mse: 15.42738. Val loss: 589.821, val mse: 24.23185\n",
            "Training done. Best rmse: 23.35687255859375\n",
            "Start #7 fold\n",
            "Epoch #1. Time: 2.7s. Train loss: 6718.689, train mse: 82.07977. Val loss: 6312.992, val mse: 79.13211\n",
            "Epoch #2. Time: 2.7s. Train loss: 5524.818, train mse: 74.35564. Val loss: 4647.289, val mse: 68.07539\n",
            "Epoch #3. Time: 2.7s. Train loss: 2735.107, train mse: 52.39303. Val loss: 1801.104, val mse: 42.76196\n",
            "Epoch #4. Time: 2.7s. Train loss: 1334.766, train mse: 36.57967. Val loss: 1286.857, val mse: 36.05042\n",
            "Epoch #5. Time: 2.7s. Train loss: 1212.588, train mse: 34.80871. Val loss: 1177.896, val mse: 34.36931\n",
            "Epoch #6. Time: 2.7s. Train loss: 1175.347, train mse: 34.22457. Val loss: 1071.766, val mse: 32.70032\n",
            "Epoch #7. Time: 2.7s. Train loss: 1101.246, train mse: 33.15976. Val loss: 1053.684, val mse: 32.23915\n",
            "Epoch #8. Time: 2.7s. Train loss: 1099.431, train mse: 33.14069. Val loss: 1088.283, val mse: 32.93935\n",
            "Epoch #9. Time: 2.7s. Train loss: 1072.609, train mse: 32.77859. Val loss: 1084.687, val mse: 32.42782\n",
            "Epoch #10. Time: 2.7s. Train loss: 1095.683, train mse: 33.04050. Val loss: 1091.426, val mse: 33.01596\n",
            "Epoch #11. Time: 6.6s. Train loss: 1055.652, train mse: 32.49149. Val loss: 945.084, val mse: 30.84198\n",
            "Epoch #12. Time: 6.5s. Train loss: 1077.474, train mse: 32.87067. Val loss: 845.557, val mse: 29.18754\n",
            "Epoch #13. Time: 6.5s. Train loss: 982.872, train mse: 31.37737. Val loss: 819.474, val mse: 28.74847\n",
            "Epoch #14. Time: 6.5s. Train loss: 903.916, train mse: 30.09847. Val loss: 1023.701, val mse: 32.16322\n",
            "Epoch #15. Time: 6.6s. Train loss: 924.010, train mse: 30.25265. Val loss: 880.724, val mse: 29.68234\n",
            "Epoch #16. Time: 6.5s. Train loss: 883.117, train mse: 29.79090. Val loss: 763.735, val mse: 27.57571\n",
            "Epoch #17. Time: 6.5s. Train loss: 856.015, train mse: 29.28589. Val loss: 833.463, val mse: 29.31097\n",
            "Epoch #18. Time: 6.5s. Train loss: 763.209, train mse: 27.61156. Val loss: 782.744, val mse: 27.28527\n",
            "Epoch #19. Time: 6.5s. Train loss: 732.133, train mse: 26.99447. Val loss: 742.066, val mse: 27.46006\n",
            "Epoch #20. Time: 6.5s. Train loss: 673.973, train mse: 25.96822. Val loss: 930.994, val mse: 29.70080\n",
            "Epoch #21. Time: 6.5s. Train loss: 630.510, train mse: 25.10450. Val loss: 566.401, val mse: 23.98497\n",
            "Epoch #22. Time: 6.5s. Train loss: 591.223, train mse: 24.24951. Val loss: 646.525, val mse: 24.89251\n",
            "Epoch #23. Time: 6.5s. Train loss: 561.158, train mse: 23.68603. Val loss: 540.090, val mse: 23.34330\n",
            "Epoch #24. Time: 6.5s. Train loss: 515.028, train mse: 22.61790. Val loss: 524.001, val mse: 22.84899\n",
            "Epoch #25. Time: 6.5s. Train loss: 531.239, train mse: 23.03089. Val loss: 481.329, val mse: 22.20463\n",
            "Epoch #26. Time: 6.5s. Train loss: 465.312, train mse: 21.57137. Val loss: 515.616, val mse: 22.23606\n",
            "Epoch #27. Time: 6.5s. Train loss: 468.335, train mse: 21.65147. Val loss: 491.685, val mse: 22.16904\n",
            "Epoch #28. Time: 6.5s. Train loss: 417.046, train mse: 20.41649. Val loss: 456.874, val mse: 21.19550\n",
            "Epoch #29. Time: 6.5s. Train loss: 457.515, train mse: 21.36034. Val loss: 424.126, val mse: 21.04448\n",
            "Epoch #30. Time: 6.5s. Train loss: 414.883, train mse: 20.35007. Val loss: 476.436, val mse: 21.96835\n",
            "Epoch #31. Time: 6.5s. Train loss: 406.644, train mse: 20.13568. Val loss: 519.277, val mse: 22.61669\n",
            "Epoch #32. Time: 6.4s. Train loss: 411.018, train mse: 20.28309. Val loss: 454.911, val mse: 21.51202\n",
            "Epoch #33. Time: 6.5s. Train loss: 437.025, train mse: 20.94013. Val loss: 544.386, val mse: 23.33377\n",
            "Epoch #34. Time: 6.5s. Train loss: 467.880, train mse: 21.58181. Val loss: 525.168, val mse: 22.89668\n",
            "Epoch #35. Time: 6.5s. Train loss: 428.695, train mse: 20.74519. Val loss: 572.365, val mse: 24.10585\n",
            "Epoch #36. Time: 6.5s. Train loss: 423.683, train mse: 20.56427. Val loss: 515.476, val mse: 22.60634\n",
            "Epoch #37. Time: 6.5s. Train loss: 393.969, train mse: 19.86536. Val loss: 588.188, val mse: 24.46566\n",
            "Epoch #38. Time: 6.5s. Train loss: 378.697, train mse: 19.46349. Val loss: 756.852, val mse: 27.34442\n",
            "Epoch #39. Time: 6.5s. Train loss: 361.664, train mse: 18.98504. Val loss: 579.290, val mse: 23.89896\n",
            "Epoch #40. Time: 6.4s. Train loss: 344.381, train mse: 18.49802. Val loss: 543.474, val mse: 23.68031\n",
            "Epoch #41. Time: 6.5s. Train loss: 305.384, train mse: 17.45655. Val loss: 575.334, val mse: 23.88393\n",
            "Epoch #42. Time: 6.5s. Train loss: 296.176, train mse: 17.21763. Val loss: 498.846, val mse: 22.28414\n",
            "Epoch #43. Time: 6.5s. Train loss: 279.970, train mse: 16.77575. Val loss: 506.113, val mse: 21.65386\n",
            "Epoch #44. Time: 6.5s. Train loss: 261.944, train mse: 16.16042. Val loss: 498.344, val mse: 22.40335\n",
            "Epoch #45. Time: 6.5s. Train loss: 255.379, train mse: 15.99989. Val loss: 506.068, val mse: 22.43546\n",
            "Epoch #46. Time: 6.4s. Train loss: 271.909, train mse: 16.48772. Val loss: 488.903, val mse: 22.27956\n",
            "Epoch #47. Time: 6.4s. Train loss: 234.511, train mse: 15.31746. Val loss: 482.908, val mse: 22.23578\n",
            "Epoch #48. Time: 6.5s. Train loss: 233.748, train mse: 15.25091. Val loss: 529.879, val mse: 23.00924\n",
            "Epoch #49. Time: 6.5s. Train loss: 233.431, train mse: 15.30212. Val loss: 540.387, val mse: 23.48953\n",
            "Epoch #50. Time: 6.5s. Train loss: 231.449, train mse: 15.22794. Val loss: 475.271, val mse: 22.12849\n",
            "Training done. Best rmse: 21.044483184814453\n",
            "Start #8 fold\n",
            "Epoch #1. Time: 2.7s. Train loss: 6720.247, train mse: 82.01574. Val loss: 6116.330, val mse: 78.14219\n",
            "Epoch #2. Time: 2.7s. Train loss: 5451.143, train mse: 73.83263. Val loss: 4494.840, val mse: 66.89902\n",
            "Epoch #3. Time: 2.7s. Train loss: 2604.722, train mse: 51.09765. Val loss: 1861.505, val mse: 42.93554\n",
            "Epoch #4. Time: 2.7s. Train loss: 1277.567, train mse: 35.78218. Val loss: 1386.323, val mse: 37.12747\n",
            "Epoch #5. Time: 2.7s. Train loss: 1222.591, train mse: 34.96177. Val loss: 1197.859, val mse: 34.57784\n",
            "Epoch #6. Time: 2.7s. Train loss: 1207.949, train mse: 34.78015. Val loss: 1117.998, val mse: 33.08825\n",
            "Epoch #7. Time: 2.7s. Train loss: 1126.699, train mse: 33.56213. Val loss: 1020.073, val mse: 31.32184\n",
            "Epoch #8. Time: 2.7s. Train loss: 1145.060, train mse: 33.89280. Val loss: 1161.981, val mse: 34.25105\n",
            "Epoch #9. Time: 2.7s. Train loss: 1154.138, train mse: 33.96749. Val loss: 1010.999, val mse: 32.11223\n",
            "Epoch #10. Time: 2.7s. Train loss: 1088.541, train mse: 33.01591. Val loss: 951.311, val mse: 30.82431\n",
            "Epoch #11. Time: 6.5s. Train loss: 1091.802, train mse: 33.00839. Val loss: 797.742, val mse: 28.50314\n",
            "Epoch #12. Time: 6.4s. Train loss: 1015.355, train mse: 31.80629. Val loss: 940.004, val mse: 30.79236\n",
            "Epoch #13. Time: 6.6s. Train loss: 1015.490, train mse: 31.78822. Val loss: 620.562, val mse: 24.94452\n",
            "Epoch #14. Time: 6.6s. Train loss: 936.305, train mse: 30.57088. Val loss: 735.684, val mse: 27.08641\n",
            "Epoch #15. Time: 6.5s. Train loss: 926.491, train mse: 30.36651. Val loss: 695.892, val mse: 26.26633\n",
            "Epoch #16. Time: 6.5s. Train loss: 870.409, train mse: 29.51998. Val loss: 765.943, val mse: 27.46272\n",
            "Epoch #17. Time: 6.6s. Train loss: 801.600, train mse: 28.33273. Val loss: 559.065, val mse: 24.04473\n",
            "Epoch #18. Time: 6.5s. Train loss: 794.969, train mse: 28.18307. Val loss: 630.393, val mse: 25.46747\n",
            "Epoch #19. Time: 6.5s. Train loss: 714.501, train mse: 26.78174. Val loss: 625.993, val mse: 25.27643\n",
            "Epoch #20. Time: 6.5s. Train loss: 737.777, train mse: 27.21420. Val loss: 571.243, val mse: 24.09687\n",
            "Epoch #21. Time: 6.4s. Train loss: 652.353, train mse: 25.54468. Val loss: 492.948, val mse: 22.10779\n",
            "Epoch #22. Time: 6.5s. Train loss: 593.141, train mse: 24.33809. Val loss: 644.513, val mse: 25.34790\n",
            "Epoch #23. Time: 6.5s. Train loss: 558.793, train mse: 23.61498. Val loss: 534.909, val mse: 22.69660\n",
            "Epoch #24. Time: 6.5s. Train loss: 532.882, train mse: 23.13450. Val loss: 500.108, val mse: 22.58718\n",
            "Epoch #25. Time: 6.5s. Train loss: 478.587, train mse: 21.90970. Val loss: 510.188, val mse: 22.57312\n",
            "Epoch #26. Time: 6.5s. Train loss: 467.188, train mse: 21.59094. Val loss: 603.201, val mse: 23.60518\n",
            "Epoch #27. Time: 6.4s. Train loss: 451.954, train mse: 21.22215. Val loss: 549.221, val mse: 23.80329\n",
            "Epoch #28. Time: 6.5s. Train loss: 438.498, train mse: 20.97474. Val loss: 513.092, val mse: 22.42441\n",
            "Epoch #29. Time: 6.5s. Train loss: 411.269, train mse: 20.24337. Val loss: 561.041, val mse: 23.15334\n",
            "Epoch #30. Time: 6.5s. Train loss: 418.230, train mse: 20.44266. Val loss: 544.182, val mse: 23.12363\n",
            "Epoch #31. Time: 6.4s. Train loss: 431.566, train mse: 20.79056. Val loss: 472.113, val mse: 22.07694\n",
            "Epoch #32. Time: 6.5s. Train loss: 422.390, train mse: 20.53635. Val loss: 693.032, val mse: 25.65548\n",
            "Epoch #33. Time: 6.5s. Train loss: 424.741, train mse: 20.62898. Val loss: 558.209, val mse: 23.55609\n",
            "Epoch #34. Time: 6.5s. Train loss: 436.929, train mse: 20.84216. Val loss: 538.682, val mse: 23.34923\n",
            "Epoch #35. Time: 6.4s. Train loss: 440.662, train mse: 21.02200. Val loss: 513.277, val mse: 22.06452\n",
            "Epoch #36. Time: 6.5s. Train loss: 434.260, train mse: 20.86342. Val loss: 471.395, val mse: 21.78057\n",
            "Epoch #37. Time: 6.5s. Train loss: 398.199, train mse: 19.97751. Val loss: 469.615, val mse: 21.82415\n",
            "Epoch #38. Time: 6.5s. Train loss: 394.132, train mse: 19.88841. Val loss: 481.066, val mse: 22.26400\n",
            "Epoch #39. Time: 6.5s. Train loss: 361.855, train mse: 19.06830. Val loss: 488.807, val mse: 22.10886\n",
            "Epoch #40. Time: 6.5s. Train loss: 348.514, train mse: 18.63270. Val loss: 572.369, val mse: 23.57935\n",
            "Epoch #41. Time: 6.5s. Train loss: 365.048, train mse: 19.15215. Val loss: 484.783, val mse: 21.76396\n",
            "Epoch #42. Time: 6.5s. Train loss: 320.250, train mse: 17.91875. Val loss: 469.597, val mse: 21.73409\n",
            "Epoch #43. Time: 6.5s. Train loss: 310.272, train mse: 17.63930. Val loss: 458.894, val mse: 21.44377\n",
            "Epoch #44. Time: 6.5s. Train loss: 287.298, train mse: 16.91748. Val loss: 439.049, val mse: 20.95254\n",
            "Epoch #45. Time: 6.5s. Train loss: 252.719, train mse: 15.88462. Val loss: 458.149, val mse: 21.62281\n",
            "Epoch #46. Time: 6.5s. Train loss: 256.714, train mse: 15.98066. Val loss: 447.780, val mse: 21.51624\n",
            "Epoch #47. Time: 6.5s. Train loss: 228.632, train mse: 15.13666. Val loss: 476.493, val mse: 21.77268\n",
            "Epoch #48. Time: 6.4s. Train loss: 240.250, train mse: 15.53759. Val loss: 408.093, val mse: 20.48367\n",
            "Epoch #49. Time: 6.5s. Train loss: 220.828, train mse: 14.84787. Val loss: 419.150, val mse: 20.75927\n",
            "Epoch #50. Time: 6.5s. Train loss: 226.154, train mse: 15.05896. Val loss: 481.642, val mse: 21.75938\n",
            "Training done. Best rmse: 20.48366928100586\n",
            "Start #9 fold\n",
            "Epoch #1. Time: 2.7s. Train loss: 6604.821, train mse: 81.34525. Val loss: 6823.393, val mse: 82.83874\n",
            "Epoch #2. Time: 2.7s. Train loss: 5273.052, train mse: 72.66125. Val loss: 5180.589, val mse: 71.01432\n",
            "Epoch #3. Time: 2.7s. Train loss: 2419.399, train mse: 49.26752. Val loss: 2522.924, val mse: 50.18192\n",
            "Epoch #4. Time: 2.7s. Train loss: 1331.206, train mse: 36.52572. Val loss: 1674.590, val mse: 40.52991\n",
            "Epoch #5. Time: 2.7s. Train loss: 1202.480, train mse: 34.65602. Val loss: 1505.282, val mse: 39.17640\n",
            "Epoch #6. Time: 2.7s. Train loss: 1090.834, train mse: 33.04501. Val loss: 1583.732, val mse: 39.37224\n",
            "Epoch #7. Time: 2.7s. Train loss: 1139.088, train mse: 33.75561. Val loss: 1474.394, val mse: 39.12867\n",
            "Epoch #8. Time: 2.7s. Train loss: 1056.448, train mse: 32.50393. Val loss: 1501.612, val mse: 39.06206\n",
            "Epoch #9. Time: 2.7s. Train loss: 1056.100, train mse: 32.53962. Val loss: 1432.396, val mse: 37.86037\n",
            "Epoch #10. Time: 2.7s. Train loss: 1027.409, train mse: 32.09808. Val loss: 1338.990, val mse: 36.76028\n",
            "Epoch #11. Time: 6.5s. Train loss: 1021.630, train mse: 31.99309. Val loss: 1100.041, val mse: 33.68915\n",
            "Epoch #12. Time: 6.5s. Train loss: 1016.881, train mse: 31.88715. Val loss: 1026.780, val mse: 32.36945\n",
            "Epoch #13. Time: 6.5s. Train loss: 979.093, train mse: 31.25206. Val loss: 967.721, val mse: 31.03081\n",
            "Epoch #14. Time: 6.6s. Train loss: 966.906, train mse: 31.10014. Val loss: 937.483, val mse: 30.67269\n",
            "Epoch #15. Time: 6.5s. Train loss: 937.136, train mse: 30.63189. Val loss: 1285.129, val mse: 36.28916\n",
            "Epoch #16. Time: 6.5s. Train loss: 878.600, train mse: 29.60416. Val loss: 1627.382, val mse: 40.18499\n",
            "Epoch #17. Time: 6.4s. Train loss: 804.862, train mse: 28.36266. Val loss: 1254.490, val mse: 35.88781\n",
            "Epoch #18. Time: 6.4s. Train loss: 741.701, train mse: 27.22541. Val loss: 921.840, val mse: 30.23024\n",
            "Epoch #19. Time: 6.5s. Train loss: 696.282, train mse: 26.45073. Val loss: 917.319, val mse: 30.33686\n",
            "Epoch #20. Time: 6.5s. Train loss: 694.731, train mse: 26.27825. Val loss: 975.543, val mse: 31.18307\n",
            "Epoch #21. Time: 6.5s. Train loss: 665.327, train mse: 25.76109. Val loss: 871.611, val mse: 29.46613\n",
            "Epoch #22. Time: 6.5s. Train loss: 621.949, train mse: 24.97311. Val loss: 825.143, val mse: 29.13467\n",
            "Epoch #23. Time: 6.5s. Train loss: 540.781, train mse: 23.23656. Val loss: 883.146, val mse: 29.95557\n",
            "Epoch #24. Time: 6.5s. Train loss: 558.139, train mse: 23.62234. Val loss: 788.376, val mse: 28.50316\n",
            "Epoch #25. Time: 6.5s. Train loss: 481.775, train mse: 21.95860. Val loss: 788.497, val mse: 28.02370\n",
            "Epoch #26. Time: 6.5s. Train loss: 467.692, train mse: 21.65389. Val loss: 756.906, val mse: 28.04143\n",
            "Epoch #27. Time: 6.5s. Train loss: 451.453, train mse: 21.22907. Val loss: 779.868, val mse: 27.18458\n",
            "Epoch #28. Time: 6.4s. Train loss: 448.483, train mse: 21.20548. Val loss: 759.389, val mse: 27.31921\n",
            "Epoch #29. Time: 6.5s. Train loss: 437.496, train mse: 20.87332. Val loss: 753.968, val mse: 27.54011\n",
            "Epoch #30. Time: 6.5s. Train loss: 417.121, train mse: 20.44174. Val loss: 725.102, val mse: 26.86905\n",
            "Epoch #31. Time: 6.5s. Train loss: 417.122, train mse: 20.43606. Val loss: 757.795, val mse: 27.71651\n",
            "Epoch #32. Time: 6.5s. Train loss: 454.545, train mse: 21.32014. Val loss: 744.593, val mse: 27.31147\n",
            "Epoch #33. Time: 6.5s. Train loss: 470.802, train mse: 21.69625. Val loss: 712.160, val mse: 26.81916\n",
            "Epoch #34. Time: 6.4s. Train loss: 475.356, train mse: 21.82739. Val loss: 772.313, val mse: 28.18137\n",
            "Epoch #35. Time: 6.5s. Train loss: 449.241, train mse: 21.25465. Val loss: 813.351, val mse: 28.43738\n",
            "Epoch #36. Time: 6.5s. Train loss: 472.834, train mse: 21.77435. Val loss: 708.545, val mse: 26.77486\n",
            "Epoch #37. Time: 6.5s. Train loss: 408.618, train mse: 20.20185. Val loss: 891.876, val mse: 29.33238\n",
            "Epoch #38. Time: 6.5s. Train loss: 414.710, train mse: 20.35818. Val loss: 747.733, val mse: 27.38535\n",
            "Epoch #39. Time: 6.5s. Train loss: 374.051, train mse: 19.34532. Val loss: 789.435, val mse: 27.89738\n",
            "Epoch #40. Time: 6.5s. Train loss: 354.007, train mse: 18.82975. Val loss: 780.434, val mse: 27.99938\n",
            "Epoch #41. Time: 6.5s. Train loss: 327.137, train mse: 18.09626. Val loss: 754.025, val mse: 27.83774\n",
            "Epoch #42. Time: 6.4s. Train loss: 357.829, train mse: 18.91389. Val loss: 804.341, val mse: 27.70825\n",
            "Epoch #43. Time: 6.5s. Train loss: 273.249, train mse: 16.54427. Val loss: 789.307, val mse: 28.31948\n",
            "Epoch #44. Time: 6.4s. Train loss: 264.500, train mse: 16.26090. Val loss: 734.650, val mse: 27.23550\n",
            "Epoch #45. Time: 6.5s. Train loss: 273.027, train mse: 16.54863. Val loss: 732.192, val mse: 26.88993\n",
            "Epoch #46. Time: 6.5s. Train loss: 264.007, train mse: 16.26918. Val loss: 745.992, val mse: 27.57884\n",
            "Epoch #47. Time: 6.4s. Train loss: 242.050, train mse: 15.54021. Val loss: 723.489, val mse: 27.16866\n",
            "Epoch #48. Time: 6.5s. Train loss: 231.868, train mse: 15.22639. Val loss: 719.498, val mse: 27.03406\n",
            "Epoch #49. Time: 6.4s. Train loss: 229.990, train mse: 15.13720. Val loss: 732.388, val mse: 27.25849\n",
            "Epoch #50. Time: 6.4s. Train loss: 223.523, train mse: 14.97762. Val loss: 761.406, val mse: 27.82453\n",
            "Training done. Best rmse: 26.774864196777344\n",
            "Start #10 fold\n",
            "Epoch #1. Time: 2.7s. Train loss: 6659.297, train mse: 81.64815. Val loss: 6399.436, val mse: 80.24243\n",
            "Epoch #2. Time: 2.7s. Train loss: 5311.326, train mse: 72.92248. Val loss: 4483.542, val mse: 66.96481\n",
            "Epoch #3. Time: 2.8s. Train loss: 2429.022, train mse: 49.35895. Val loss: 1739.908, val mse: 42.45445\n",
            "Epoch #4. Time: 2.7s. Train loss: 1292.394, train mse: 35.94646. Val loss: 1353.727, val mse: 36.75808\n",
            "Epoch #5. Time: 2.7s. Train loss: 1205.059, train mse: 34.65218. Val loss: 1401.383, val mse: 36.45780\n",
            "Epoch #6. Time: 2.7s. Train loss: 1151.523, train mse: 33.89256. Val loss: 1174.822, val mse: 34.91766\n",
            "Epoch #7. Time: 2.7s. Train loss: 1109.835, train mse: 33.33650. Val loss: 1186.144, val mse: 33.75675\n",
            "Epoch #8. Time: 2.7s. Train loss: 1075.786, train mse: 32.76198. Val loss: 1198.202, val mse: 34.67098\n",
            "Epoch #9. Time: 2.7s. Train loss: 1089.517, train mse: 33.01114. Val loss: 1085.938, val mse: 33.05126\n",
            "Epoch #10. Time: 2.7s. Train loss: 1042.144, train mse: 32.25132. Val loss: 1091.236, val mse: 32.83816\n",
            "Epoch #11. Time: 6.6s. Train loss: 1034.914, train mse: 32.14624. Val loss: 1039.301, val mse: 31.69559\n",
            "Epoch #12. Time: 6.4s. Train loss: 984.424, train mse: 31.42124. Val loss: 918.377, val mse: 30.52593\n",
            "Epoch #13. Time: 6.6s. Train loss: 955.818, train mse: 30.89547. Val loss: 1485.780, val mse: 39.31829\n",
            "Epoch #14. Time: 6.5s. Train loss: 1017.271, train mse: 31.89374. Val loss: 1053.473, val mse: 32.02493\n",
            "Epoch #15. Time: 6.6s. Train loss: 904.952, train mse: 30.04313. Val loss: 892.704, val mse: 28.84054\n",
            "Epoch #16. Time: 6.5s. Train loss: 837.467, train mse: 28.92256. Val loss: 2003.365, val mse: 44.44056\n",
            "Epoch #17. Time: 6.5s. Train loss: 792.052, train mse: 28.15192. Val loss: 791.005, val mse: 28.71253\n",
            "Epoch #18. Time: 6.5s. Train loss: 722.619, train mse: 26.86890. Val loss: 842.316, val mse: 29.07255\n",
            "Epoch #19. Time: 6.6s. Train loss: 721.354, train mse: 26.91141. Val loss: 803.624, val mse: 28.55154\n",
            "Epoch #20. Time: 6.5s. Train loss: 672.269, train mse: 25.92215. Val loss: 722.594, val mse: 26.25347\n",
            "Epoch #21. Time: 6.4s. Train loss: 634.766, train mse: 25.22994. Val loss: 643.633, val mse: 25.72061\n",
            "Epoch #22. Time: 6.4s. Train loss: 568.933, train mse: 23.86616. Val loss: 678.347, val mse: 25.82006\n",
            "Epoch #23. Time: 6.5s. Train loss: 563.147, train mse: 23.70777. Val loss: 793.194, val mse: 28.39495\n",
            "Epoch #24. Time: 6.6s. Train loss: 538.975, train mse: 23.21385. Val loss: 844.320, val mse: 28.41946\n",
            "Epoch #25. Time: 6.5s. Train loss: 536.140, train mse: 23.14886. Val loss: 657.396, val mse: 25.33628\n",
            "Epoch #26. Time: 6.5s. Train loss: 465.802, train mse: 21.59381. Val loss: 620.963, val mse: 25.25112\n",
            "Epoch #27. Time: 6.5s. Train loss: 465.522, train mse: 21.51341. Val loss: 748.668, val mse: 26.91546\n",
            "Epoch #28. Time: 6.5s. Train loss: 427.169, train mse: 20.65889. Val loss: 730.579, val mse: 25.35885\n",
            "Epoch #29. Time: 6.5s. Train loss: 445.366, train mse: 21.12054. Val loss: 634.845, val mse: 24.77953\n",
            "Epoch #30. Time: 6.5s. Train loss: 424.139, train mse: 20.61332. Val loss: 650.541, val mse: 25.36079\n",
            "Epoch #31. Time: 6.5s. Train loss: 431.074, train mse: 20.79226. Val loss: 618.489, val mse: 24.99324\n",
            "Epoch #32. Time: 6.5s. Train loss: 419.704, train mse: 20.49444. Val loss: 548.288, val mse: 23.96107\n",
            "Epoch #33. Time: 6.6s. Train loss: 443.158, train mse: 20.99187. Val loss: 631.095, val mse: 25.10766\n",
            "Epoch #34. Time: 6.6s. Train loss: 478.558, train mse: 21.90858. Val loss: 592.809, val mse: 24.12197\n",
            "Epoch #35. Time: 6.5s. Train loss: 464.252, train mse: 21.55606. Val loss: 666.534, val mse: 25.88608\n",
            "Epoch #36. Time: 6.5s. Train loss: 449.450, train mse: 21.22882. Val loss: 602.980, val mse: 24.57887\n",
            "Epoch #37. Time: 6.4s. Train loss: 434.556, train mse: 20.81299. Val loss: 691.950, val mse: 26.28983\n",
            "Epoch #38. Time: 6.5s. Train loss: 364.829, train mse: 18.98134. Val loss: 625.092, val mse: 25.22421\n",
            "Epoch #39. Time: 6.5s. Train loss: 357.845, train mse: 18.90648. Val loss: 560.023, val mse: 24.10144\n",
            "Epoch #40. Time: 6.5s. Train loss: 354.203, train mse: 18.83374. Val loss: 768.651, val mse: 27.98721\n",
            "Epoch #41. Time: 6.5s. Train loss: 335.506, train mse: 18.35830. Val loss: 596.882, val mse: 24.61455\n",
            "Epoch #42. Time: 6.5s. Train loss: 329.306, train mse: 18.15863. Val loss: 628.321, val mse: 25.47192\n",
            "Epoch #43. Time: 6.4s. Train loss: 301.522, train mse: 17.37143. Val loss: 601.613, val mse: 24.88554\n",
            "Epoch #44. Time: 6.5s. Train loss: 277.233, train mse: 16.65738. Val loss: 585.778, val mse: 24.41362\n",
            "Epoch #45. Time: 6.4s. Train loss: 266.166, train mse: 16.29994. Val loss: 572.194, val mse: 24.16807\n",
            "Epoch #46. Time: 6.5s. Train loss: 253.774, train mse: 15.93336. Val loss: 601.914, val mse: 25.07788\n",
            "Epoch #47. Time: 6.5s. Train loss: 261.117, train mse: 16.18054. Val loss: 601.947, val mse: 24.79375\n",
            "Epoch #48. Time: 6.5s. Train loss: 253.192, train mse: 15.85334. Val loss: 662.242, val mse: 24.89622\n",
            "Epoch #49. Time: 6.4s. Train loss: 249.534, train mse: 15.80850. Val loss: 567.810, val mse: 24.09967\n",
            "Epoch #50. Time: 6.4s. Train loss: 230.387, train mse: 15.10848. Val loss: 630.710, val mse: 24.80555\n",
            "Training done. Best rmse: 23.961074829101562\n",
            "Total time: 48.202504845460254m\n",
            "Total rmse: 22.576969146728516 +- 2.077378988265991\n",
            "[24.100998, 20.308279, 21.424421, 24.04542, 20.269598, 23.356873, 21.044483, 20.48367, 26.774864, 23.961075]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqBA-gsfvNih"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}