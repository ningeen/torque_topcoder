{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exp1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHgLZpLpo3Wj"
      },
      "source": [
        "# from google.colab import drive\r\n",
        "# drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP4BT5QEo7Ot",
        "outputId": "8526cffe-3cae-46de-fc68-aa6eab49445c"
      },
      "source": [
        "%cd /content/drive/MyDrive/topcoder"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/topcoder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pglu-X_4paow"
      },
      "source": [
        "# Новый раздел"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jp6My88p8on"
      },
      "source": [
        "import pickle\r\n",
        "import yaml\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import os\r\n",
        "import random\r\n",
        "import time\r\n",
        "\r\n",
        "import torch\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from torch import nn\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dbn0AJYuq_2Q"
      },
      "source": [
        "import math\r\n",
        "\r\n",
        "\r\n",
        "__all__ = ['mobilenetv3_large', 'mobilenetv3_small']\r\n",
        "\r\n",
        "\r\n",
        "def _make_divisible(v, divisor, min_value=None):\r\n",
        "    \"\"\"\r\n",
        "    This function is taken from the original tf repo.\r\n",
        "    It ensures that all layers have a channel number that is divisible by 8\r\n",
        "    It can be seen here:\r\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\r\n",
        "    :param v:\r\n",
        "    :param divisor:\r\n",
        "    :param min_value:\r\n",
        "    :return:\r\n",
        "    \"\"\"\r\n",
        "    if min_value is None:\r\n",
        "        min_value = divisor\r\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\r\n",
        "    # Make sure that round down does not go down by more than 10%.\r\n",
        "    if new_v < 0.9 * v:\r\n",
        "        new_v += divisor\r\n",
        "    return new_v\r\n",
        "\r\n",
        "\r\n",
        "class h_sigmoid(nn.Module):\r\n",
        "    def __init__(self, inplace=True):\r\n",
        "        super(h_sigmoid, self).__init__()\r\n",
        "        self.relu = nn.ReLU6(inplace=inplace)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return self.relu(x + 3) / 6\r\n",
        "\r\n",
        "\r\n",
        "class h_swish(nn.Module):\r\n",
        "    def __init__(self, inplace=True):\r\n",
        "        super(h_swish, self).__init__()\r\n",
        "        self.sigmoid = h_sigmoid(inplace=inplace)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return x * self.sigmoid(x)\r\n",
        "\r\n",
        "\r\n",
        "class SELayer(nn.Module):\r\n",
        "    def __init__(self, channel, reduction=4):\r\n",
        "        super(SELayer, self).__init__()\r\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\r\n",
        "        self.fc = nn.Sequential(\r\n",
        "                nn.Linear(channel, _make_divisible(channel // reduction, 8)),\r\n",
        "                nn.ReLU(inplace=True),\r\n",
        "                nn.Linear(_make_divisible(channel // reduction, 8), channel),\r\n",
        "                h_sigmoid()\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        b, c, _, _ = x.size()\r\n",
        "        y = self.avg_pool(x).view(b, c)\r\n",
        "        y = self.fc(y).view(b, c, 1, 1)\r\n",
        "        return x * y\r\n",
        "\r\n",
        "\r\n",
        "def conv_3x3_bn(inp, oup, stride):\r\n",
        "    return nn.Sequential(\r\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\r\n",
        "        nn.BatchNorm2d(oup),\r\n",
        "        h_swish()\r\n",
        "    )\r\n",
        "\r\n",
        "\r\n",
        "def conv_1x1_bn(inp, oup):\r\n",
        "    return nn.Sequential(\r\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\r\n",
        "        nn.BatchNorm2d(oup),\r\n",
        "        h_swish()\r\n",
        "    )\r\n",
        "\r\n",
        "\r\n",
        "class InvertedResidual(nn.Module):\r\n",
        "    def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se, use_hs):\r\n",
        "        super(InvertedResidual, self).__init__()\r\n",
        "        assert stride in [1, 2]\r\n",
        "\r\n",
        "        self.identity = stride == 1 and inp == oup\r\n",
        "\r\n",
        "        if inp == hidden_dim:\r\n",
        "            self.conv = nn.Sequential(\r\n",
        "                # dw\r\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\r\n",
        "                nn.BatchNorm2d(hidden_dim),\r\n",
        "                h_swish() if use_hs else nn.ReLU(inplace=True),\r\n",
        "                # Squeeze-and-Excite\r\n",
        "                SELayer(hidden_dim) if use_se else nn.Identity(),\r\n",
        "                # pw-linear\r\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\r\n",
        "                nn.BatchNorm2d(oup),\r\n",
        "            )\r\n",
        "        else:\r\n",
        "            self.conv = nn.Sequential(\r\n",
        "                # pw\r\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\r\n",
        "                nn.BatchNorm2d(hidden_dim),\r\n",
        "                h_swish() if use_hs else nn.ReLU(inplace=True),\r\n",
        "                # dw\r\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\r\n",
        "                nn.BatchNorm2d(hidden_dim),\r\n",
        "                # Squeeze-and-Excite\r\n",
        "                SELayer(hidden_dim) if use_se else nn.Identity(),\r\n",
        "                h_swish() if use_hs else nn.ReLU(inplace=True),\r\n",
        "                # pw-linear\r\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\r\n",
        "                nn.BatchNorm2d(oup),\r\n",
        "            )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        if self.identity:\r\n",
        "            return x + self.conv(x)\r\n",
        "        else:\r\n",
        "            return self.conv(x)\r\n",
        "\r\n",
        "\r\n",
        "class MobileNetV3(nn.Module):\r\n",
        "    def __init__(self, cfgs, mode, num_classes=1000, width_mult=1.):\r\n",
        "        super(MobileNetV3, self).__init__()\r\n",
        "        # setting of inverted residual blocks\r\n",
        "        self.cfgs = cfgs\r\n",
        "        assert mode in ['large', 'small']\r\n",
        "\r\n",
        "        # building first layer\r\n",
        "        input_channel = _make_divisible(16 * width_mult, 8)\r\n",
        "        layers = [conv_3x3_bn(3, input_channel, 2)]\r\n",
        "        # building inverted residual blocks\r\n",
        "        block = InvertedResidual\r\n",
        "        for k, t, c, use_se, use_hs, s in self.cfgs:\r\n",
        "            output_channel = _make_divisible(c * width_mult, 8)\r\n",
        "            exp_size = _make_divisible(input_channel * t, 8)\r\n",
        "            layers.append(block(input_channel, exp_size, output_channel, k, s, use_se, use_hs))\r\n",
        "            input_channel = output_channel\r\n",
        "        self.features = nn.Sequential(*layers)\r\n",
        "        # building last several layers\r\n",
        "        self.conv = conv_1x1_bn(input_channel, exp_size)\r\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\r\n",
        "        output_channel = {'large': 1280, 'small': 1024}\r\n",
        "        output_channel = _make_divisible(output_channel[mode] * width_mult, 8) if width_mult > 1.0 else output_channel[mode]\r\n",
        "        self.classifier = nn.Sequential(\r\n",
        "            nn.Linear(exp_size, output_channel),\r\n",
        "            h_swish(),\r\n",
        "            nn.Dropout(0.2),\r\n",
        "            nn.Linear(output_channel, num_classes),\r\n",
        "        )\r\n",
        "\r\n",
        "        self._initialize_weights()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.features(x)\r\n",
        "        x = self.conv(x)\r\n",
        "        x = self.avgpool(x)\r\n",
        "        x = x.view(x.size(0), -1)\r\n",
        "        x = self.classifier(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "    def _initialize_weights(self):\r\n",
        "        for m in self.modules():\r\n",
        "            if isinstance(m, nn.Conv2d):\r\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\r\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\r\n",
        "                if m.bias is not None:\r\n",
        "                    m.bias.data.zero_()\r\n",
        "            elif isinstance(m, nn.BatchNorm2d):\r\n",
        "                m.weight.data.fill_(1)\r\n",
        "                m.bias.data.zero_()\r\n",
        "            elif isinstance(m, nn.Linear):\r\n",
        "                n = m.weight.size(1)\r\n",
        "                m.weight.data.normal_(0, 0.01)\r\n",
        "                m.bias.data.zero_()\r\n",
        "\r\n",
        "\r\n",
        "def mobilenetv3_large(**kwargs):\r\n",
        "    \"\"\"\r\n",
        "    Constructs a MobileNetV3-Large model\r\n",
        "    \"\"\"\r\n",
        "    cfgs = [\r\n",
        "        # k, t, c, SE, HS, s \r\n",
        "        [3,   1,  16, 0, 0, 1],\r\n",
        "        [3,   4,  24, 0, 0, 2],\r\n",
        "        [3,   3,  24, 0, 0, 1],\r\n",
        "        [5,   3,  40, 1, 0, 2],\r\n",
        "        [5,   3,  40, 1, 0, 1],\r\n",
        "        [5,   3,  40, 1, 0, 1],\r\n",
        "        [3,   6,  80, 0, 1, 2],\r\n",
        "        [3, 2.5,  80, 0, 1, 1],\r\n",
        "        [3, 2.3,  80, 0, 1, 1],\r\n",
        "        [3, 2.3,  80, 0, 1, 1],\r\n",
        "        [3,   6, 112, 1, 1, 1],\r\n",
        "        [3,   6, 112, 1, 1, 1],\r\n",
        "        [5,   6, 160, 1, 1, 2],\r\n",
        "        [5,   6, 160, 1, 1, 1],\r\n",
        "        [5,   6, 160, 1, 1, 1]\r\n",
        "    ]\r\n",
        "    return MobileNetV3(cfgs, mode='large', **kwargs)\r\n",
        "\r\n",
        "\r\n",
        "def mobilenetv3_small(**kwargs):\r\n",
        "    \"\"\"\r\n",
        "    Constructs a MobileNetV3-Small model\r\n",
        "    \"\"\"\r\n",
        "    cfgs = [\r\n",
        "        # k, t, c, SE, HS, s \r\n",
        "        [3,    1,  16, 1, 0, 2],\r\n",
        "        [3,  4.5,  24, 0, 0, 2],\r\n",
        "        [3, 3.67,  24, 0, 0, 1],\r\n",
        "        [5,    4,  40, 1, 1, 2],\r\n",
        "        [5,    6,  40, 1, 1, 1],\r\n",
        "        [5,    6,  40, 1, 1, 1],\r\n",
        "        [5,    3,  48, 1, 1, 1],\r\n",
        "        [5,    3,  48, 1, 1, 1],\r\n",
        "        [5,    6,  96, 1, 1, 2],\r\n",
        "        [5,    6,  96, 1, 1, 1],\r\n",
        "        [5,    6,  96, 1, 1, 1],\r\n",
        "    ]\r\n",
        "\r\n",
        "    return MobileNetV3(cfgs, mode='small', **kwargs)\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13rtbK2h8sCE"
      },
      "source": [
        "import math\r\n",
        "import torch\r\n",
        "from torch.optim.lr_scheduler import _LRScheduler\r\n",
        "\r\n",
        "class CosineAnnealingWarmupRestarts(_LRScheduler):\r\n",
        "    \"\"\"\r\n",
        "        optimizer (Optimizer): Wrapped optimizer.\r\n",
        "        first_cycle_steps (int): First cycle step size.\r\n",
        "        cycle_mult(float): Cycle steps magnification. Default: -1.\r\n",
        "        max_lr(float): First cycle's max learning rate. Default: 0.1.\r\n",
        "        min_lr(float): Min learning rate. Default: 0.001.\r\n",
        "        warmup_steps(int): Linear warmup step size. Default: 0.\r\n",
        "        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\r\n",
        "        last_epoch (int): The index of last epoch. Default: -1.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    def __init__(self,\r\n",
        "                 optimizer : torch.optim.Optimizer,\r\n",
        "                 first_cycle_steps : int,\r\n",
        "                 cycle_mult : float = 1.,\r\n",
        "                 max_lr : float = 0.1,\r\n",
        "                 min_lr : float = 0.001,\r\n",
        "                 warmup_steps : int = 0,\r\n",
        "                 gamma : float = 1.,\r\n",
        "                 last_epoch : int = -1\r\n",
        "        ):\r\n",
        "        assert warmup_steps < first_cycle_steps\r\n",
        "        \r\n",
        "        self.first_cycle_steps = first_cycle_steps # first cycle step size\r\n",
        "        self.cycle_mult = cycle_mult # cycle steps magnification\r\n",
        "        self.base_max_lr = max_lr # first max learning rate\r\n",
        "        self.max_lr = max_lr # max learning rate in the current cycle\r\n",
        "        self.min_lr = min_lr # min learning rate\r\n",
        "        self.warmup_steps = warmup_steps # warmup step size\r\n",
        "        self.gamma = gamma # decrease rate of max learning rate by cycle\r\n",
        "        \r\n",
        "        self.cur_cycle_steps = first_cycle_steps # first cycle step size\r\n",
        "        self.cycle = 0 # cycle count\r\n",
        "        self.step_in_cycle = last_epoch # step size of the current cycle\r\n",
        "        \r\n",
        "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\r\n",
        "        \r\n",
        "        # set learning rate min_lr\r\n",
        "        self.init_lr()\r\n",
        "    \r\n",
        "    def init_lr(self):\r\n",
        "        self.base_lrs = []\r\n",
        "        for param_group in self.optimizer.param_groups:\r\n",
        "            param_group['lr'] = self.min_lr\r\n",
        "            self.base_lrs.append(self.min_lr)\r\n",
        "    \r\n",
        "    def get_lr(self):\r\n",
        "        if self.step_in_cycle == -1:\r\n",
        "            return self.base_lrs\r\n",
        "        elif self.step_in_cycle < self.warmup_steps:\r\n",
        "            return [(self.max_lr - base_lr)*self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\r\n",
        "        else:\r\n",
        "            return [base_lr + (self.max_lr - base_lr) \\\r\n",
        "                    * (1 + math.cos(math.pi * (self.step_in_cycle-self.warmup_steps) \\\r\n",
        "                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\r\n",
        "                    for base_lr in self.base_lrs]\r\n",
        "\r\n",
        "    def step(self, epoch=None):\r\n",
        "        if epoch is None:\r\n",
        "            epoch = self.last_epoch + 1\r\n",
        "            self.step_in_cycle = self.step_in_cycle + 1\r\n",
        "            if self.step_in_cycle >= self.cur_cycle_steps:\r\n",
        "                self.cycle += 1\r\n",
        "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\r\n",
        "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\r\n",
        "        else:\r\n",
        "            if epoch >= self.first_cycle_steps:\r\n",
        "                if self.cycle_mult == 1.:\r\n",
        "                    self.step_in_cycle = epoch % self.first_cycle_steps\r\n",
        "                    self.cycle = epoch // self.first_cycle_steps\r\n",
        "                else:\r\n",
        "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\r\n",
        "                    self.cycle = n\r\n",
        "                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\r\n",
        "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\r\n",
        "            else:\r\n",
        "                self.cur_cycle_steps = self.first_cycle_steps\r\n",
        "                self.step_in_cycle = epoch\r\n",
        "                \r\n",
        "        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\r\n",
        "        self.last_epoch = math.floor(epoch)\r\n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\r\n",
        "            param_group['lr'] = lr"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4ElywIvqA6K"
      },
      "source": [
        "CONFIG_PATH = \"proj_config.yaml\"\r\n",
        "with open(CONFIG_PATH, 'r') as stream:\r\n",
        "    CONFIG = yaml.safe_load(stream)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP7AkAj8MAsZ"
      },
      "source": [
        "def spec_augment(spec: np.ndarray, num_mask=2, freq_masking_max_percentage=0.05, time_masking_max_percentage=0.1):\r\n",
        "    spec = spec.copy()\r\n",
        "    for i in range(num_mask):\r\n",
        "        num_freqs, num_frames = spec.shape\r\n",
        "        freq_percentage = random.uniform(0.0, freq_masking_max_percentage)\r\n",
        "        time_percentage = random.uniform(0.0, time_masking_max_percentage)\r\n",
        "        \r\n",
        "        num_freqs_to_mask = int(freq_percentage * num_freqs)\r\n",
        "        num_frames_to_mask = int(time_percentage * num_frames)\r\n",
        "        \r\n",
        "        t0 = int(np.random.uniform(low=0.0, high=num_frames - num_frames_to_mask))\r\n",
        "        f0 = int(np.random.uniform(low=0.0, high=num_freqs - num_freqs_to_mask))\r\n",
        "        \r\n",
        "        spec[:, t0:t0 + num_frames_to_mask] = 0      \r\n",
        "        spec[f0:f0 + num_freqs_to_mask, :] = 0 \r\n",
        "        \r\n",
        "    return spec"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G43KN85lp4D0"
      },
      "source": [
        "def uniform_len(mel, input_len):\r\n",
        "    mel_len = mel.shape[-1]\r\n",
        "    if mel_len > input_len:\r\n",
        "        diff = mel_len - input_len\r\n",
        "        start = np.random.randint(diff)\r\n",
        "        end = start + input_len\r\n",
        "        mel = mel[:, start: end]\r\n",
        "    elif mel_len < input_len:\r\n",
        "        diff = input_len - mel_len\r\n",
        "        offset = np.random.randint(diff)\r\n",
        "        offset_right = diff - offset\r\n",
        "        mel = np.pad(\r\n",
        "            mel,\r\n",
        "            ((0, 0), (offset, offset_right)),\r\n",
        "            \"symmetric\",  # constant\r\n",
        "        )\r\n",
        "    return mel\r\n",
        "\r\n",
        "\r\n",
        "class TorqueDataset(Dataset):\r\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, data, mel_logs, labels=None, transform=None):\r\n",
        "        \"\"\"Init Dataset\"\"\"\r\n",
        "        self.mel_logs = mel_logs\r\n",
        "        self.data = data\r\n",
        "        self.labels = labels\r\n",
        "        self.transform = transform\r\n",
        "        self.input_len = CONFIG['mel']['mel_len']\r\n",
        "        self.mode = 'test' if self.labels is None else 'train'\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        \"\"\"Length\"\"\"\r\n",
        "        return len(self.mel_logs)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        \"\"\"Generates one sample of data\"\"\"\r\n",
        "        table_data = self.data[[index]]\r\n",
        "\r\n",
        "        label = None\r\n",
        "        if self.mode == 'train':\r\n",
        "            label = self.labels[[index]]\r\n",
        "\r\n",
        "        mel_data = uniform_len(self.mel_logs[index], self.input_len)\r\n",
        "        if self.transform and self.mode == 'train':\r\n",
        "            mel_data = self.transform(mel_data)\r\n",
        "\r\n",
        "        mel_data = np.expand_dims(mel_data, axis=0)\r\n",
        "        return mel_data, label"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdOpMc7Rp-AZ"
      },
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "def seed_everything(seed=1234):\r\n",
        "    \"\"\"Fix random seeds\"\"\"\r\n",
        "    random.seed(seed)\r\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\r\n",
        "    np.random.seed(seed)\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    torch.cuda.manual_seed(seed)\r\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vJdztTNqiv0"
      },
      "source": [
        "def get_model(pretrained_mn3_path=\"\", pretrained_path=\"\"):\r\n",
        "    \"\"\"Load MobilenetV3 model with specified in and out channels\"\"\"\r\n",
        "    # model = mobilenetv3_small().to(DEVICE)\r\n",
        "    model = mobilenetv3_large().to(DEVICE)\r\n",
        "    if pretrained_mn3_path and not pretrained_path:\r\n",
        "        model.load_state_dict(torch.load(pretrained_mn3_path))\r\n",
        "\r\n",
        "    model.features[0][0].weight.data = torch.sum(\r\n",
        "        model.features[0][0].weight.data, dim=1, keepdim=True\r\n",
        "    )\r\n",
        "    model.features[0][0].in_channels = 1\r\n",
        "\r\n",
        "    model.classifier[-1].weight.data = torch.sum(\r\n",
        "        model.classifier[-1].weight.data, dim=0, keepdim=True\r\n",
        "    )\r\n",
        "\r\n",
        "    model.classifier[-1].bias.data = torch.sum(\r\n",
        "        model.classifier[-1].bias.data, dim=0, keepdim=True\r\n",
        "    )\r\n",
        "    model.classifier[-1].out_features = 1\r\n",
        "\r\n",
        "    if pretrained_path:\r\n",
        "        model.load_state_dict(torch.load(pretrained_path))\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "def process_epoch(model, criterion, optimizer, loader):\r\n",
        "    \"\"\"Calc one epoch\"\"\"\r\n",
        "    losses = []\r\n",
        "    y_true = []\r\n",
        "    y_pred = []\r\n",
        "    with torch.set_grad_enabled(model.training):\r\n",
        "        for local_batch, local_labels in loader:\r\n",
        "            local_batch, local_labels = \\\r\n",
        "                local_batch.to(DEVICE), local_labels.to(DEVICE)\r\n",
        "\r\n",
        "            optimizer.zero_grad()\r\n",
        "            outputs = model(local_batch)\r\n",
        "\r\n",
        "            loss = criterion(outputs, local_labels)\r\n",
        "            if model.training:\r\n",
        "                loss.backward()\r\n",
        "                optimizer.step()\r\n",
        "\r\n",
        "            losses.append(loss)\r\n",
        "            y_true.append(local_labels.detach().cpu().numpy())\r\n",
        "            y_pred.append(outputs.data.detach().cpu().numpy())\r\n",
        "    loss_train = np.array(losses).astype(np.float32).mean()\r\n",
        "    y_true = np.concatenate(y_true)\r\n",
        "    y_pred = np.concatenate(y_pred)\r\n",
        "    rmse_train = mean_squared_error(y_true, y_pred, squared=False)\r\n",
        "    return loss_train, rmse_train, y_true, y_pred"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT95qJpQqlu9"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, train_loader, test_loader, n_fold):\r\n",
        "    \"\"\"Training loop\"\"\"\r\n",
        "    logs = {'loss_train': [], 'loss_val': [], 'mse_train': [], 'mse_val': []}\r\n",
        "    best_true = None\r\n",
        "    best_pred = None\r\n",
        "    for epoch in range(CONFIG['num_epochs']):\r\n",
        "        start_time = time.time()\r\n",
        "        scheduler.step()\r\n",
        "\r\n",
        "        # Training\r\n",
        "        model.train()\r\n",
        "        loss_train, mse_train, _, _ = \\\r\n",
        "            process_epoch(model, criterion, optimizer, train_loader)\r\n",
        "        logs['loss_train'].append(loss_train)\r\n",
        "        logs['mse_train'].append(mse_train)\r\n",
        "\r\n",
        "        # Validation\r\n",
        "        model.eval()\r\n",
        "        loss_val, mse_val, y_true, y_pred = \\\r\n",
        "            process_epoch(model, criterion, optimizer, test_loader)\r\n",
        "        logs['loss_val'].append(loss_val)\r\n",
        "        logs['mse_val'].append(mse_val)\r\n",
        "        print(\r\n",
        "            f\"Epoch #{epoch + 1}. \"\r\n",
        "            f\"Time: {(time.time() - start_time):.1f}s. \"\r\n",
        "            f\"Train loss: {loss_train:.3f}, train mse: {mse_train:.5f}. \"\r\n",
        "            f\"Val loss: {loss_val:.3f}, val mse: {mse_val:.5f}\"\r\n",
        "        )\r\n",
        "        if mse_val <= np.min(logs['mse_val']):\r\n",
        "            if CONFIG['save_model']:\r\n",
        "                torch.save(\r\n",
        "                    model.state_dict(),\r\n",
        "                    os.path.join(\r\n",
        "                        CONFIG['model_dir'],\r\n",
        "                        f\"work_{CONFIG['experiment_name']}_fold{n_fold}.pt\"\r\n",
        "                    )\r\n",
        "                )\r\n",
        "            best_true = y_true\r\n",
        "            best_pred = y_pred\r\n",
        "    return best_true, best_pred\r\n",
        "\r\n",
        "\r\n",
        "def run_training():\r\n",
        "    with open(CONFIG['data_path'], 'rb') as f:\r\n",
        "        (data, mel_logs, target) = pickle.load(f)\r\n",
        "\r\n",
        "    folds = KFold(\r\n",
        "        n_splits=CONFIG['n_folds'],\r\n",
        "        shuffle=True,\r\n",
        "        random_state=CONFIG['fold_seed']\r\n",
        "    )\r\n",
        "    splits = list(folds.split(mel_logs))\r\n",
        "\r\n",
        "    total_rmse = list()\r\n",
        "\r\n",
        "    for n_fold, (train_idx, val_idx) in enumerate(splits):\r\n",
        "        print(f\"Start #{n_fold + 1} fold\")\r\n",
        "        train_dataset = TorqueDataset(\r\n",
        "            data[train_idx],\r\n",
        "            [mel_logs[i] for i in train_idx],\r\n",
        "            target[train_idx],\r\n",
        "            transform=spec_augment\r\n",
        "        )\r\n",
        "        val_dataset = TorqueDataset(\r\n",
        "            data[val_idx],\r\n",
        "            [mel_logs[i] for i in val_idx],\r\n",
        "            target[val_idx]\r\n",
        "        )\r\n",
        "        train_loader = DataLoader(train_dataset, **CONFIG['loader_params'])\r\n",
        "        val_loader = DataLoader(val_dataset, **CONFIG['loader_params'])\r\n",
        "\r\n",
        "        model = get_model(CONFIG['pretrained_path'])\r\n",
        "        criterion = nn.MSELoss()\r\n",
        "        optimizer = torch.optim.Adam(model.parameters(), CONFIG['lr'])\r\n",
        "        scheduler = CosineAnnealingWarmupRestarts(optimizer, **CONFIG['scheduler_params'])\r\n",
        "\r\n",
        "        best_true, best_pred = \\\r\n",
        "            train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, n_fold)\r\n",
        "\r\n",
        "        rmse = mean_squared_error(best_true, best_pred, squared=False)\r\n",
        "        print(f\"Training done. Best rmse: {rmse}\")\r\n",
        "        total_rmse.append(rmse)\r\n",
        "    print(f\"Total rmse: {np.mean(total_rmse)}\")\r\n",
        "    print(total_rmse)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1aNynqjtHPj"
      },
      "source": [
        "seed_everything()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiLbhqtFtEMt"
      },
      "source": [
        "CONFIG['loader_params'] = {'batch_size': 16, 'shuffle': True, 'num_workers': 4}\r\n",
        "CONFIG['lr'] = 0.0001\r\n",
        "CONFIG['num_epochs'] = 40\r\n",
        "\r\n",
        "CONFIG['pretrained_path'] = './pretrained/mobilenetv3-large-1cd25616.pth'\r\n",
        "\r\n",
        "CONFIG['scheduler_params'] = {'first_cycle_steps':20,\r\n",
        "                            'cycle_mult':1.0,\r\n",
        "                            'max_lr':CONFIG['lr'] * 6,\r\n",
        "                            'min_lr':CONFIG['lr'] / 8,\r\n",
        "                            'warmup_steps':5,\r\n",
        "                            'gamma':0.9}\r\n",
        "\r\n",
        "CONFIG['experiment_name'] = 'one_cosine'\r\n",
        "\r\n",
        "CONFIG['freq_masking_max_percentage'] = 0.15\r\n",
        "CONFIG['time_masking_max_percentage'] = 0"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jM8j3o0mMK7",
        "outputId": "9b7bdec9-43ad-4bec-94ac-3cbea9efe79b"
      },
      "source": [
        "run_training() \r\n",
        "\r\n",
        "# Total rmse: 22.120615005493164\r\n",
        "# [22.944857, 20.443523, 19.873909, 23.853697, 20.806335, 21.964138, 20.037605, 22.54679, 24.713093, 24.0222]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start #1 fold\n",
            "Epoch #1. Time: 6.4s. Train loss: 5768.606, train mse: 76.07491. Val loss: 4435.967, val mse: 66.72330\n",
            "Epoch #2. Time: 6.4s. Train loss: 1252.637, train mse: 35.36698. Val loss: 847.327, val mse: 29.37916\n",
            "Epoch #3. Time: 6.5s. Train loss: 874.189, train mse: 29.50181. Val loss: 833.632, val mse: 28.86547\n",
            "Epoch #4. Time: 6.4s. Train loss: 870.914, train mse: 29.52761. Val loss: 816.574, val mse: 28.10573\n",
            "Epoch #5. Time: 6.4s. Train loss: 823.081, train mse: 28.69410. Val loss: 800.026, val mse: 28.13097\n",
            "Epoch #6. Time: 6.4s. Train loss: 772.562, train mse: 27.78125. Val loss: 810.655, val mse: 28.92309\n",
            "Epoch #7. Time: 6.4s. Train loss: 754.283, train mse: 27.51840. Val loss: 761.099, val mse: 27.84222\n",
            "Epoch #8. Time: 6.4s. Train loss: 743.080, train mse: 27.24125. Val loss: 733.774, val mse: 26.69363\n",
            "Epoch #9. Time: 6.4s. Train loss: 695.480, train mse: 26.33726. Val loss: 697.167, val mse: 26.49322\n",
            "Epoch #10. Time: 6.3s. Train loss: 637.381, train mse: 25.25855. Val loss: 737.799, val mse: 26.72040\n",
            "Epoch #11. Time: 6.4s. Train loss: 604.220, train mse: 24.58105. Val loss: 861.245, val mse: 29.08095\n",
            "Epoch #12. Time: 6.4s. Train loss: 531.185, train mse: 23.09309. Val loss: 759.363, val mse: 27.57316\n",
            "Epoch #13. Time: 6.4s. Train loss: 509.957, train mse: 22.60378. Val loss: 687.510, val mse: 26.08734\n",
            "Epoch #14. Time: 6.3s. Train loss: 467.851, train mse: 21.64255. Val loss: 638.310, val mse: 24.83274\n",
            "Epoch #15. Time: 6.3s. Train loss: 442.253, train mse: 21.00555. Val loss: 646.066, val mse: 25.35261\n",
            "Epoch #16. Time: 6.4s. Train loss: 429.289, train mse: 20.77442. Val loss: 622.013, val mse: 24.83653\n",
            "Epoch #17. Time: 6.4s. Train loss: 385.294, train mse: 19.60812. Val loss: 629.524, val mse: 24.51899\n",
            "Epoch #18. Time: 6.4s. Train loss: 380.959, train mse: 19.55012. Val loss: 559.584, val mse: 23.99866\n",
            "Epoch #19. Time: 6.4s. Train loss: 357.593, train mse: 18.94718. Val loss: 653.177, val mse: 24.37553\n",
            "Epoch #20. Time: 6.4s. Train loss: 370.991, train mse: 19.27478. Val loss: 607.283, val mse: 24.49774\n",
            "Epoch #21. Time: 6.4s. Train loss: 378.739, train mse: 19.48862. Val loss: 603.973, val mse: 24.77717\n",
            "Epoch #22. Time: 6.4s. Train loss: 381.466, train mse: 19.43127. Val loss: 658.972, val mse: 25.84198\n",
            "Epoch #23. Time: 6.3s. Train loss: 397.939, train mse: 19.88876. Val loss: 619.227, val mse: 24.63221\n",
            "Epoch #24. Time: 6.3s. Train loss: 433.434, train mse: 20.83856. Val loss: 606.135, val mse: 24.72992\n",
            "Epoch #25. Time: 6.4s. Train loss: 431.811, train mse: 20.71051. Val loss: 629.595, val mse: 25.23169\n",
            "Epoch #26. Time: 6.4s. Train loss: 447.065, train mse: 21.13747. Val loss: 606.818, val mse: 24.67459\n",
            "Epoch #27. Time: 6.3s. Train loss: 400.396, train mse: 20.03638. Val loss: 693.838, val mse: 26.55412\n",
            "Epoch #28. Time: 6.4s. Train loss: 420.888, train mse: 20.52527. Val loss: 589.439, val mse: 24.35341\n",
            "Epoch #29. Time: 6.3s. Train loss: 362.644, train mse: 19.06796. Val loss: 683.147, val mse: 26.42568\n",
            "Epoch #30. Time: 6.3s. Train loss: 371.459, train mse: 19.29795. Val loss: 560.038, val mse: 23.87715\n",
            "Epoch #31. Time: 6.4s. Train loss: 333.430, train mse: 18.20049. Val loss: 700.495, val mse: 25.03582\n",
            "Epoch #32. Time: 6.4s. Train loss: 340.016, train mse: 18.31198. Val loss: 618.462, val mse: 25.19074\n",
            "Epoch #33. Time: 6.3s. Train loss: 305.196, train mse: 17.44907. Val loss: 556.555, val mse: 23.96333\n",
            "Epoch #34. Time: 6.4s. Train loss: 252.146, train mse: 15.88099. Val loss: 553.159, val mse: 23.84670\n",
            "Epoch #35. Time: 6.4s. Train loss: 237.509, train mse: 15.44206. Val loss: 533.465, val mse: 23.52525\n",
            "Epoch #36. Time: 6.4s. Train loss: 230.189, train mse: 15.08358. Val loss: 533.507, val mse: 23.36334\n",
            "Epoch #37. Time: 6.4s. Train loss: 220.411, train mse: 14.84086. Val loss: 591.166, val mse: 24.56107\n",
            "Epoch #38. Time: 6.4s. Train loss: 207.046, train mse: 14.40077. Val loss: 594.081, val mse: 24.74224\n",
            "Epoch #39. Time: 6.4s. Train loss: 210.130, train mse: 14.47286. Val loss: 572.614, val mse: 23.94780\n",
            "Epoch #40. Time: 6.4s. Train loss: 200.583, train mse: 14.17105. Val loss: 535.730, val mse: 22.94486\n",
            "Training done. Best rmse: 22.944856643676758\n",
            "Start #2 fold\n",
            "Epoch #1. Time: 6.4s. Train loss: 5798.298, train mse: 76.22243. Val loss: 3992.046, val mse: 63.21988\n",
            "Epoch #2. Time: 6.4s. Train loss: 1321.589, train mse: 36.39589. Val loss: 773.928, val mse: 27.95192\n",
            "Epoch #3. Time: 6.4s. Train loss: 867.934, train mse: 29.48101. Val loss: 877.195, val mse: 29.33199\n",
            "Epoch #4. Time: 6.3s. Train loss: 868.178, train mse: 29.44311. Val loss: 746.640, val mse: 27.48444\n",
            "Epoch #5. Time: 6.4s. Train loss: 837.623, train mse: 28.96515. Val loss: 695.164, val mse: 26.14523\n",
            "Epoch #6. Time: 6.4s. Train loss: 762.124, train mse: 27.58640. Val loss: 726.187, val mse: 27.11809\n",
            "Epoch #7. Time: 6.4s. Train loss: 760.051, train mse: 27.57976. Val loss: 826.957, val mse: 28.09385\n",
            "Epoch #8. Time: 6.3s. Train loss: 747.307, train mse: 27.31737. Val loss: 689.919, val mse: 26.45480\n",
            "Epoch #9. Time: 6.4s. Train loss: 688.742, train mse: 26.29325. Val loss: 627.082, val mse: 25.03943\n",
            "Epoch #10. Time: 6.4s. Train loss: 652.085, train mse: 25.57175. Val loss: 673.031, val mse: 26.04832\n",
            "Epoch #11. Time: 6.3s. Train loss: 652.092, train mse: 25.56700. Val loss: 614.224, val mse: 24.91916\n",
            "Epoch #12. Time: 6.4s. Train loss: 592.284, train mse: 24.36802. Val loss: 588.778, val mse: 24.27370\n",
            "Epoch #13. Time: 6.3s. Train loss: 547.429, train mse: 23.41543. Val loss: 549.925, val mse: 23.79630\n",
            "Epoch #14. Time: 6.4s. Train loss: 528.189, train mse: 22.96653. Val loss: 569.833, val mse: 23.30993\n",
            "Epoch #15. Time: 6.4s. Train loss: 481.535, train mse: 21.94663. Val loss: 456.959, val mse: 21.63313\n",
            "Epoch #16. Time: 6.4s. Train loss: 444.266, train mse: 21.08061. Val loss: 481.569, val mse: 22.26801\n",
            "Epoch #17. Time: 6.3s. Train loss: 438.556, train mse: 20.95881. Val loss: 483.821, val mse: 21.84069\n",
            "Epoch #18. Time: 6.4s. Train loss: 384.369, train mse: 19.57539. Val loss: 485.496, val mse: 22.33167\n",
            "Epoch #19. Time: 6.4s. Train loss: 417.119, train mse: 20.45497. Val loss: 488.799, val mse: 22.39107\n",
            "Epoch #20. Time: 6.3s. Train loss: 371.938, train mse: 19.25462. Val loss: 522.483, val mse: 22.99776\n",
            "Epoch #21. Time: 6.3s. Train loss: 442.396, train mse: 21.04856. Val loss: 503.779, val mse: 22.32329\n",
            "Epoch #22. Time: 6.4s. Train loss: 421.962, train mse: 20.51931. Val loss: 431.698, val mse: 20.44352\n",
            "Epoch #23. Time: 6.4s. Train loss: 448.280, train mse: 21.16786. Val loss: 521.922, val mse: 22.60584\n",
            "Epoch #24. Time: 6.3s. Train loss: 467.862, train mse: 21.63373. Val loss: 485.127, val mse: 21.94249\n",
            "Epoch #25. Time: 6.3s. Train loss: 471.837, train mse: 21.68617. Val loss: 817.798, val mse: 28.53718\n",
            "Epoch #26. Time: 6.4s. Train loss: 468.910, train mse: 21.68078. Val loss: 501.933, val mse: 22.66248\n",
            "Epoch #27. Time: 6.4s. Train loss: 431.443, train mse: 20.79027. Val loss: 562.580, val mse: 24.02096\n",
            "Epoch #28. Time: 6.4s. Train loss: 435.734, train mse: 20.87354. Val loss: 685.635, val mse: 26.48315\n",
            "Epoch #29. Time: 6.4s. Train loss: 372.279, train mse: 19.26026. Val loss: 654.816, val mse: 25.26966\n",
            "Epoch #30. Time: 6.4s. Train loss: 351.858, train mse: 18.80364. Val loss: 632.522, val mse: 25.32423\n",
            "Epoch #31. Time: 6.3s. Train loss: 337.473, train mse: 18.37335. Val loss: 520.153, val mse: 23.00730\n",
            "Epoch #32. Time: 6.3s. Train loss: 306.672, train mse: 17.54101. Val loss: 522.563, val mse: 22.68235\n",
            "Epoch #33. Time: 6.4s. Train loss: 296.750, train mse: 17.20882. Val loss: 577.277, val mse: 24.30381\n",
            "Epoch #34. Time: 6.4s. Train loss: 256.311, train mse: 15.98700. Val loss: 592.843, val mse: 24.50121\n",
            "Epoch #35. Time: 6.3s. Train loss: 241.632, train mse: 15.51783. Val loss: 538.795, val mse: 22.93809\n",
            "Epoch #36. Time: 6.3s. Train loss: 253.515, train mse: 15.89289. Val loss: 544.197, val mse: 22.99161\n",
            "Epoch #37. Time: 6.4s. Train loss: 226.603, train mse: 15.04726. Val loss: 536.330, val mse: 23.30562\n",
            "Epoch #38. Time: 6.3s. Train loss: 232.494, train mse: 15.26132. Val loss: 596.805, val mse: 23.42550\n",
            "Epoch #39. Time: 6.3s. Train loss: 236.834, train mse: 15.40081. Val loss: 592.737, val mse: 24.17683\n",
            "Epoch #40. Time: 6.3s. Train loss: 211.537, train mse: 14.56786. Val loss: 515.218, val mse: 22.91562\n",
            "Training done. Best rmse: 20.443523406982422\n",
            "Start #3 fold\n",
            "Epoch #1. Time: 6.3s. Train loss: 5829.175, train mse: 76.41195. Val loss: 3991.709, val mse: 63.64437\n",
            "Epoch #2. Time: 6.4s. Train loss: 1293.763, train mse: 36.00950. Val loss: 819.170, val mse: 28.84019\n",
            "Epoch #3. Time: 6.3s. Train loss: 900.349, train mse: 29.95697. Val loss: 849.472, val mse: 29.19867\n",
            "Epoch #4. Time: 6.3s. Train loss: 831.176, train mse: 28.86575. Val loss: 850.839, val mse: 28.95536\n",
            "Epoch #5. Time: 6.3s. Train loss: 830.085, train mse: 28.79477. Val loss: 740.099, val mse: 26.74981\n",
            "Epoch #6. Time: 6.3s. Train loss: 807.081, train mse: 28.41898. Val loss: 722.533, val mse: 26.69911\n",
            "Epoch #7. Time: 6.4s. Train loss: 753.698, train mse: 27.38840. Val loss: 665.792, val mse: 25.59846\n",
            "Epoch #8. Time: 6.4s. Train loss: 720.117, train mse: 26.82225. Val loss: 586.402, val mse: 24.25383\n",
            "Epoch #9. Time: 6.4s. Train loss: 717.308, train mse: 26.77159. Val loss: 576.948, val mse: 24.05202\n",
            "Epoch #10. Time: 6.4s. Train loss: 639.384, train mse: 25.32448. Val loss: 547.522, val mse: 23.66949\n",
            "Epoch #11. Time: 6.3s. Train loss: 576.078, train mse: 24.00834. Val loss: 731.467, val mse: 27.19127\n",
            "Epoch #12. Time: 6.4s. Train loss: 526.656, train mse: 22.99554. Val loss: 514.528, val mse: 22.81240\n",
            "Epoch #13. Time: 6.4s. Train loss: 490.779, train mse: 22.18131. Val loss: 440.089, val mse: 20.91904\n",
            "Epoch #14. Time: 6.4s. Train loss: 442.091, train mse: 21.00271. Val loss: 457.089, val mse: 21.18886\n",
            "Epoch #15. Time: 6.3s. Train loss: 411.262, train mse: 20.25655. Val loss: 479.889, val mse: 21.89733\n",
            "Epoch #16. Time: 6.3s. Train loss: 413.544, train mse: 20.33714. Val loss: 564.850, val mse: 23.96383\n",
            "Epoch #17. Time: 6.3s. Train loss: 383.512, train mse: 19.53397. Val loss: 445.179, val mse: 21.15238\n",
            "Epoch #18. Time: 6.4s. Train loss: 359.920, train mse: 19.00145. Val loss: 443.663, val mse: 20.90560\n",
            "Epoch #19. Time: 6.4s. Train loss: 376.207, train mse: 19.41890. Val loss: 507.407, val mse: 22.13324\n",
            "Epoch #20. Time: 6.4s. Train loss: 320.626, train mse: 17.86523. Val loss: 497.854, val mse: 22.62631\n",
            "Epoch #21. Time: 6.4s. Train loss: 367.794, train mse: 19.10818. Val loss: 438.555, val mse: 20.86301\n",
            "Epoch #22. Time: 6.4s. Train loss: 380.293, train mse: 19.52268. Val loss: 445.901, val mse: 21.23092\n",
            "Epoch #23. Time: 6.3s. Train loss: 384.139, train mse: 19.58716. Val loss: 562.263, val mse: 23.74594\n",
            "Epoch #24. Time: 6.3s. Train loss: 422.648, train mse: 20.52561. Val loss: 651.555, val mse: 25.24804\n",
            "Epoch #25. Time: 6.3s. Train loss: 436.176, train mse: 20.92363. Val loss: 550.781, val mse: 23.36837\n",
            "Epoch #26. Time: 6.3s. Train loss: 409.167, train mse: 20.22823. Val loss: 553.999, val mse: 23.56521\n",
            "Epoch #27. Time: 6.3s. Train loss: 393.071, train mse: 19.86419. Val loss: 478.732, val mse: 21.51579\n",
            "Epoch #28. Time: 6.3s. Train loss: 331.950, train mse: 18.23893. Val loss: 485.508, val mse: 21.66025\n",
            "Epoch #29. Time: 6.4s. Train loss: 342.260, train mse: 18.51412. Val loss: 396.380, val mse: 19.94163\n",
            "Epoch #30. Time: 6.4s. Train loss: 327.234, train mse: 18.11153. Val loss: 648.966, val mse: 25.37584\n",
            "Epoch #31. Time: 6.4s. Train loss: 289.900, train mse: 17.02985. Val loss: 457.024, val mse: 21.78040\n",
            "Epoch #32. Time: 6.3s. Train loss: 282.009, train mse: 16.81789. Val loss: 467.342, val mse: 21.53487\n",
            "Epoch #33. Time: 6.3s. Train loss: 255.863, train mse: 16.03050. Val loss: 410.625, val mse: 20.46398\n",
            "Epoch #34. Time: 6.4s. Train loss: 265.279, train mse: 16.26919. Val loss: 463.311, val mse: 21.77022\n",
            "Epoch #35. Time: 6.3s. Train loss: 230.339, train mse: 15.15132. Val loss: 448.039, val mse: 21.25834\n",
            "Epoch #36. Time: 6.3s. Train loss: 206.163, train mse: 14.34410. Val loss: 398.042, val mse: 19.87391\n",
            "Epoch #37. Time: 6.4s. Train loss: 184.804, train mse: 13.52982. Val loss: 406.647, val mse: 20.27889\n",
            "Epoch #38. Time: 6.4s. Train loss: 180.243, train mse: 13.38417. Val loss: 441.274, val mse: 20.88960\n",
            "Epoch #39. Time: 6.4s. Train loss: 178.413, train mse: 13.35295. Val loss: 426.146, val mse: 20.39294\n",
            "Epoch #40. Time: 6.3s. Train loss: 185.502, train mse: 13.63781. Val loss: 403.683, val mse: 20.12918\n",
            "Training done. Best rmse: 19.87390899658203\n",
            "Start #4 fold\n",
            "Epoch #1. Time: 6.4s. Train loss: 5855.189, train mse: 76.64264. Val loss: 4503.463, val mse: 66.33276\n",
            "Epoch #2. Time: 6.4s. Train loss: 1295.975, train mse: 36.09425. Val loss: 1157.136, val mse: 34.42349\n",
            "Epoch #3. Time: 6.3s. Train loss: 873.946, train mse: 29.48473. Val loss: 1025.574, val mse: 32.20070\n",
            "Epoch #4. Time: 6.3s. Train loss: 834.951, train mse: 28.93242. Val loss: 939.607, val mse: 30.72552\n",
            "Epoch #5. Time: 6.4s. Train loss: 821.519, train mse: 28.71063. Val loss: 804.770, val mse: 28.55352\n",
            "Epoch #6. Time: 6.3s. Train loss: 802.126, train mse: 28.24995. Val loss: 709.771, val mse: 26.78320\n",
            "Epoch #7. Time: 6.3s. Train loss: 764.144, train mse: 27.55811. Val loss: 827.376, val mse: 28.47701\n",
            "Epoch #8. Time: 6.3s. Train loss: 733.418, train mse: 27.02825. Val loss: 670.322, val mse: 25.81986\n",
            "Epoch #9. Time: 6.4s. Train loss: 670.671, train mse: 25.88005. Val loss: 682.900, val mse: 26.07695\n",
            "Epoch #10. Time: 6.3s. Train loss: 715.791, train mse: 26.67536. Val loss: 724.694, val mse: 26.94490\n",
            "Epoch #11. Time: 6.3s. Train loss: 611.364, train mse: 24.71525. Val loss: 711.385, val mse: 26.64995\n",
            "Epoch #12. Time: 6.4s. Train loss: 592.291, train mse: 24.33954. Val loss: 636.557, val mse: 25.49255\n",
            "Epoch #13. Time: 6.3s. Train loss: 542.988, train mse: 23.32364. Val loss: 629.927, val mse: 25.01366\n",
            "Epoch #14. Time: 6.4s. Train loss: 493.748, train mse: 22.25443. Val loss: 711.891, val mse: 26.70270\n",
            "Epoch #15. Time: 6.4s. Train loss: 466.703, train mse: 21.60670. Val loss: 638.057, val mse: 25.29937\n",
            "Epoch #16. Time: 6.3s. Train loss: 441.706, train mse: 21.01229. Val loss: 657.415, val mse: 26.01395\n",
            "Epoch #17. Time: 6.3s. Train loss: 409.811, train mse: 20.24770. Val loss: 642.240, val mse: 24.96637\n",
            "Epoch #18. Time: 6.4s. Train loss: 384.790, train mse: 19.60191. Val loss: 643.282, val mse: 25.50583\n",
            "Epoch #19. Time: 6.3s. Train loss: 376.410, train mse: 19.40460. Val loss: 697.663, val mse: 26.29437\n",
            "Epoch #20. Time: 6.3s. Train loss: 382.950, train mse: 19.59566. Val loss: 675.126, val mse: 26.22625\n",
            "Epoch #21. Time: 6.4s. Train loss: 423.289, train mse: 20.46137. Val loss: 630.798, val mse: 25.08535\n",
            "Epoch #22. Time: 6.3s. Train loss: 408.854, train mse: 20.18591. Val loss: 728.803, val mse: 27.05241\n",
            "Epoch #23. Time: 6.4s. Train loss: 437.217, train mse: 20.96455. Val loss: 607.935, val mse: 24.70983\n",
            "Epoch #24. Time: 6.3s. Train loss: 429.857, train mse: 20.75179. Val loss: 655.607, val mse: 25.79282\n",
            "Epoch #25. Time: 6.3s. Train loss: 461.304, train mse: 21.52522. Val loss: 680.515, val mse: 26.36908\n",
            "Epoch #26. Time: 6.3s. Train loss: 443.850, train mse: 21.05202. Val loss: 636.245, val mse: 25.05817\n",
            "Epoch #27. Time: 6.3s. Train loss: 414.979, train mse: 20.36154. Val loss: 603.922, val mse: 24.38725\n",
            "Epoch #28. Time: 6.3s. Train loss: 392.304, train mse: 19.83104. Val loss: 801.320, val mse: 28.34675\n",
            "Epoch #29. Time: 6.4s. Train loss: 384.395, train mse: 19.63710. Val loss: 958.099, val mse: 31.09454\n",
            "Epoch #30. Time: 6.3s. Train loss: 325.817, train mse: 18.05566. Val loss: 611.620, val mse: 24.98087\n",
            "Epoch #31. Time: 6.3s. Train loss: 298.322, train mse: 17.28151. Val loss: 583.950, val mse: 24.32650\n",
            "Epoch #32. Time: 6.4s. Train loss: 284.867, train mse: 16.85709. Val loss: 558.195, val mse: 23.94243\n",
            "Epoch #33. Time: 6.3s. Train loss: 272.179, train mse: 16.43763. Val loss: 662.997, val mse: 24.40643\n",
            "Epoch #34. Time: 6.4s. Train loss: 275.707, train mse: 16.61137. Val loss: 585.080, val mse: 23.85370\n",
            "Epoch #35. Time: 6.3s. Train loss: 232.922, train mse: 15.24824. Val loss: 576.333, val mse: 24.18995\n",
            "Epoch #36. Time: 6.3s. Train loss: 215.303, train mse: 14.66014. Val loss: 608.721, val mse: 24.93753\n",
            "Epoch #37. Time: 6.3s. Train loss: 203.625, train mse: 14.26737. Val loss: 625.526, val mse: 24.90724\n",
            "Epoch #38. Time: 6.4s. Train loss: 217.859, train mse: 14.74149. Val loss: 554.871, val mse: 23.87551\n",
            "Epoch #39. Time: 6.4s. Train loss: 191.108, train mse: 13.84483. Val loss: 653.504, val mse: 24.51412\n",
            "Epoch #40. Time: 6.3s. Train loss: 189.012, train mse: 13.76539. Val loss: 584.873, val mse: 24.55438\n",
            "Training done. Best rmse: 23.853696823120117\n",
            "Start #5 fold\n",
            "Epoch #1. Time: 6.3s. Train loss: 5784.789, train mse: 76.20673. Val loss: 4002.196, val mse: 63.12139\n",
            "Epoch #2. Time: 6.3s. Train loss: 1238.188, train mse: 35.21356. Val loss: 994.716, val mse: 31.18997\n",
            "Epoch #3. Time: 6.4s. Train loss: 859.200, train mse: 29.27904. Val loss: 767.132, val mse: 27.61738\n",
            "Epoch #4. Time: 6.3s. Train loss: 852.377, train mse: 29.23500. Val loss: 807.899, val mse: 28.32956\n",
            "Epoch #5. Time: 6.4s. Train loss: 859.043, train mse: 29.34316. Val loss: 799.057, val mse: 28.12915\n",
            "Epoch #6. Time: 6.4s. Train loss: 805.830, train mse: 28.29732. Val loss: 796.302, val mse: 27.81273\n",
            "Epoch #7. Time: 6.4s. Train loss: 768.317, train mse: 27.74681. Val loss: 678.417, val mse: 26.40009\n",
            "Epoch #8. Time: 6.4s. Train loss: 717.359, train mse: 26.80375. Val loss: 607.669, val mse: 24.70250\n",
            "Epoch #9. Time: 6.4s. Train loss: 678.695, train mse: 26.03485. Val loss: 559.591, val mse: 23.69681\n",
            "Epoch #10. Time: 6.4s. Train loss: 624.956, train mse: 24.94754. Val loss: 663.249, val mse: 25.71073\n",
            "Epoch #11. Time: 6.3s. Train loss: 584.180, train mse: 24.18459. Val loss: 656.470, val mse: 26.14504\n",
            "Epoch #12. Time: 6.3s. Train loss: 545.693, train mse: 23.40761. Val loss: 588.464, val mse: 24.36338\n",
            "Epoch #13. Time: 6.3s. Train loss: 528.131, train mse: 22.94786. Val loss: 484.708, val mse: 22.12224\n",
            "Epoch #14. Time: 6.4s. Train loss: 487.704, train mse: 22.09534. Val loss: 515.310, val mse: 22.71908\n",
            "Epoch #15. Time: 6.3s. Train loss: 448.059, train mse: 21.11137. Val loss: 493.667, val mse: 22.16712\n",
            "Epoch #16. Time: 6.3s. Train loss: 443.329, train mse: 21.09414. Val loss: 441.681, val mse: 21.10861\n",
            "Epoch #17. Time: 6.4s. Train loss: 421.451, train mse: 20.53465. Val loss: 526.964, val mse: 22.72869\n",
            "Epoch #18. Time: 6.3s. Train loss: 395.614, train mse: 19.87206. Val loss: 527.061, val mse: 22.86836\n",
            "Epoch #19. Time: 6.3s. Train loss: 404.050, train mse: 20.01513. Val loss: 448.557, val mse: 21.21910\n",
            "Epoch #20. Time: 6.3s. Train loss: 373.009, train mse: 19.30876. Val loss: 453.151, val mse: 21.37637\n",
            "Epoch #21. Time: 6.3s. Train loss: 380.910, train mse: 19.50564. Val loss: 582.411, val mse: 23.46334\n",
            "Epoch #22. Time: 6.3s. Train loss: 416.759, train mse: 20.38488. Val loss: 474.834, val mse: 22.03475\n",
            "Epoch #23. Time: 6.3s. Train loss: 423.225, train mse: 20.56713. Val loss: 643.293, val mse: 25.26937\n",
            "Epoch #24. Time: 6.3s. Train loss: 418.958, train mse: 20.46895. Val loss: 569.929, val mse: 23.34592\n",
            "Epoch #25. Time: 6.3s. Train loss: 475.052, train mse: 21.71336. Val loss: 600.621, val mse: 24.45704\n",
            "Epoch #26. Time: 6.4s. Train loss: 448.559, train mse: 21.13750. Val loss: 598.237, val mse: 24.62035\n",
            "Epoch #27. Time: 6.3s. Train loss: 424.449, train mse: 20.62349. Val loss: 437.548, val mse: 21.27685\n",
            "Epoch #28. Time: 6.3s. Train loss: 375.337, train mse: 19.39813. Val loss: 456.926, val mse: 21.22122\n",
            "Epoch #29. Time: 6.3s. Train loss: 358.313, train mse: 18.96435. Val loss: 456.672, val mse: 21.39435\n",
            "Epoch #30. Time: 6.5s. Train loss: 324.254, train mse: 18.02753. Val loss: 625.278, val mse: 24.55599\n",
            "Epoch #31. Time: 6.3s. Train loss: 312.025, train mse: 17.68580. Val loss: 505.984, val mse: 22.40184\n",
            "Epoch #32. Time: 6.3s. Train loss: 304.145, train mse: 17.48262. Val loss: 428.595, val mse: 20.90217\n",
            "Epoch #33. Time: 6.3s. Train loss: 278.030, train mse: 16.67408. Val loss: 454.556, val mse: 21.45713\n",
            "Epoch #34. Time: 6.3s. Train loss: 270.886, train mse: 16.36812. Val loss: 429.589, val mse: 20.80634\n",
            "Epoch #35. Time: 6.4s. Train loss: 226.806, train mse: 15.03088. Val loss: 466.728, val mse: 21.40599\n",
            "Epoch #36. Time: 6.3s. Train loss: 222.504, train mse: 14.92027. Val loss: 445.862, val mse: 21.18090\n",
            "Epoch #37. Time: 6.3s. Train loss: 204.605, train mse: 14.30903. Val loss: 456.448, val mse: 21.48878\n",
            "Epoch #38. Time: 6.3s. Train loss: 199.237, train mse: 14.10890. Val loss: 496.852, val mse: 22.15269\n",
            "Epoch #39. Time: 6.3s. Train loss: 217.987, train mse: 14.74144. Val loss: 468.544, val mse: 21.80140\n",
            "Epoch #40. Time: 6.3s. Train loss: 191.084, train mse: 13.78524. Val loss: 464.225, val mse: 21.89053\n",
            "Training done. Best rmse: 20.80633544921875\n",
            "Start #6 fold\n",
            "Epoch #1. Time: 6.3s. Train loss: 5860.046, train mse: 76.60275. Val loss: 3833.939, val mse: 61.59978\n",
            "Epoch #2. Time: 6.3s. Train loss: 1298.380, train mse: 36.05799. Val loss: 732.071, val mse: 27.34589\n",
            "Epoch #3. Time: 6.4s. Train loss: 871.742, train mse: 29.55555. Val loss: 780.446, val mse: 28.16136\n",
            "Epoch #4. Time: 6.4s. Train loss: 872.214, train mse: 29.50088. Val loss: 829.616, val mse: 28.18655\n",
            "Epoch #5. Time: 6.3s. Train loss: 821.143, train mse: 28.66405. Val loss: 724.046, val mse: 26.88466\n",
            "Epoch #6. Time: 6.3s. Train loss: 770.283, train mse: 27.79888. Val loss: 803.176, val mse: 28.32888\n",
            "Epoch #7. Time: 6.3s. Train loss: 732.777, train mse: 27.13686. Val loss: 788.371, val mse: 28.15725\n",
            "Epoch #8. Time: 6.4s. Train loss: 760.897, train mse: 27.55509. Val loss: 656.020, val mse: 25.79683\n",
            "Epoch #9. Time: 6.3s. Train loss: 697.056, train mse: 26.39313. Val loss: 596.445, val mse: 24.77149\n",
            "Epoch #10. Time: 6.3s. Train loss: 681.359, train mse: 26.02527. Val loss: 579.211, val mse: 23.95631\n",
            "Epoch #11. Time: 6.3s. Train loss: 596.085, train mse: 24.33162. Val loss: 610.678, val mse: 24.73394\n",
            "Epoch #12. Time: 6.3s. Train loss: 553.290, train mse: 23.54944. Val loss: 536.494, val mse: 23.29969\n",
            "Epoch #13. Time: 6.3s. Train loss: 502.192, train mse: 22.43508. Val loss: 544.457, val mse: 23.01199\n",
            "Epoch #14. Time: 6.2s. Train loss: 480.709, train mse: 21.94553. Val loss: 586.945, val mse: 24.23761\n",
            "Epoch #15. Time: 6.4s. Train loss: 474.630, train mse: 21.82615. Val loss: 575.864, val mse: 23.95507\n",
            "Epoch #16. Time: 6.3s. Train loss: 443.006, train mse: 21.05228. Val loss: 521.047, val mse: 22.69114\n",
            "Epoch #17. Time: 6.4s. Train loss: 387.138, train mse: 19.68972. Val loss: 553.562, val mse: 23.78535\n",
            "Epoch #18. Time: 6.4s. Train loss: 401.067, train mse: 20.03888. Val loss: 542.284, val mse: 23.66037\n",
            "Epoch #19. Time: 6.4s. Train loss: 354.583, train mse: 18.84998. Val loss: 587.760, val mse: 24.42591\n",
            "Epoch #20. Time: 6.2s. Train loss: 372.017, train mse: 19.29484. Val loss: 580.617, val mse: 23.88202\n",
            "Epoch #21. Time: 6.3s. Train loss: 355.839, train mse: 18.87083. Val loss: 604.362, val mse: 24.73591\n",
            "Epoch #22. Time: 6.3s. Train loss: 405.785, train mse: 20.17682. Val loss: 568.753, val mse: 23.82565\n",
            "Epoch #23. Time: 6.3s. Train loss: 428.700, train mse: 20.69449. Val loss: 816.330, val mse: 28.51108\n",
            "Epoch #24. Time: 6.3s. Train loss: 422.279, train mse: 20.54081. Val loss: 581.998, val mse: 23.75537\n",
            "Epoch #25. Time: 6.3s. Train loss: 424.430, train mse: 20.62336. Val loss: 570.331, val mse: 23.98068\n",
            "Epoch #26. Time: 6.3s. Train loss: 453.635, train mse: 21.34282. Val loss: 723.137, val mse: 27.08018\n",
            "Epoch #27. Time: 6.3s. Train loss: 392.752, train mse: 19.82354. Val loss: 560.877, val mse: 23.80958\n",
            "Epoch #28. Time: 6.4s. Train loss: 352.327, train mse: 18.73118. Val loss: 742.368, val mse: 27.52802\n",
            "Epoch #29. Time: 6.3s. Train loss: 352.233, train mse: 18.81958. Val loss: 493.235, val mse: 22.43899\n",
            "Epoch #30. Time: 6.3s. Train loss: 334.016, train mse: 18.29003. Val loss: 616.111, val mse: 25.12381\n",
            "Epoch #31. Time: 6.4s. Train loss: 311.312, train mse: 17.68265. Val loss: 726.825, val mse: 26.79535\n",
            "Epoch #32. Time: 6.4s. Train loss: 310.282, train mse: 17.53456. Val loss: 553.542, val mse: 23.85213\n",
            "Epoch #33. Time: 6.4s. Train loss: 275.125, train mse: 16.62620. Val loss: 556.164, val mse: 23.80909\n",
            "Epoch #34. Time: 6.3s. Train loss: 248.136, train mse: 15.75486. Val loss: 524.431, val mse: 22.62584\n",
            "Epoch #35. Time: 6.3s. Train loss: 247.472, train mse: 15.72888. Val loss: 504.939, val mse: 22.46373\n",
            "Epoch #36. Time: 6.3s. Train loss: 222.512, train mse: 14.91312. Val loss: 556.976, val mse: 23.85151\n",
            "Epoch #37. Time: 6.4s. Train loss: 216.303, train mse: 14.74386. Val loss: 556.052, val mse: 23.76163\n",
            "Epoch #38. Time: 6.3s. Train loss: 204.229, train mse: 14.22631. Val loss: 508.844, val mse: 21.96414\n",
            "Epoch #39. Time: 6.3s. Train loss: 190.669, train mse: 13.81302. Val loss: 568.601, val mse: 23.36421\n",
            "Epoch #40. Time: 6.3s. Train loss: 174.445, train mse: 13.22619. Val loss: 580.757, val mse: 23.85902\n",
            "Training done. Best rmse: 21.96413803100586\n",
            "Start #7 fold\n",
            "Epoch #1. Time: 6.3s. Train loss: 5848.582, train mse: 76.66079. Val loss: 3965.557, val mse: 62.83543\n",
            "Epoch #2. Time: 6.3s. Train loss: 1313.631, train mse: 36.27276. Val loss: 884.688, val mse: 29.71503\n",
            "Epoch #3. Time: 6.3s. Train loss: 873.228, train mse: 29.59328. Val loss: 810.663, val mse: 28.23027\n",
            "Epoch #4. Time: 6.3s. Train loss: 845.097, train mse: 29.12437. Val loss: 815.892, val mse: 28.45910\n",
            "Epoch #5. Time: 6.3s. Train loss: 797.981, train mse: 28.26298. Val loss: 878.575, val mse: 29.50036\n",
            "Epoch #6. Time: 6.3s. Train loss: 788.547, train mse: 28.13184. Val loss: 696.476, val mse: 26.53093\n",
            "Epoch #7. Time: 6.3s. Train loss: 752.461, train mse: 27.45838. Val loss: 733.052, val mse: 27.16321\n",
            "Epoch #8. Time: 6.3s. Train loss: 726.084, train mse: 26.97493. Val loss: 628.809, val mse: 25.05140\n",
            "Epoch #9. Time: 6.3s. Train loss: 723.950, train mse: 26.87827. Val loss: 668.180, val mse: 26.03541\n",
            "Epoch #10. Time: 6.4s. Train loss: 658.792, train mse: 25.71488. Val loss: 803.273, val mse: 28.13003\n",
            "Epoch #11. Time: 6.4s. Train loss: 624.264, train mse: 24.98278. Val loss: 635.540, val mse: 25.35355\n",
            "Epoch #12. Time: 6.3s. Train loss: 617.700, train mse: 24.86028. Val loss: 619.904, val mse: 24.81590\n",
            "Epoch #13. Time: 6.3s. Train loss: 550.804, train mse: 23.47055. Val loss: 517.223, val mse: 22.87031\n",
            "Epoch #14. Time: 6.4s. Train loss: 516.587, train mse: 22.73127. Val loss: 485.718, val mse: 22.15458\n",
            "Epoch #15. Time: 6.4s. Train loss: 466.078, train mse: 21.47342. Val loss: 507.942, val mse: 21.94126\n",
            "Epoch #16. Time: 6.3s. Train loss: 433.804, train mse: 20.83184. Val loss: 507.629, val mse: 22.34261\n",
            "Epoch #17. Time: 6.3s. Train loss: 426.165, train mse: 20.61882. Val loss: 464.562, val mse: 21.44221\n",
            "Epoch #18. Time: 6.4s. Train loss: 419.916, train mse: 20.51177. Val loss: 450.025, val mse: 21.30723\n",
            "Epoch #19. Time: 6.4s. Train loss: 395.517, train mse: 19.89869. Val loss: 448.874, val mse: 21.17280\n",
            "Epoch #20. Time: 6.3s. Train loss: 418.509, train mse: 20.43218. Val loss: 446.900, val mse: 21.32360\n",
            "Epoch #21. Time: 6.4s. Train loss: 409.143, train mse: 20.25765. Val loss: 425.680, val mse: 20.03761\n",
            "Epoch #22. Time: 6.4s. Train loss: 390.918, train mse: 19.75190. Val loss: 480.793, val mse: 22.01281\n",
            "Epoch #23. Time: 6.4s. Train loss: 412.046, train mse: 20.32487. Val loss: 538.694, val mse: 23.20181\n",
            "Epoch #24. Time: 6.4s. Train loss: 474.756, train mse: 21.82439. Val loss: 490.227, val mse: 22.05757\n",
            "Epoch #25. Time: 6.4s. Train loss: 458.520, train mse: 21.44851. Val loss: 550.901, val mse: 23.40226\n",
            "Epoch #26. Time: 6.3s. Train loss: 425.992, train mse: 20.59688. Val loss: 677.969, val mse: 26.22062\n",
            "Epoch #27. Time: 6.4s. Train loss: 419.072, train mse: 20.41563. Val loss: 476.640, val mse: 21.87771\n",
            "Epoch #28. Time: 6.4s. Train loss: 417.849, train mse: 20.43934. Val loss: 436.781, val mse: 20.59873\n",
            "Epoch #29. Time: 6.4s. Train loss: 374.117, train mse: 19.16467. Val loss: 418.604, val mse: 20.49267\n",
            "Epoch #30. Time: 6.4s. Train loss: 360.789, train mse: 18.93455. Val loss: 585.379, val mse: 24.35137\n",
            "Epoch #31. Time: 6.5s. Train loss: 348.696, train mse: 18.68930. Val loss: 505.280, val mse: 22.64548\n",
            "Epoch #32. Time: 6.4s. Train loss: 300.661, train mse: 17.35683. Val loss: 426.827, val mse: 20.72535\n",
            "Epoch #33. Time: 6.3s. Train loss: 277.920, train mse: 16.65438. Val loss: 427.908, val mse: 20.81903\n",
            "Epoch #34. Time: 6.3s. Train loss: 257.820, train mse: 16.09049. Val loss: 427.293, val mse: 20.83225\n",
            "Epoch #35. Time: 6.3s. Train loss: 236.643, train mse: 15.33687. Val loss: 402.245, val mse: 20.14763\n",
            "Epoch #36. Time: 6.3s. Train loss: 248.404, train mse: 15.76851. Val loss: 411.935, val mse: 20.22799\n",
            "Epoch #37. Time: 6.3s. Train loss: 206.725, train mse: 14.33010. Val loss: 400.624, val mse: 20.17547\n",
            "Epoch #38. Time: 6.4s. Train loss: 213.850, train mse: 14.63842. Val loss: 417.218, val mse: 20.64550\n",
            "Epoch #39. Time: 6.4s. Train loss: 231.272, train mse: 15.19417. Val loss: 427.580, val mse: 20.79595\n",
            "Epoch #40. Time: 6.3s. Train loss: 183.696, train mse: 13.56497. Val loss: 436.573, val mse: 20.08180\n",
            "Training done. Best rmse: 20.03760528564453\n",
            "Start #8 fold\n",
            "Epoch #1. Time: 6.4s. Train loss: 5874.167, train mse: 76.73331. Val loss: 4077.264, val mse: 64.11961\n",
            "Epoch #2. Time: 6.4s. Train loss: 1312.781, train mse: 36.27373. Val loss: 1253.037, val mse: 34.47912\n",
            "Epoch #3. Time: 6.4s. Train loss: 874.627, train mse: 29.55771. Val loss: 753.401, val mse: 27.61805\n",
            "Epoch #4. Time: 6.5s. Train loss: 873.249, train mse: 29.56782. Val loss: 646.548, val mse: 25.46953\n",
            "Epoch #5. Time: 6.4s. Train loss: 814.216, train mse: 28.58322. Val loss: 617.645, val mse: 24.88025\n",
            "Epoch #6. Time: 6.3s. Train loss: 823.062, train mse: 28.61610. Val loss: 734.894, val mse: 26.69273\n",
            "Epoch #7. Time: 6.3s. Train loss: 766.358, train mse: 27.72897. Val loss: 717.568, val mse: 26.43257\n",
            "Epoch #8. Time: 6.3s. Train loss: 745.313, train mse: 27.30309. Val loss: 618.470, val mse: 24.24459\n",
            "Epoch #9. Time: 6.4s. Train loss: 695.935, train mse: 26.39254. Val loss: 691.841, val mse: 26.15040\n",
            "Epoch #10. Time: 6.4s. Train loss: 671.526, train mse: 25.91691. Val loss: 584.073, val mse: 24.30477\n",
            "Epoch #11. Time: 6.5s. Train loss: 651.055, train mse: 25.52536. Val loss: 583.797, val mse: 24.06020\n",
            "Epoch #12. Time: 6.3s. Train loss: 615.219, train mse: 24.85424. Val loss: 591.571, val mse: 24.31617\n",
            "Epoch #13. Time: 6.4s. Train loss: 558.107, train mse: 23.61783. Val loss: 655.352, val mse: 25.52343\n",
            "Epoch #14. Time: 6.4s. Train loss: 518.694, train mse: 22.80037. Val loss: 613.656, val mse: 23.96376\n",
            "Epoch #15. Time: 6.4s. Train loss: 485.219, train mse: 22.06744. Val loss: 636.661, val mse: 25.27170\n",
            "Epoch #16. Time: 6.3s. Train loss: 436.147, train mse: 20.89644. Val loss: 549.732, val mse: 23.50790\n",
            "Epoch #17. Time: 6.4s. Train loss: 422.293, train mse: 20.49465. Val loss: 609.422, val mse: 24.60773\n",
            "Epoch #18. Time: 6.3s. Train loss: 440.483, train mse: 20.99761. Val loss: 559.418, val mse: 24.00401\n",
            "Epoch #19. Time: 6.4s. Train loss: 425.885, train mse: 20.63313. Val loss: 598.970, val mse: 24.24021\n",
            "Epoch #20. Time: 6.3s. Train loss: 426.949, train mse: 20.62425. Val loss: 526.404, val mse: 23.20048\n",
            "Epoch #21. Time: 6.4s. Train loss: 422.943, train mse: 20.56025. Val loss: 509.021, val mse: 22.97204\n",
            "Epoch #22. Time: 6.4s. Train loss: 468.929, train mse: 21.56107. Val loss: 603.333, val mse: 24.61524\n",
            "Epoch #23. Time: 6.3s. Train loss: 479.716, train mse: 21.89520. Val loss: 610.379, val mse: 24.80483\n",
            "Epoch #24. Time: 6.3s. Train loss: 505.323, train mse: 22.47577. Val loss: 577.735, val mse: 24.08973\n",
            "Epoch #25. Time: 6.3s. Train loss: 539.524, train mse: 23.26548. Val loss: 688.581, val mse: 25.58503\n",
            "Epoch #26. Time: 6.4s. Train loss: 439.472, train mse: 20.94648. Val loss: 599.311, val mse: 24.01608\n",
            "Epoch #27. Time: 6.4s. Train loss: 458.015, train mse: 21.40997. Val loss: 729.463, val mse: 27.12975\n",
            "Epoch #28. Time: 6.4s. Train loss: 434.787, train mse: 20.86541. Val loss: 536.842, val mse: 22.68394\n",
            "Epoch #29. Time: 6.4s. Train loss: 398.692, train mse: 19.97838. Val loss: 542.641, val mse: 23.37357\n",
            "Epoch #30. Time: 6.3s. Train loss: 355.505, train mse: 18.89549. Val loss: 505.174, val mse: 22.72942\n",
            "Epoch #31. Time: 6.4s. Train loss: 342.173, train mse: 18.46279. Val loss: 506.067, val mse: 22.78943\n",
            "Epoch #32. Time: 6.3s. Train loss: 302.733, train mse: 17.33420. Val loss: 542.720, val mse: 23.28265\n",
            "Epoch #33. Time: 6.4s. Train loss: 287.289, train mse: 16.96811. Val loss: 583.326, val mse: 23.96753\n",
            "Epoch #34. Time: 6.3s. Train loss: 268.312, train mse: 16.40759. Val loss: 515.515, val mse: 22.83692\n",
            "Epoch #35. Time: 6.3s. Train loss: 265.024, train mse: 16.25974. Val loss: 519.041, val mse: 23.06070\n",
            "Epoch #36. Time: 6.3s. Train loss: 247.092, train mse: 15.74661. Val loss: 499.111, val mse: 22.54679\n",
            "Epoch #37. Time: 6.5s. Train loss: 253.963, train mse: 15.89880. Val loss: 525.707, val mse: 22.69482\n",
            "Epoch #38. Time: 6.3s. Train loss: 223.409, train mse: 14.93714. Val loss: 557.761, val mse: 23.41843\n",
            "Epoch #39. Time: 6.4s. Train loss: 209.481, train mse: 14.50083. Val loss: 504.419, val mse: 22.88073\n",
            "Epoch #40. Time: 6.3s. Train loss: 206.179, train mse: 14.31645. Val loss: 496.464, val mse: 22.67577\n",
            "Training done. Best rmse: 22.546789169311523\n",
            "Start #9 fold\n",
            "Epoch #1. Time: 6.4s. Train loss: 5780.367, train mse: 76.10072. Val loss: 4446.420, val mse: 66.54595\n",
            "Epoch #2. Time: 6.3s. Train loss: 1205.917, train mse: 34.79649. Val loss: 1744.201, val mse: 40.64486\n",
            "Epoch #3. Time: 6.3s. Train loss: 882.465, train mse: 29.75793. Val loss: 1022.809, val mse: 31.71168\n",
            "Epoch #4. Time: 6.3s. Train loss: 830.372, train mse: 28.80437. Val loss: 1125.317, val mse: 33.16930\n",
            "Epoch #5. Time: 6.4s. Train loss: 790.185, train mse: 28.11610. Val loss: 1140.477, val mse: 33.69531\n",
            "Epoch #6. Time: 6.4s. Train loss: 768.826, train mse: 27.67461. Val loss: 956.763, val mse: 30.36262\n",
            "Epoch #7. Time: 6.4s. Train loss: 777.513, train mse: 27.85128. Val loss: 979.666, val mse: 31.35700\n",
            "Epoch #8. Time: 6.3s. Train loss: 709.917, train mse: 26.67610. Val loss: 768.361, val mse: 27.99143\n",
            "Epoch #9. Time: 6.3s. Train loss: 665.622, train mse: 25.78708. Val loss: 825.375, val mse: 29.09847\n",
            "Epoch #10. Time: 6.3s. Train loss: 613.014, train mse: 24.79272. Val loss: 807.728, val mse: 28.16556\n",
            "Epoch #11. Time: 6.3s. Train loss: 601.028, train mse: 24.57320. Val loss: 814.950, val mse: 28.49040\n",
            "Epoch #12. Time: 6.3s. Train loss: 536.144, train mse: 23.17591. Val loss: 675.512, val mse: 24.94311\n",
            "Epoch #13. Time: 6.3s. Train loss: 493.729, train mse: 22.23987. Val loss: 713.178, val mse: 26.76334\n",
            "Epoch #14. Time: 6.4s. Train loss: 495.693, train mse: 22.15458. Val loss: 804.482, val mse: 28.49244\n",
            "Epoch #15. Time: 6.4s. Train loss: 437.998, train mse: 20.92424. Val loss: 686.961, val mse: 26.57249\n",
            "Epoch #16. Time: 6.4s. Train loss: 398.249, train mse: 19.99227. Val loss: 669.933, val mse: 26.10590\n",
            "Epoch #17. Time: 6.4s. Train loss: 414.749, train mse: 20.36861. Val loss: 603.426, val mse: 24.71844\n",
            "Epoch #18. Time: 6.3s. Train loss: 372.634, train mse: 19.29179. Val loss: 615.156, val mse: 24.74048\n",
            "Epoch #19. Time: 6.3s. Train loss: 358.871, train mse: 18.93936. Val loss: 638.609, val mse: 24.85054\n",
            "Epoch #20. Time: 6.3s. Train loss: 371.796, train mse: 19.22074. Val loss: 604.037, val mse: 24.71309\n",
            "Epoch #21. Time: 6.5s. Train loss: 387.823, train mse: 19.73537. Val loss: 620.435, val mse: 24.75515\n",
            "Epoch #22. Time: 6.3s. Train loss: 394.888, train mse: 19.90895. Val loss: 856.407, val mse: 29.53312\n",
            "Epoch #23. Time: 6.4s. Train loss: 401.833, train mse: 20.08264. Val loss: 778.620, val mse: 27.64476\n",
            "Epoch #24. Time: 6.4s. Train loss: 453.057, train mse: 21.32160. Val loss: 929.412, val mse: 30.31581\n",
            "Epoch #25. Time: 6.3s. Train loss: 446.190, train mse: 21.09910. Val loss: 704.840, val mse: 26.62942\n",
            "Epoch #26. Time: 6.4s. Train loss: 440.296, train mse: 20.96563. Val loss: 694.321, val mse: 26.72783\n",
            "Epoch #27. Time: 6.4s. Train loss: 438.351, train mse: 20.92238. Val loss: 731.587, val mse: 26.88153\n",
            "Epoch #28. Time: 6.3s. Train loss: 388.532, train mse: 19.73204. Val loss: 801.716, val mse: 28.22001\n",
            "Epoch #29. Time: 6.3s. Train loss: 376.012, train mse: 19.33786. Val loss: 1025.820, val mse: 32.52663\n",
            "Epoch #30. Time: 6.3s. Train loss: 372.573, train mse: 19.30977. Val loss: 700.574, val mse: 26.52925\n",
            "Epoch #31. Time: 6.4s. Train loss: 311.200, train mse: 17.66164. Val loss: 796.296, val mse: 28.21633\n",
            "Epoch #32. Time: 6.4s. Train loss: 293.018, train mse: 17.09651. Val loss: 695.106, val mse: 26.52880\n",
            "Epoch #33. Time: 6.4s. Train loss: 267.648, train mse: 16.32333. Val loss: 702.441, val mse: 26.18866\n",
            "Epoch #34. Time: 6.4s. Train loss: 249.407, train mse: 15.80026. Val loss: 658.156, val mse: 26.04031\n",
            "Epoch #35. Time: 6.3s. Train loss: 224.972, train mse: 14.96659. Val loss: 663.705, val mse: 25.86207\n",
            "Epoch #36. Time: 6.3s. Train loss: 231.210, train mse: 15.20410. Val loss: 702.928, val mse: 26.86301\n",
            "Epoch #37. Time: 6.4s. Train loss: 200.352, train mse: 14.15896. Val loss: 650.468, val mse: 25.57634\n",
            "Epoch #38. Time: 6.4s. Train loss: 195.462, train mse: 13.92675. Val loss: 713.301, val mse: 26.44141\n",
            "Epoch #39. Time: 6.4s. Train loss: 203.636, train mse: 14.27558. Val loss: 653.915, val mse: 25.84175\n",
            "Epoch #40. Time: 6.3s. Train loss: 200.561, train mse: 14.15954. Val loss: 655.831, val mse: 25.94618\n",
            "Training done. Best rmse: 24.713092803955078\n",
            "Start #10 fold\n",
            "Epoch #1. Time: 6.3s. Train loss: 5864.837, train mse: 76.69245. Val loss: 4581.419, val mse: 66.94737\n",
            "Epoch #2. Time: 6.4s. Train loss: 1286.805, train mse: 35.92556. Val loss: 881.034, val mse: 29.76114\n",
            "Epoch #3. Time: 6.3s. Train loss: 865.859, train mse: 29.46899. Val loss: 853.561, val mse: 29.16299\n",
            "Epoch #4. Time: 6.3s. Train loss: 859.043, train mse: 29.26760. Val loss: 860.286, val mse: 29.55953\n",
            "Epoch #5. Time: 6.3s. Train loss: 825.430, train mse: 28.79261. Val loss: 794.532, val mse: 27.76474\n",
            "Epoch #6. Time: 6.3s. Train loss: 817.437, train mse: 28.59939. Val loss: 787.468, val mse: 27.39671\n",
            "Epoch #7. Time: 6.4s. Train loss: 778.230, train mse: 27.86814. Val loss: 862.201, val mse: 29.52240\n",
            "Epoch #8. Time: 6.3s. Train loss: 726.146, train mse: 26.92480. Val loss: 733.956, val mse: 26.90581\n",
            "Epoch #9. Time: 6.4s. Train loss: 699.702, train mse: 26.46533. Val loss: 703.985, val mse: 26.73291\n",
            "Epoch #10. Time: 6.3s. Train loss: 650.108, train mse: 25.46146. Val loss: 773.167, val mse: 27.76660\n",
            "Epoch #11. Time: 6.4s. Train loss: 611.900, train mse: 24.69443. Val loss: 796.670, val mse: 28.33487\n",
            "Epoch #12. Time: 6.3s. Train loss: 602.993, train mse: 24.41825. Val loss: 700.000, val mse: 26.72821\n",
            "Epoch #13. Time: 6.4s. Train loss: 544.277, train mse: 23.35337. Val loss: 655.266, val mse: 26.06093\n",
            "Epoch #14. Time: 6.4s. Train loss: 475.398, train mse: 21.80649. Val loss: 678.700, val mse: 26.18155\n",
            "Epoch #15. Time: 6.3s. Train loss: 457.818, train mse: 21.35504. Val loss: 683.395, val mse: 26.19886\n",
            "Epoch #16. Time: 6.4s. Train loss: 442.610, train mse: 21.04399. Val loss: 637.343, val mse: 25.73016\n",
            "Epoch #17. Time: 6.3s. Train loss: 426.233, train mse: 20.62924. Val loss: 729.304, val mse: 26.69077\n",
            "Epoch #18. Time: 6.4s. Train loss: 395.496, train mse: 19.87578. Val loss: 662.718, val mse: 26.05715\n",
            "Epoch #19. Time: 6.4s. Train loss: 379.798, train mse: 19.41029. Val loss: 628.811, val mse: 25.51793\n",
            "Epoch #20. Time: 6.4s. Train loss: 392.008, train mse: 19.78359. Val loss: 666.398, val mse: 25.13183\n",
            "Epoch #21. Time: 6.4s. Train loss: 405.605, train mse: 20.16063. Val loss: 644.236, val mse: 25.74599\n",
            "Epoch #22. Time: 6.4s. Train loss: 395.771, train mse: 19.92226. Val loss: 670.972, val mse: 26.17842\n",
            "Epoch #23. Time: 6.4s. Train loss: 433.291, train mse: 20.78846. Val loss: 753.890, val mse: 27.83263\n",
            "Epoch #24. Time: 6.4s. Train loss: 446.397, train mse: 21.14100. Val loss: 709.430, val mse: 27.09689\n",
            "Epoch #25. Time: 6.3s. Train loss: 448.120, train mse: 21.15117. Val loss: 702.196, val mse: 26.72315\n",
            "Epoch #26. Time: 6.4s. Train loss: 442.383, train mse: 20.96551. Val loss: 683.191, val mse: 26.25104\n",
            "Epoch #27. Time: 6.3s. Train loss: 428.470, train mse: 20.71896. Val loss: 743.293, val mse: 27.61957\n",
            "Epoch #28. Time: 6.3s. Train loss: 394.604, train mse: 19.84503. Val loss: 728.982, val mse: 27.16048\n",
            "Epoch #29. Time: 6.3s. Train loss: 363.497, train mse: 19.08065. Val loss: 776.195, val mse: 27.36341\n",
            "Epoch #30. Time: 6.3s. Train loss: 360.282, train mse: 18.97833. Val loss: 730.657, val mse: 26.72993\n",
            "Epoch #31. Time: 6.4s. Train loss: 315.156, train mse: 17.74495. Val loss: 751.623, val mse: 27.65564\n",
            "Epoch #32. Time: 6.3s. Train loss: 329.881, train mse: 18.14776. Val loss: 689.330, val mse: 26.09400\n",
            "Epoch #33. Time: 6.4s. Train loss: 290.775, train mse: 17.04334. Val loss: 626.422, val mse: 25.10336\n",
            "Epoch #34. Time: 6.4s. Train loss: 257.277, train mse: 16.04995. Val loss: 622.460, val mse: 25.24158\n",
            "Epoch #35. Time: 6.4s. Train loss: 235.233, train mse: 15.36917. Val loss: 647.301, val mse: 25.59012\n",
            "Epoch #36. Time: 6.3s. Train loss: 221.381, train mse: 14.89718. Val loss: 639.319, val mse: 24.82406\n",
            "Epoch #37. Time: 6.4s. Train loss: 213.944, train mse: 14.63278. Val loss: 604.147, val mse: 24.20307\n",
            "Epoch #38. Time: 6.4s. Train loss: 198.322, train mse: 14.03484. Val loss: 604.540, val mse: 24.54339\n",
            "Epoch #39. Time: 6.4s. Train loss: 181.406, train mse: 13.46380. Val loss: 572.849, val mse: 24.02220\n",
            "Epoch #40. Time: 6.4s. Train loss: 185.900, train mse: 13.63291. Val loss: 593.149, val mse: 24.65133\n",
            "Training done. Best rmse: 24.022199630737305\n",
            "Total rmse: 22.120615005493164\n",
            "[22.944857, 20.443523, 19.873909, 23.853697, 20.806335, 21.964138, 20.037605, 22.54679, 24.713093, 24.0222]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqBA-gsfvNih"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}