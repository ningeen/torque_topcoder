{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exp1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHgLZpLpo3Wj"
      },
      "source": [
        "# from google.colab import drive\r\n",
        "# drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP4BT5QEo7Ot",
        "outputId": "79e083fe-e88d-4c68-baf9-dd36a05a9972"
      },
      "source": [
        "%cd /content/drive/MyDrive/topcoder"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/topcoder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pglu-X_4paow"
      },
      "source": [
        "# Новый раздел"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jp6My88p8on"
      },
      "source": [
        "import pickle\r\n",
        "import yaml\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import os\r\n",
        "import random\r\n",
        "import time\r\n",
        "\r\n",
        "import torch\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from torch import nn\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dbn0AJYuq_2Q"
      },
      "source": [
        "import math\r\n",
        "\r\n",
        "\r\n",
        "__all__ = ['mobilenetv3_large', 'mobilenetv3_small']\r\n",
        "\r\n",
        "\r\n",
        "def _make_divisible(v, divisor, min_value=None):\r\n",
        "    \"\"\"\r\n",
        "    This function is taken from the original tf repo.\r\n",
        "    It ensures that all layers have a channel number that is divisible by 8\r\n",
        "    It can be seen here:\r\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\r\n",
        "    :param v:\r\n",
        "    :param divisor:\r\n",
        "    :param min_value:\r\n",
        "    :return:\r\n",
        "    \"\"\"\r\n",
        "    if min_value is None:\r\n",
        "        min_value = divisor\r\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\r\n",
        "    # Make sure that round down does not go down by more than 10%.\r\n",
        "    if new_v < 0.9 * v:\r\n",
        "        new_v += divisor\r\n",
        "    return new_v\r\n",
        "\r\n",
        "\r\n",
        "class h_sigmoid(nn.Module):\r\n",
        "    def __init__(self, inplace=True):\r\n",
        "        super(h_sigmoid, self).__init__()\r\n",
        "        self.relu = nn.ReLU6(inplace=inplace)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return self.relu(x + 3) / 6\r\n",
        "\r\n",
        "\r\n",
        "class h_swish(nn.Module):\r\n",
        "    def __init__(self, inplace=True):\r\n",
        "        super(h_swish, self).__init__()\r\n",
        "        self.sigmoid = h_sigmoid(inplace=inplace)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return x * self.sigmoid(x)\r\n",
        "\r\n",
        "\r\n",
        "class SELayer(nn.Module):\r\n",
        "    def __init__(self, channel, reduction=4):\r\n",
        "        super(SELayer, self).__init__()\r\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\r\n",
        "        self.fc = nn.Sequential(\r\n",
        "                nn.Linear(channel, _make_divisible(channel // reduction, 8)),\r\n",
        "                nn.ReLU(inplace=True),\r\n",
        "                nn.Linear(_make_divisible(channel // reduction, 8), channel),\r\n",
        "                h_sigmoid()\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        b, c, _, _ = x.size()\r\n",
        "        y = self.avg_pool(x).view(b, c)\r\n",
        "        y = self.fc(y).view(b, c, 1, 1)\r\n",
        "        return x * y\r\n",
        "\r\n",
        "\r\n",
        "def conv_3x3_bn(inp, oup, stride):\r\n",
        "    return nn.Sequential(\r\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\r\n",
        "        nn.BatchNorm2d(oup),\r\n",
        "        h_swish()\r\n",
        "    )\r\n",
        "\r\n",
        "\r\n",
        "def conv_1x1_bn(inp, oup):\r\n",
        "    return nn.Sequential(\r\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\r\n",
        "        nn.BatchNorm2d(oup),\r\n",
        "        h_swish()\r\n",
        "    )\r\n",
        "\r\n",
        "\r\n",
        "class InvertedResidual(nn.Module):\r\n",
        "    def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se, use_hs):\r\n",
        "        super(InvertedResidual, self).__init__()\r\n",
        "        assert stride in [1, 2]\r\n",
        "\r\n",
        "        self.identity = stride == 1 and inp == oup\r\n",
        "\r\n",
        "        if inp == hidden_dim:\r\n",
        "            self.conv = nn.Sequential(\r\n",
        "                # dw\r\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\r\n",
        "                nn.BatchNorm2d(hidden_dim),\r\n",
        "                h_swish() if use_hs else nn.ReLU(inplace=True),\r\n",
        "                # Squeeze-and-Excite\r\n",
        "                SELayer(hidden_dim) if use_se else nn.Identity(),\r\n",
        "                # pw-linear\r\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\r\n",
        "                nn.BatchNorm2d(oup),\r\n",
        "            )\r\n",
        "        else:\r\n",
        "            self.conv = nn.Sequential(\r\n",
        "                # pw\r\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\r\n",
        "                nn.BatchNorm2d(hidden_dim),\r\n",
        "                h_swish() if use_hs else nn.ReLU(inplace=True),\r\n",
        "                # dw\r\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\r\n",
        "                nn.BatchNorm2d(hidden_dim),\r\n",
        "                # Squeeze-and-Excite\r\n",
        "                SELayer(hidden_dim) if use_se else nn.Identity(),\r\n",
        "                h_swish() if use_hs else nn.ReLU(inplace=True),\r\n",
        "                # pw-linear\r\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\r\n",
        "                nn.BatchNorm2d(oup),\r\n",
        "            )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        if self.identity:\r\n",
        "            return x + self.conv(x)\r\n",
        "        else:\r\n",
        "            return self.conv(x)\r\n",
        "\r\n",
        "\r\n",
        "class MobileNetV3(nn.Module):\r\n",
        "    def __init__(self, cfgs, mode, num_classes=1000, width_mult=1.):\r\n",
        "        super(MobileNetV3, self).__init__()\r\n",
        "        # setting of inverted residual blocks\r\n",
        "        self.cfgs = cfgs\r\n",
        "        assert mode in ['large', 'small']\r\n",
        "\r\n",
        "        # building first layer\r\n",
        "        input_channel = _make_divisible(16 * width_mult, 8)\r\n",
        "        layers = [conv_3x3_bn(3, input_channel, 2)]\r\n",
        "        # building inverted residual blocks\r\n",
        "        block = InvertedResidual\r\n",
        "        for k, t, c, use_se, use_hs, s in self.cfgs:\r\n",
        "            output_channel = _make_divisible(c * width_mult, 8)\r\n",
        "            exp_size = _make_divisible(input_channel * t, 8)\r\n",
        "            layers.append(block(input_channel, exp_size, output_channel, k, s, use_se, use_hs))\r\n",
        "            input_channel = output_channel\r\n",
        "        self.features = nn.Sequential(*layers)\r\n",
        "        # building last several layers\r\n",
        "        self.conv = conv_1x1_bn(input_channel, exp_size)\r\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\r\n",
        "        output_channel = {'large': 1280, 'small': 1024}\r\n",
        "        output_channel = _make_divisible(output_channel[mode] * width_mult, 8) if width_mult > 1.0 else output_channel[mode]\r\n",
        "        self.classifier = nn.Sequential(\r\n",
        "            nn.Linear(exp_size, output_channel),\r\n",
        "            h_swish(),\r\n",
        "            nn.Dropout(0.2),\r\n",
        "            nn.Linear(output_channel, num_classes),\r\n",
        "        )\r\n",
        "\r\n",
        "        self._initialize_weights()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.features(x)\r\n",
        "        x = self.conv(x)\r\n",
        "        x = self.avgpool(x)\r\n",
        "        x = x.view(x.size(0), -1)\r\n",
        "        x = self.classifier(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "    def _initialize_weights(self):\r\n",
        "        for m in self.modules():\r\n",
        "            if isinstance(m, nn.Conv2d):\r\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\r\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\r\n",
        "                if m.bias is not None:\r\n",
        "                    m.bias.data.zero_()\r\n",
        "            elif isinstance(m, nn.BatchNorm2d):\r\n",
        "                m.weight.data.fill_(1)\r\n",
        "                m.bias.data.zero_()\r\n",
        "            elif isinstance(m, nn.Linear):\r\n",
        "                n = m.weight.size(1)\r\n",
        "                m.weight.data.normal_(0, 0.01)\r\n",
        "                m.bias.data.zero_()\r\n",
        "\r\n",
        "\r\n",
        "def mobilenetv3_large(**kwargs):\r\n",
        "    \"\"\"\r\n",
        "    Constructs a MobileNetV3-Large model\r\n",
        "    \"\"\"\r\n",
        "    cfgs = [\r\n",
        "        # k, t, c, SE, HS, s \r\n",
        "        [3,   1,  16, 0, 0, 1],\r\n",
        "        [3,   4,  24, 0, 0, 2],\r\n",
        "        [3,   3,  24, 0, 0, 1],\r\n",
        "        [5,   3,  40, 1, 0, 2],\r\n",
        "        [5,   3,  40, 1, 0, 1],\r\n",
        "        [5,   3,  40, 1, 0, 1],\r\n",
        "        [3,   6,  80, 0, 1, 2],\r\n",
        "        [3, 2.5,  80, 0, 1, 1],\r\n",
        "        [3, 2.3,  80, 0, 1, 1],\r\n",
        "        [3, 2.3,  80, 0, 1, 1],\r\n",
        "        [3,   6, 112, 1, 1, 1],\r\n",
        "        [3,   6, 112, 1, 1, 1],\r\n",
        "        [5,   6, 160, 1, 1, 2],\r\n",
        "        [5,   6, 160, 1, 1, 1],\r\n",
        "        [5,   6, 160, 1, 1, 1]\r\n",
        "    ]\r\n",
        "    return MobileNetV3(cfgs, mode='large', **kwargs)\r\n",
        "\r\n",
        "\r\n",
        "def mobilenetv3_small(**kwargs):\r\n",
        "    \"\"\"\r\n",
        "    Constructs a MobileNetV3-Small model\r\n",
        "    \"\"\"\r\n",
        "    cfgs = [\r\n",
        "        # k, t, c, SE, HS, s \r\n",
        "        [3,    1,  16, 1, 0, 2],\r\n",
        "        [3,  4.5,  24, 0, 0, 2],\r\n",
        "        [3, 3.67,  24, 0, 0, 1],\r\n",
        "        [5,    4,  40, 1, 1, 2],\r\n",
        "        [5,    6,  40, 1, 1, 1],\r\n",
        "        [5,    6,  40, 1, 1, 1],\r\n",
        "        [5,    3,  48, 1, 1, 1],\r\n",
        "        [5,    3,  48, 1, 1, 1],\r\n",
        "        [5,    6,  96, 1, 1, 2],\r\n",
        "        [5,    6,  96, 1, 1, 1],\r\n",
        "        [5,    6,  96, 1, 1, 1],\r\n",
        "    ]\r\n",
        "\r\n",
        "    return MobileNetV3(cfgs, mode='small', **kwargs)\r\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13rtbK2h8sCE"
      },
      "source": [
        "import math\r\n",
        "import torch\r\n",
        "from torch.optim.lr_scheduler import _LRScheduler\r\n",
        "\r\n",
        "class CosineAnnealingWarmupRestarts(_LRScheduler):\r\n",
        "    \"\"\"\r\n",
        "        optimizer (Optimizer): Wrapped optimizer.\r\n",
        "        first_cycle_steps (int): First cycle step size.\r\n",
        "        cycle_mult(float): Cycle steps magnification. Default: -1.\r\n",
        "        max_lr(float): First cycle's max learning rate. Default: 0.1.\r\n",
        "        min_lr(float): Min learning rate. Default: 0.001.\r\n",
        "        warmup_steps(int): Linear warmup step size. Default: 0.\r\n",
        "        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\r\n",
        "        last_epoch (int): The index of last epoch. Default: -1.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    def __init__(self,\r\n",
        "                 optimizer : torch.optim.Optimizer,\r\n",
        "                 first_cycle_steps : int,\r\n",
        "                 cycle_mult : float = 1.,\r\n",
        "                 max_lr : float = 0.1,\r\n",
        "                 min_lr : float = 0.001,\r\n",
        "                 warmup_steps : int = 0,\r\n",
        "                 gamma : float = 1.,\r\n",
        "                 last_epoch : int = -1\r\n",
        "        ):\r\n",
        "        assert warmup_steps < first_cycle_steps\r\n",
        "        \r\n",
        "        self.first_cycle_steps = first_cycle_steps # first cycle step size\r\n",
        "        self.cycle_mult = cycle_mult # cycle steps magnification\r\n",
        "        self.base_max_lr = max_lr # first max learning rate\r\n",
        "        self.max_lr = max_lr # max learning rate in the current cycle\r\n",
        "        self.min_lr = min_lr # min learning rate\r\n",
        "        self.warmup_steps = warmup_steps # warmup step size\r\n",
        "        self.gamma = gamma # decrease rate of max learning rate by cycle\r\n",
        "        \r\n",
        "        self.cur_cycle_steps = first_cycle_steps # first cycle step size\r\n",
        "        self.cycle = 0 # cycle count\r\n",
        "        self.step_in_cycle = last_epoch # step size of the current cycle\r\n",
        "        \r\n",
        "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\r\n",
        "        \r\n",
        "        # set learning rate min_lr\r\n",
        "        self.init_lr()\r\n",
        "    \r\n",
        "    def init_lr(self):\r\n",
        "        self.base_lrs = []\r\n",
        "        for param_group in self.optimizer.param_groups:\r\n",
        "            param_group['lr'] = self.min_lr\r\n",
        "            self.base_lrs.append(self.min_lr)\r\n",
        "    \r\n",
        "    def get_lr(self):\r\n",
        "        if self.step_in_cycle == -1:\r\n",
        "            return self.base_lrs\r\n",
        "        elif self.step_in_cycle < self.warmup_steps:\r\n",
        "            return [(self.max_lr - base_lr)*self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\r\n",
        "        else:\r\n",
        "            return [base_lr + (self.max_lr - base_lr) \\\r\n",
        "                    * (1 + math.cos(math.pi * (self.step_in_cycle-self.warmup_steps) \\\r\n",
        "                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\r\n",
        "                    for base_lr in self.base_lrs]\r\n",
        "\r\n",
        "    def step(self, epoch=None):\r\n",
        "        if epoch is None:\r\n",
        "            epoch = self.last_epoch + 1\r\n",
        "            self.step_in_cycle = self.step_in_cycle + 1\r\n",
        "            if self.step_in_cycle >= self.cur_cycle_steps:\r\n",
        "                self.cycle += 1\r\n",
        "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\r\n",
        "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\r\n",
        "        else:\r\n",
        "            if epoch >= self.first_cycle_steps:\r\n",
        "                if self.cycle_mult == 1.:\r\n",
        "                    self.step_in_cycle = epoch % self.first_cycle_steps\r\n",
        "                    self.cycle = epoch // self.first_cycle_steps\r\n",
        "                else:\r\n",
        "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\r\n",
        "                    self.cycle = n\r\n",
        "                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\r\n",
        "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\r\n",
        "            else:\r\n",
        "                self.cur_cycle_steps = self.first_cycle_steps\r\n",
        "                self.step_in_cycle = epoch\r\n",
        "                \r\n",
        "        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\r\n",
        "        self.last_epoch = math.floor(epoch)\r\n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\r\n",
        "            param_group['lr'] = lr"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4ElywIvqA6K"
      },
      "source": [
        "CONFIG_PATH = \"proj_config.yaml\"\r\n",
        "with open(CONFIG_PATH, 'r') as stream:\r\n",
        "    CONFIG = yaml.safe_load(stream)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G43KN85lp4D0"
      },
      "source": [
        "def uniform_len(mel, input_len):\r\n",
        "    mel_len = mel.shape[-1]\r\n",
        "    if mel_len > input_len:\r\n",
        "        diff = mel_len - input_len\r\n",
        "        start = np.random.randint(diff)\r\n",
        "        end = start + input_len\r\n",
        "        mel = mel[:, start: end]\r\n",
        "    elif mel_len < input_len:\r\n",
        "        diff = input_len - mel_len\r\n",
        "        offset = np.random.randint(diff)\r\n",
        "        offset_right = diff - offset\r\n",
        "        mel = np.pad(\r\n",
        "            mel,\r\n",
        "            ((0, 0), (offset, offset_right)),\r\n",
        "            \"symmetric\",  # constant\r\n",
        "        )\r\n",
        "    return mel\r\n",
        "\r\n",
        "\r\n",
        "class TorqueDataset(Dataset):\r\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, data, mel_logs, labels=None, transform=None):\r\n",
        "        \"\"\"Init Dataset\"\"\"\r\n",
        "        self.mel_logs = mel_logs\r\n",
        "        self.data = data\r\n",
        "        self.labels = labels\r\n",
        "        self.transform = transform\r\n",
        "        self.input_len = CONFIG['mel']['mel_len']\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        \"\"\"Length\"\"\"\r\n",
        "        return len(self.mel_logs)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        \"\"\"Generates one sample of data\"\"\"\r\n",
        "        table_data = self.data[[index]]\r\n",
        "\r\n",
        "        label = None\r\n",
        "        if self.labels is not None:\r\n",
        "            label = self.labels[[index]]\r\n",
        "\r\n",
        "        mel_data = uniform_len(self.mel_logs[index], self.input_len)\r\n",
        "        if self.transform:\r\n",
        "            mel_data = self.transform(mel_data)\r\n",
        "\r\n",
        "        mel_data = np.expand_dims(mel_data, axis=0)\r\n",
        "        return mel_data, label"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdOpMc7Rp-AZ"
      },
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "def seed_everything(seed=1234):\r\n",
        "    \"\"\"Fix random seeds\"\"\"\r\n",
        "    random.seed(seed)\r\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\r\n",
        "    np.random.seed(seed)\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    torch.cuda.manual_seed(seed)\r\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vJdztTNqiv0"
      },
      "source": [
        "def get_model(pretrained_mn3_path=\"\", pretrained_path=\"\"):\r\n",
        "    \"\"\"Load MobilenetV3 model with specified in and out channels\"\"\"\r\n",
        "    # model = mobilenetv3_small().to(DEVICE)\r\n",
        "    model = mobilenetv3_large().to(DEVICE)\r\n",
        "    if pretrained_mn3_path and not pretrained_path:\r\n",
        "        model.load_state_dict(torch.load(pretrained_mn3_path))\r\n",
        "\r\n",
        "    model.features[0][0].weight.data = torch.sum(\r\n",
        "        model.features[0][0].weight.data, dim=1, keepdim=True\r\n",
        "    )\r\n",
        "    model.features[0][0].in_channels = 1\r\n",
        "\r\n",
        "    model.classifier[-1].weight.data = torch.sum(\r\n",
        "        model.classifier[-1].weight.data, dim=0, keepdim=True\r\n",
        "    )\r\n",
        "\r\n",
        "    model.classifier[-1].bias.data = torch.sum(\r\n",
        "        model.classifier[-1].bias.data, dim=0, keepdim=True\r\n",
        "    )\r\n",
        "    model.classifier[-1].out_features = 1\r\n",
        "\r\n",
        "    if pretrained_path:\r\n",
        "        model.load_state_dict(torch.load(pretrained_path))\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "def process_epoch(model, criterion, optimizer, loader):\r\n",
        "    \"\"\"Calc one epoch\"\"\"\r\n",
        "    losses = []\r\n",
        "    y_true = []\r\n",
        "    y_pred = []\r\n",
        "    with torch.set_grad_enabled(model.training):\r\n",
        "        for local_batch, local_labels in loader:\r\n",
        "            local_batch, local_labels = \\\r\n",
        "                local_batch.to(DEVICE), local_labels.to(DEVICE)\r\n",
        "\r\n",
        "            optimizer.zero_grad()\r\n",
        "            outputs = model(local_batch)\r\n",
        "\r\n",
        "            loss = criterion(outputs, local_labels)\r\n",
        "            if model.training:\r\n",
        "                loss.backward()\r\n",
        "                optimizer.step()\r\n",
        "\r\n",
        "            losses.append(loss)\r\n",
        "            y_true.append(local_labels.detach().cpu().numpy())\r\n",
        "            y_pred.append(outputs.data.detach().cpu().numpy())\r\n",
        "    loss_train = np.array(losses).astype(np.float32).mean()\r\n",
        "    y_true = np.concatenate(y_true)\r\n",
        "    y_pred = np.concatenate(y_pred)\r\n",
        "    rmse_train = mean_squared_error(y_true, y_pred, squared=False)\r\n",
        "    return loss_train, rmse_train, y_true, y_pred"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT95qJpQqlu9"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, train_loader, test_loader, n_fold):\r\n",
        "    \"\"\"Training loop\"\"\"\r\n",
        "    logs = {'loss_train': [], 'loss_val': [], 'mse_train': [], 'mse_val': []}\r\n",
        "    best_true = None\r\n",
        "    best_pred = None\r\n",
        "    for epoch in range(CONFIG['num_epochs']):\r\n",
        "        start_time = time.time()\r\n",
        "        scheduler.step()\r\n",
        "\r\n",
        "        # Training\r\n",
        "        model.train()\r\n",
        "        loss_train, mse_train, _, _ = \\\r\n",
        "            process_epoch(model, criterion, optimizer, train_loader)\r\n",
        "        logs['loss_train'].append(loss_train)\r\n",
        "        logs['mse_train'].append(mse_train)\r\n",
        "\r\n",
        "        # Validation\r\n",
        "        model.eval()\r\n",
        "        loss_val, mse_val, y_true, y_pred = \\\r\n",
        "            process_epoch(model, criterion, optimizer, test_loader)\r\n",
        "        logs['loss_val'].append(loss_val)\r\n",
        "        logs['mse_val'].append(mse_val)\r\n",
        "        print(\r\n",
        "            f\"Epoch #{epoch + 1}. \"\r\n",
        "            f\"Time: {(time.time() - start_time):.1f}s. \"\r\n",
        "            f\"Train loss: {loss_train:.3f}, train mse: {mse_train:.5f}. \"\r\n",
        "            f\"Val loss: {loss_val:.3f}, val mse: {mse_val:.5f}\"\r\n",
        "        )\r\n",
        "        if mse_val <= np.min(logs['mse_val']):\r\n",
        "            if CONFIG['save_model']:\r\n",
        "                torch.save(\r\n",
        "                    model.state_dict(),\r\n",
        "                    os.path.join(\r\n",
        "                        CONFIG['model_dir'],\r\n",
        "                        f\"work_{CONFIG['experiment_name']}_fold{n_fold}.pt\"\r\n",
        "                    )\r\n",
        "                )\r\n",
        "            best_true = y_true\r\n",
        "            best_pred = y_pred\r\n",
        "    return best_true, best_pred\r\n",
        "\r\n",
        "\r\n",
        "def run_training():\r\n",
        "    with open(CONFIG['data_path'], 'rb') as f:\r\n",
        "        (data, mel_logs, target) = pickle.load(f)\r\n",
        "\r\n",
        "    folds = KFold(\r\n",
        "        n_splits=CONFIG['n_folds'],\r\n",
        "        shuffle=True,\r\n",
        "        random_state=CONFIG['fold_seed']\r\n",
        "    )\r\n",
        "    splits = list(folds.split(mel_logs))\r\n",
        "\r\n",
        "    total_rmse = list()\r\n",
        "\r\n",
        "    for n_fold, (train_idx, val_idx) in enumerate(splits):\r\n",
        "        print(f\"Start #{n_fold + 1} fold\")\r\n",
        "        train_dataset = TorqueDataset(\r\n",
        "            data[train_idx],\r\n",
        "            [mel_logs[i] for i in train_idx],\r\n",
        "            target[train_idx]\r\n",
        "        )\r\n",
        "        val_dataset = TorqueDataset(\r\n",
        "            data[val_idx],\r\n",
        "            [mel_logs[i] for i in val_idx],\r\n",
        "            target[val_idx]\r\n",
        "        )\r\n",
        "        train_loader = DataLoader(train_dataset, **CONFIG['loader_params'])\r\n",
        "        val_loader = DataLoader(val_dataset, **CONFIG['loader_params'])\r\n",
        "\r\n",
        "        model = get_model(CONFIG['pretrained_path'])\r\n",
        "        criterion = nn.MSELoss()\r\n",
        "        optimizer = torch.optim.Adam(model.parameters(), CONFIG['lr'])\r\n",
        "        scheduler = CosineAnnealingWarmupRestarts(optimizer, **CONFIG['scheduler_params'])\r\n",
        "\r\n",
        "        best_true, best_pred = \\\r\n",
        "            train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, n_fold)\r\n",
        "\r\n",
        "        rmse = mean_squared_error(best_true, best_pred, squared=False)\r\n",
        "        print(f\"Training done. Best rmse: {rmse}\")\r\n",
        "        total_rmse.append(rmse)\r\n",
        "    print(f\"Total rmse: {np.mean(total_rmse)}\")"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1aNynqjtHPj"
      },
      "source": [
        "seed_everything()"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiLbhqtFtEMt"
      },
      "source": [
        "CONFIG['loader_params'] = {'batch_size': 16, 'shuffle': True, 'num_workers': 4}\r\n",
        "CONFIG['lr'] = 0.00003\r\n",
        "CONFIG['num_epochs'] = 16\r\n",
        "\r\n",
        "CONFIG['pretrained_path'] = './pretrained/mobilenetv3-large-1cd25616.pth'\r\n",
        "\r\n",
        "CONFIG['scheduler_params'] = {'first_cycle_steps':8,\r\n",
        "                            'cycle_mult':1.0,\r\n",
        "                            'max_lr':CONFIG['lr'] * 5,\r\n",
        "                            'min_lr':CONFIG['lr'] / 10,\r\n",
        "                            'warmup_steps':2,\r\n",
        "                            'gamma':1.0}"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJcuLg6jquZj",
        "outputId": "ed347483-c674-4654-da9e-38fc9c57ff1c"
      },
      "source": [
        "run_training()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start #1 fold\n",
            "Epoch #1. Time: 6.4s. Train loss: 6510.379, train mse: 80.71249. Val loss: 6343.029, val mse: 78.99902\n",
            "Epoch #2. Time: 6.5s. Train loss: 2655.947, train mse: 51.67586. Val loss: 819.527, val mse: 28.70235\n",
            "Epoch #3. Time: 6.5s. Train loss: 592.408, train mse: 24.33532. Val loss: 675.616, val mse: 25.62947\n",
            "Epoch #4. Time: 6.5s. Train loss: 449.159, train mse: 21.19279. Val loss: 662.370, val mse: 26.05268\n",
            "Epoch #5. Time: 6.6s. Train loss: 367.134, train mse: 19.15670. Val loss: 622.818, val mse: 24.77261\n",
            "Epoch #6. Time: 6.5s. Train loss: 287.236, train mse: 16.93477. Val loss: 602.057, val mse: 24.79088\n",
            "Epoch #7. Time: 6.5s. Train loss: 253.311, train mse: 15.94760. Val loss: 601.848, val mse: 23.82651\n",
            "Epoch #8. Time: 6.4s. Train loss: 246.747, train mse: 15.73232. Val loss: 600.693, val mse: 24.31365\n",
            "Epoch #9. Time: 6.4s. Train loss: 268.951, train mse: 16.42664. Val loss: 636.703, val mse: 25.46850\n",
            "Epoch #10. Time: 6.4s. Train loss: 284.551, train mse: 16.88956. Val loss: 863.551, val mse: 29.31600\n",
            "Epoch #11. Time: 6.4s. Train loss: 255.574, train mse: 15.99951. Val loss: 809.855, val mse: 28.59990\n",
            "Epoch #12. Time: 6.4s. Train loss: 199.541, train mse: 14.08493. Val loss: 534.602, val mse: 23.50529\n",
            "Epoch #13. Time: 6.4s. Train loss: 165.887, train mse: 12.91215. Val loss: 522.290, val mse: 23.06910\n",
            "Epoch #14. Time: 6.4s. Train loss: 140.384, train mse: 11.73096. Val loss: 580.627, val mse: 24.00091\n",
            "Epoch #15. Time: 6.5s. Train loss: 135.772, train mse: 11.63956. Val loss: 575.921, val mse: 24.05280\n",
            "Epoch #16. Time: 6.4s. Train loss: 119.495, train mse: 10.93079. Val loss: 575.208, val mse: 24.04713\n",
            "Training done. Best rmse: 23.06909942626953\n",
            "Start #2 fold\n",
            "Epoch #1. Time: 6.5s. Train loss: 6515.320, train mse: 80.77354. Val loss: 5962.231, val mse: 77.19158\n",
            "Epoch #2. Time: 6.5s. Train loss: 2546.869, train mse: 50.59023. Val loss: 713.650, val mse: 26.80799\n",
            "Epoch #3. Time: 6.4s. Train loss: 592.231, train mse: 24.30850. Val loss: 803.374, val mse: 28.43238\n",
            "Epoch #4. Time: 6.5s. Train loss: 426.811, train mse: 20.67331. Val loss: 479.656, val mse: 22.01132\n",
            "Epoch #5. Time: 6.4s. Train loss: 355.139, train mse: 18.87309. Val loss: 543.586, val mse: 23.31605\n",
            "Epoch #6. Time: 6.4s. Train loss: 291.261, train mse: 16.98589. Val loss: 512.830, val mse: 22.99411\n",
            "Epoch #7. Time: 6.5s. Train loss: 262.326, train mse: 16.06298. Val loss: 552.104, val mse: 23.47246\n",
            "Epoch #8. Time: 6.4s. Train loss: 238.767, train mse: 15.43424. Val loss: 549.741, val mse: 22.39721\n",
            "Epoch #9. Time: 6.4s. Train loss: 217.817, train mse: 14.76474. Val loss: 557.535, val mse: 23.84447\n",
            "Epoch #10. Time: 6.4s. Train loss: 257.617, train mse: 16.02508. Val loss: 589.033, val mse: 24.30927\n",
            "Epoch #11. Time: 6.4s. Train loss: 251.738, train mse: 15.85689. Val loss: 628.014, val mse: 25.37542\n",
            "Epoch #12. Time: 6.4s. Train loss: 197.481, train mse: 14.07872. Val loss: 755.941, val mse: 27.92845\n",
            "Epoch #13. Time: 6.4s. Train loss: 179.175, train mse: 13.38193. Val loss: 489.103, val mse: 21.98188\n",
            "Epoch #14. Time: 6.5s. Train loss: 141.975, train mse: 11.93878. Val loss: 559.141, val mse: 23.68002\n",
            "Epoch #15. Time: 6.4s. Train loss: 124.715, train mse: 11.17927. Val loss: 531.273, val mse: 23.33134\n",
            "Epoch #16. Time: 6.5s. Train loss: 116.982, train mse: 10.84311. Val loss: 631.044, val mse: 25.14744\n",
            "Training done. Best rmse: 21.98187828063965\n",
            "Start #3 fold\n",
            "Epoch #1. Time: 6.4s. Train loss: 6475.766, train mse: 80.56168. Val loss: 6425.920, val mse: 79.66841\n",
            "Epoch #2. Time: 6.5s. Train loss: 2557.009, train mse: 50.61974. Val loss: 691.922, val mse: 26.09687\n",
            "Epoch #3. Time: 6.4s. Train loss: 588.883, train mse: 24.28123. Val loss: 539.370, val mse: 23.47127\n",
            "Epoch #4. Time: 6.5s. Train loss: 478.389, train mse: 21.86099. Val loss: 663.619, val mse: 25.60693\n",
            "Epoch #5. Time: 6.4s. Train loss: 338.092, train mse: 18.39439. Val loss: 556.473, val mse: 23.65482\n",
            "Epoch #6. Time: 6.4s. Train loss: 317.568, train mse: 17.81416. Val loss: 529.713, val mse: 22.99068\n",
            "Epoch #7. Time: 6.4s. Train loss: 254.730, train mse: 15.95496. Val loss: 538.220, val mse: 22.96287\n",
            "Epoch #8. Time: 6.4s. Train loss: 240.076, train mse: 15.45484. Val loss: 548.591, val mse: 23.18810\n",
            "Epoch #9. Time: 6.5s. Train loss: 286.678, train mse: 16.92759. Val loss: 577.449, val mse: 23.99027\n",
            "Epoch #10. Time: 6.4s. Train loss: 265.948, train mse: 16.33707. Val loss: 530.086, val mse: 23.27918\n",
            "Epoch #11. Time: 6.5s. Train loss: 242.206, train mse: 15.52170. Val loss: 525.414, val mse: 22.98325\n",
            "Epoch #12. Time: 6.4s. Train loss: 209.125, train mse: 14.49637. Val loss: 491.648, val mse: 22.38845\n",
            "Epoch #13. Time: 6.4s. Train loss: 156.260, train mse: 12.51396. Val loss: 513.092, val mse: 22.99344\n",
            "Epoch #14. Time: 6.4s. Train loss: 160.760, train mse: 12.69613. Val loss: 568.440, val mse: 23.39924\n",
            "Epoch #15. Time: 6.4s. Train loss: 133.127, train mse: 11.51791. Val loss: 551.267, val mse: 23.43753\n",
            "Epoch #16. Time: 6.4s. Train loss: 132.324, train mse: 11.52038. Val loss: 574.267, val mse: 24.09822\n",
            "Training done. Best rmse: 22.388452529907227\n",
            "Start #4 fold\n",
            "Epoch #1. Time: 6.4s. Train loss: 6527.733, train mse: 80.87440. Val loss: 6116.631, val mse: 77.46353\n",
            "Epoch #2. Time: 6.5s. Train loss: 2596.799, train mse: 51.03905. Val loss: 955.703, val mse: 31.29520\n",
            "Epoch #3. Time: 6.4s. Train loss: 583.563, train mse: 24.18613. Val loss: 704.311, val mse: 26.52709\n",
            "Epoch #4. Time: 6.4s. Train loss: 458.118, train mse: 21.44135. Val loss: 877.662, val mse: 29.83204\n",
            "Epoch #5. Time: 6.4s. Train loss: 358.616, train mse: 18.92463. Val loss: 641.146, val mse: 25.26218\n",
            "Epoch #6. Time: 6.5s. Train loss: 284.441, train mse: 16.81430. Val loss: 618.486, val mse: 24.52140\n",
            "Epoch #7. Time: 6.4s. Train loss: 257.509, train mse: 15.89888. Val loss: 603.507, val mse: 24.94833\n",
            "Epoch #8. Time: 6.5s. Train loss: 251.293, train mse: 15.86557. Val loss: 686.858, val mse: 25.87357\n",
            "Epoch #9. Time: 6.4s. Train loss: 269.076, train mse: 16.39019. Val loss: 659.957, val mse: 25.42984\n",
            "Epoch #10. Time: 6.4s. Train loss: 301.670, train mse: 17.36576. Val loss: 705.235, val mse: 26.56240\n",
            "Epoch #11. Time: 6.4s. Train loss: 288.834, train mse: 17.03263. Val loss: 714.191, val mse: 26.99883\n",
            "Epoch #12. Time: 6.4s. Train loss: 218.866, train mse: 14.70536. Val loss: 639.002, val mse: 25.35556\n",
            "Epoch #13. Time: 6.5s. Train loss: 165.347, train mse: 12.88024. Val loss: 576.772, val mse: 23.98949\n",
            "Epoch #14. Time: 6.5s. Train loss: 145.562, train mse: 12.02286. Val loss: 568.543, val mse: 23.63179\n",
            "Epoch #15. Time: 6.4s. Train loss: 140.754, train mse: 11.87921. Val loss: 614.901, val mse: 24.66001\n",
            "Epoch #16. Time: 6.4s. Train loss: 141.751, train mse: 11.88097. Val loss: 561.809, val mse: 23.53386\n",
            "Training done. Best rmse: 23.533859252929688\n",
            "Start #5 fold\n",
            "Epoch #1. Time: 6.5s. Train loss: 6523.207, train mse: 80.80186. Val loss: 6307.474, val mse: 78.68482\n",
            "Epoch #2. Time: 6.4s. Train loss: 2600.526, train mse: 51.12178. Val loss: 695.767, val mse: 25.95381\n",
            "Epoch #3. Time: 6.4s. Train loss: 622.752, train mse: 24.92064. Val loss: 602.987, val mse: 24.12321\n",
            "Epoch #4. Time: 6.5s. Train loss: 463.357, train mse: 21.48219. Val loss: 602.698, val mse: 24.82569\n",
            "Epoch #5. Time: 6.4s. Train loss: 360.580, train mse: 19.00733. Val loss: 681.761, val mse: 26.17780\n",
            "Epoch #6. Time: 6.5s. Train loss: 285.166, train mse: 16.88367. Val loss: 562.380, val mse: 23.69836\n",
            "Epoch #7. Time: 6.5s. Train loss: 232.387, train mse: 15.21593. Val loss: 535.412, val mse: 23.23354\n",
            "Epoch #8. Time: 6.5s. Train loss: 244.079, train mse: 15.48873. Val loss: 554.395, val mse: 23.58443\n",
            "Epoch #9. Time: 6.5s. Train loss: 276.792, train mse: 16.60749. Val loss: 609.799, val mse: 24.44010\n",
            "Epoch #10. Time: 6.5s. Train loss: 259.395, train mse: 16.08886. Val loss: 533.280, val mse: 23.13126\n",
            "Epoch #11. Time: 6.4s. Train loss: 217.008, train mse: 14.61802. Val loss: 550.740, val mse: 23.07406\n",
            "Epoch #12. Time: 6.4s. Train loss: 208.292, train mse: 14.39714. Val loss: 622.142, val mse: 25.21924\n",
            "Epoch #13. Time: 6.4s. Train loss: 194.230, train mse: 13.96001. Val loss: 516.464, val mse: 22.92504\n",
            "Epoch #14. Time: 6.5s. Train loss: 156.951, train mse: 12.54700. Val loss: 545.814, val mse: 23.20654\n",
            "Epoch #15. Time: 6.4s. Train loss: 122.555, train mse: 11.06882. Val loss: 479.208, val mse: 22.01533\n",
            "Epoch #16. Time: 6.4s. Train loss: 128.116, train mse: 11.22662. Val loss: 530.770, val mse: 23.33667\n",
            "Training done. Best rmse: 22.01532745361328\n",
            "Start #6 fold\n",
            "Epoch #1. Time: 6.4s. Train loss: 6582.091, train mse: 81.08207. Val loss: 5503.889, val mse: 74.90552\n",
            "Epoch #2. Time: 6.5s. Train loss: 2630.648, train mse: 51.38899. Val loss: 741.856, val mse: 27.72893\n",
            "Epoch #3. Time: 6.4s. Train loss: 564.835, train mse: 23.79757. Val loss: 704.757, val mse: 26.03488\n",
            "Epoch #4. Time: 6.5s. Train loss: 441.732, train mse: 21.03445. Val loss: 764.706, val mse: 27.85321\n",
            "Epoch #5. Time: 6.5s. Train loss: 347.770, train mse: 18.57419. Val loss: 760.978, val mse: 27.32789\n",
            "Epoch #6. Time: 6.4s. Train loss: 289.607, train mse: 17.04406. Val loss: 715.382, val mse: 27.09674\n",
            "Epoch #7. Time: 6.4s. Train loss: 225.228, train mse: 15.01706. Val loss: 700.985, val mse: 26.20534\n",
            "Epoch #8. Time: 6.4s. Train loss: 220.777, train mse: 14.87236. Val loss: 713.345, val mse: 26.47854\n",
            "Epoch #9. Time: 6.4s. Train loss: 253.510, train mse: 15.89740. Val loss: 654.535, val mse: 25.91450\n",
            "Epoch #10. Time: 6.4s. Train loss: 256.874, train mse: 15.98865. Val loss: 726.242, val mse: 27.17896\n",
            "Epoch #11. Time: 6.4s. Train loss: 261.811, train mse: 16.18813. Val loss: 759.294, val mse: 27.06889\n",
            "Epoch #12. Time: 6.5s. Train loss: 206.255, train mse: 14.35757. Val loss: 720.200, val mse: 26.84152\n",
            "Epoch #13. Time: 6.4s. Train loss: 162.128, train mse: 12.71832. Val loss: 667.749, val mse: 26.19181\n",
            "Epoch #14. Time: 6.5s. Train loss: 137.701, train mse: 11.71518. Val loss: 739.745, val mse: 26.85356\n",
            "Epoch #15. Time: 6.4s. Train loss: 129.358, train mse: 11.38911. Val loss: 740.762, val mse: 27.14664\n",
            "Epoch #16. Time: 6.5s. Train loss: 122.818, train mse: 11.09775. Val loss: 699.417, val mse: 26.92008\n",
            "Training done. Best rmse: 25.91450309753418\n",
            "Start #7 fold\n",
            "Epoch #1. Time: 6.4s. Train loss: 6564.010, train mse: 81.04604. Val loss: 5712.922, val mse: 76.12356\n",
            "Epoch #2. Time: 6.4s. Train loss: 2616.944, train mse: 51.27913. Val loss: 912.377, val mse: 30.23116\n",
            "Epoch #3. Time: 6.5s. Train loss: 630.178, train mse: 25.07688. Val loss: 728.743, val mse: 27.09303\n",
            "Epoch #4. Time: 6.5s. Train loss: 447.981, train mse: 21.12388. Val loss: 544.912, val mse: 23.49029\n",
            "Epoch #5. Time: 6.5s. Train loss: 369.695, train mse: 19.20245. Val loss: 578.199, val mse: 23.91489\n",
            "Epoch #6. Time: 6.4s. Train loss: 286.708, train mse: 16.94204. Val loss: 531.098, val mse: 22.81374\n",
            "Epoch #7. Time: 6.5s. Train loss: 280.639, train mse: 16.63492. Val loss: 517.868, val mse: 22.99464\n",
            "Epoch #8. Time: 6.5s. Train loss: 249.224, train mse: 15.80476. Val loss: 542.091, val mse: 23.46964\n",
            "Epoch #9. Time: 6.4s. Train loss: 262.329, train mse: 16.22488. Val loss: 587.198, val mse: 24.01567\n",
            "Epoch #10. Time: 6.5s. Train loss: 279.872, train mse: 16.75349. Val loss: 566.999, val mse: 23.86292\n",
            "Epoch #11. Time: 6.4s. Train loss: 265.188, train mse: 16.30300. Val loss: 1017.462, val mse: 31.49204\n",
            "Epoch #12. Time: 6.4s. Train loss: 215.985, train mse: 14.72830. Val loss: 549.851, val mse: 23.62888\n",
            "Epoch #13. Time: 6.5s. Train loss: 201.536, train mse: 14.14104. Val loss: 543.329, val mse: 23.51355\n",
            "Epoch #14. Time: 6.5s. Train loss: 169.276, train mse: 12.98566. Val loss: 557.540, val mse: 23.78017\n",
            "Epoch #15. Time: 6.4s. Train loss: 117.841, train mse: 10.86185. Val loss: 561.019, val mse: 23.80339\n",
            "Epoch #16. Time: 6.4s. Train loss: 144.764, train mse: 11.98224. Val loss: 532.303, val mse: 23.03635\n",
            "Training done. Best rmse: 22.813743591308594\n",
            "Start #8 fold\n",
            "Epoch #1. Time: 6.5s. Train loss: 6563.233, train mse: 81.08402. Val loss: 5701.742, val mse: 74.98087\n",
            "Epoch #2. Time: 6.4s. Train loss: 2636.264, train mse: 51.45861. Val loss: 608.967, val mse: 25.02907\n",
            "Epoch #3. Time: 6.4s. Train loss: 603.293, train mse: 24.58437. Val loss: 781.992, val mse: 27.90750\n",
            "Epoch #4. Time: 6.4s. Train loss: 492.324, train mse: 22.18302. Val loss: 560.119, val mse: 23.40961\n",
            "Epoch #5. Time: 6.5s. Train loss: 341.436, train mse: 18.47923. Val loss: 579.988, val mse: 24.06996\n",
            "Epoch #6. Time: 6.4s. Train loss: 303.716, train mse: 17.40891. Val loss: 570.263, val mse: 23.76538\n",
            "Epoch #7. Time: 6.4s. Train loss: 273.158, train mse: 16.55087. Val loss: 548.389, val mse: 23.27942\n",
            "Epoch #8. Time: 6.4s. Train loss: 256.892, train mse: 16.04803. Val loss: 532.448, val mse: 23.53364\n",
            "Epoch #9. Time: 6.5s. Train loss: 254.021, train mse: 15.93962. Val loss: 549.712, val mse: 23.53212\n",
            "Epoch #10. Time: 6.5s. Train loss: 281.320, train mse: 16.74778. Val loss: 555.985, val mse: 23.85624\n",
            "Epoch #11. Time: 6.5s. Train loss: 259.190, train mse: 16.12197. Val loss: 596.543, val mse: 24.76455\n",
            "Epoch #12. Time: 6.4s. Train loss: 229.160, train mse: 15.16158. Val loss: 550.674, val mse: 23.66763\n",
            "Epoch #13. Time: 6.5s. Train loss: 177.235, train mse: 13.26115. Val loss: 527.934, val mse: 23.20874\n",
            "Epoch #14. Time: 6.5s. Train loss: 153.917, train mse: 12.42077. Val loss: 536.079, val mse: 23.48679\n",
            "Epoch #15. Time: 6.5s. Train loss: 160.053, train mse: 12.66811. Val loss: 560.537, val mse: 23.72624\n",
            "Epoch #16. Time: 6.4s. Train loss: 128.391, train mse: 11.18951. Val loss: 504.429, val mse: 22.57069\n",
            "Training done. Best rmse: 22.57069206237793\n",
            "Start #9 fold\n",
            "Epoch #1. Time: 6.5s. Train loss: 6470.710, train mse: 80.49295. Val loss: 6384.484, val mse: 80.50777\n",
            "Epoch #2. Time: 6.4s. Train loss: 2582.222, train mse: 50.93975. Val loss: 954.475, val mse: 31.26322\n",
            "Epoch #3. Time: 6.4s. Train loss: 581.700, train mse: 24.16992. Val loss: 797.081, val mse: 28.61900\n",
            "Epoch #4. Time: 6.5s. Train loss: 461.498, train mse: 21.40285. Val loss: 786.116, val mse: 28.08092\n",
            "Epoch #5. Time: 6.5s. Train loss: 336.927, train mse: 18.38357. Val loss: 713.091, val mse: 26.67874\n",
            "Epoch #6. Time: 6.4s. Train loss: 258.817, train mse: 16.08573. Val loss: 771.535, val mse: 27.80109\n",
            "Epoch #7. Time: 6.4s. Train loss: 242.874, train mse: 15.57656. Val loss: 827.907, val mse: 28.09034\n",
            "Epoch #8. Time: 6.5s. Train loss: 223.628, train mse: 14.98769. Val loss: 784.175, val mse: 28.03797\n",
            "Epoch #9. Time: 6.4s. Train loss: 248.882, train mse: 15.76952. Val loss: 885.650, val mse: 29.62700\n",
            "Epoch #10. Time: 6.4s. Train loss: 283.130, train mse: 16.84966. Val loss: 790.749, val mse: 28.40062\n",
            "Epoch #11. Time: 6.4s. Train loss: 241.019, train mse: 15.46733. Val loss: 830.459, val mse: 27.78849\n",
            "Epoch #12. Time: 6.4s. Train loss: 194.735, train mse: 13.95765. Val loss: 908.419, val mse: 29.92091\n",
            "Epoch #13. Time: 6.5s. Train loss: 162.139, train mse: 12.73890. Val loss: 778.846, val mse: 27.80516\n",
            "Epoch #14. Time: 6.5s. Train loss: 147.650, train mse: 12.14964. Val loss: 818.170, val mse: 28.68599\n",
            "Epoch #15. Time: 6.4s. Train loss: 128.139, train mse: 11.34352. Val loss: 807.566, val mse: 28.47948\n",
            "Epoch #16. Time: 6.4s. Train loss: 142.502, train mse: 11.82389. Val loss: 767.876, val mse: 27.66972\n",
            "Training done. Best rmse: 26.678741455078125\n",
            "Start #10 fold\n",
            "Epoch #1. Time: 6.5s. Train loss: 6531.350, train mse: 80.84815. Val loss: 6038.311, val mse: 77.16326\n",
            "Epoch #2. Time: 6.4s. Train loss: 2605.493, train mse: 51.14783. Val loss: 797.219, val mse: 27.92905\n",
            "Epoch #3. Time: 6.4s. Train loss: 590.442, train mse: 24.31453. Val loss: 706.225, val mse: 26.45535\n",
            "Epoch #4. Time: 6.5s. Train loss: 454.465, train mse: 21.34804. Val loss: 660.450, val mse: 25.14922\n",
            "Epoch #5. Time: 6.5s. Train loss: 350.981, train mse: 18.76216. Val loss: 692.621, val mse: 26.20950\n",
            "Epoch #6. Time: 6.4s. Train loss: 306.614, train mse: 17.48820. Val loss: 686.673, val mse: 26.31608\n",
            "Epoch #7. Time: 6.4s. Train loss: 230.205, train mse: 15.17798. Val loss: 692.701, val mse: 26.64002\n",
            "Epoch #8. Time: 6.5s. Train loss: 250.491, train mse: 15.85808. Val loss: 700.514, val mse: 26.77509\n",
            "Epoch #9. Time: 6.4s. Train loss: 264.911, train mse: 16.27974. Val loss: 831.658, val mse: 28.22713\n",
            "Epoch #10. Time: 6.4s. Train loss: 297.265, train mse: 17.21701. Val loss: 716.404, val mse: 26.74233\n",
            "Epoch #11. Time: 6.5s. Train loss: 254.858, train mse: 15.97587. Val loss: 755.108, val mse: 27.09628\n",
            "Epoch #12. Time: 6.4s. Train loss: 215.684, train mse: 14.67276. Val loss: 674.494, val mse: 25.95737\n",
            "Epoch #13. Time: 6.4s. Train loss: 180.158, train mse: 13.30975. Val loss: 692.312, val mse: 26.58205\n",
            "Epoch #14. Time: 6.4s. Train loss: 135.556, train mse: 11.65890. Val loss: 624.514, val mse: 25.10356\n",
            "Epoch #15. Time: 6.4s. Train loss: 155.717, train mse: 12.47610. Val loss: 693.247, val mse: 26.09844\n",
            "Epoch #16. Time: 6.5s. Train loss: 128.564, train mse: 11.34488. Val loss: 677.000, val mse: 26.31199\n",
            "Training done. Best rmse: 25.103557586669922\n",
            "Total rmse: 23.60698699951172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGwgqBIay3Yd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}