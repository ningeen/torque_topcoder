{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WHgLZpLpo3Wj"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iP4BT5QEo7Ot",
    "outputId": "e8dca947-e697-4a0f-d7e6-6a1874854353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/topcoder\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/topcoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4dvb9HGGOzeD"
   },
   "outputs": [],
   "source": [
    "# !pip install pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RIeylP9JRmdM"
   },
   "outputs": [],
   "source": [
    "# !pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pglu-X_4paow"
   },
   "source": [
    "# Новый раздел"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1Jp6My88p8on"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import yaml\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nve-ms3ROqeY"
   },
   "outputs": [],
   "source": [
    "# import pretrainedmodels\n",
    "# from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Dbn0AJYuq_2Q"
   },
   "outputs": [],
   "source": [
    "__all__ = ['mobilenetv3_large', 'mobilenetv3_small']\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class h_sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = nn.ReLU6(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "\n",
    "\n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(channel, _make_divisible(channel // reduction, 8)),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(_make_divisible(channel // reduction, 8), channel),\n",
    "                h_sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "def conv_3x3_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        h_swish()\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        h_swish()\n",
    "    )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se, use_hs):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        self.identity = stride == 1 and inp == oup\n",
    "\n",
    "        if inp == hidden_dim:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "    def __init__(self, cfgs, mode, num_classes=1000, width_mult=1.):\n",
    "        super(MobileNetV3, self).__init__()\n",
    "        # setting of inverted residual blocks\n",
    "        self.cfgs = cfgs\n",
    "        assert mode in ['large', 'small']\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(16 * width_mult, 8)\n",
    "        layers = [conv_3x3_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        block = InvertedResidual\n",
    "        for k, t, c, use_se, use_hs, s in self.cfgs:\n",
    "            output_channel = _make_divisible(c * width_mult, 8)\n",
    "            exp_size = _make_divisible(input_channel * t, 8)\n",
    "            layers.append(block(input_channel, exp_size, output_channel, k, s, use_se, use_hs))\n",
    "            input_channel = output_channel\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        # building last several layers\n",
    "        self.conv = conv_1x1_bn(input_channel, exp_size)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        output_channel = {'large': 1280, 'small': 1024}\n",
    "        output_channel = _make_divisible(output_channel[mode] * width_mult, 8) if width_mult > 1.0 else output_channel[mode]\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(exp_size, output_channel),\n",
    "            h_swish(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(output_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def mobilenetv3_large(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV3-Large model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # k, t, c, SE, HS, s \n",
    "        [3,   1,  16, 0, 0, 1],\n",
    "        [3,   4,  24, 0, 0, 2],\n",
    "        [3,   3,  24, 0, 0, 1],\n",
    "        [5,   3,  40, 1, 0, 2],\n",
    "        [5,   3,  40, 1, 0, 1],\n",
    "        [5,   3,  40, 1, 0, 1],\n",
    "        [3,   6,  80, 0, 1, 2],\n",
    "        [3, 2.5,  80, 0, 1, 1],\n",
    "        [3, 2.3,  80, 0, 1, 1],\n",
    "        [3, 2.3,  80, 0, 1, 1],\n",
    "        [3,   6, 112, 1, 1, 1],\n",
    "        [3,   6, 112, 1, 1, 1],\n",
    "        [5,   6, 160, 1, 1, 2],\n",
    "        [5,   6, 160, 1, 1, 1],\n",
    "        [5,   6, 160, 1, 1, 1]\n",
    "    ]\n",
    "    return MobileNetV3(cfgs, mode='large', **kwargs)\n",
    "\n",
    "\n",
    "def mobilenetv3_small(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV3-Small model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # k, t, c, SE, HS, s \n",
    "        [3,    1,  16, 1, 0, 2],\n",
    "        [3,  4.5,  24, 0, 0, 2],\n",
    "        [3, 3.67,  24, 0, 0, 1],\n",
    "        [5,    4,  40, 1, 1, 2],\n",
    "        [5,    6,  40, 1, 1, 1],\n",
    "        [5,    6,  40, 1, 1, 1],\n",
    "        [5,    3,  48, 1, 1, 1],\n",
    "        [5,    3,  48, 1, 1, 1],\n",
    "        [5,    6,  96, 1, 1, 2],\n",
    "        [5,    6,  96, 1, 1, 1],\n",
    "        [5,    6,  96, 1, 1, 1],\n",
    "    ]\n",
    "\n",
    "    return MobileNetV3(cfgs, mode='small', **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "13rtbK2h8sCE"
   },
   "outputs": [],
   "source": [
    "class CosineAnnealingWarmupRestarts(_LRScheduler):\n",
    "    \"\"\"\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        first_cycle_steps (int): First cycle step size.\n",
    "        cycle_mult(float): Cycle steps magnification. Default: -1.\n",
    "        max_lr(float): First cycle's max learning rate. Default: 0.1.\n",
    "        min_lr(float): Min learning rate. Default: 0.001.\n",
    "        warmup_steps(int): Linear warmup step size. Default: 0.\n",
    "        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 optimizer : torch.optim.Optimizer,\n",
    "                 first_cycle_steps : int,\n",
    "                 cycle_mult : float = 1.,\n",
    "                 max_lr : float = 0.1,\n",
    "                 min_lr : float = 0.001,\n",
    "                 warmup_steps : int = 0,\n",
    "                 gamma : float = 1.,\n",
    "                 last_epoch : int = -1\n",
    "        ):\n",
    "        assert warmup_steps < first_cycle_steps\n",
    "        \n",
    "        self.first_cycle_steps = first_cycle_steps # first cycle step size\n",
    "        self.cycle_mult = cycle_mult # cycle steps magnification\n",
    "        self.base_max_lr = max_lr # first max learning rate\n",
    "        self.max_lr = max_lr # max learning rate in the current cycle\n",
    "        self.min_lr = min_lr # min learning rate\n",
    "        self.warmup_steps = warmup_steps # warmup step size\n",
    "        self.gamma = gamma # decrease rate of max learning rate by cycle\n",
    "        \n",
    "        self.cur_cycle_steps = first_cycle_steps # first cycle step size\n",
    "        self.cycle = 0 # cycle count\n",
    "        self.step_in_cycle = last_epoch # step size of the current cycle\n",
    "        \n",
    "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
    "        \n",
    "        # set learning rate min_lr\n",
    "        self.init_lr()\n",
    "    \n",
    "    def init_lr(self):\n",
    "        self.base_lrs = []\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.min_lr\n",
    "            self.base_lrs.append(self.min_lr)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.step_in_cycle == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.step_in_cycle < self.warmup_steps:\n",
    "            return [(self.max_lr - base_lr)*self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.max_lr - base_lr) \\\n",
    "                    * (1 + math.cos(math.pi * (self.step_in_cycle-self.warmup_steps) \\\n",
    "                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.step_in_cycle = self.step_in_cycle + 1\n",
    "            if self.step_in_cycle >= self.cur_cycle_steps:\n",
    "                self.cycle += 1\n",
    "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\n",
    "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
    "        else:\n",
    "            if epoch >= self.first_cycle_steps:\n",
    "                if self.cycle_mult == 1.:\n",
    "                    self.step_in_cycle = epoch % self.first_cycle_steps\n",
    "                    self.cycle = epoch // self.first_cycle_steps\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n",
    "                    self.cycle = n\n",
    "                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n",
    "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\n",
    "            else:\n",
    "                self.cur_cycle_steps = self.first_cycle_steps\n",
    "                self.step_in_cycle = epoch\n",
    "                \n",
    "        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "V4ElywIvqA6K"
   },
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"proj_config.yaml\"\n",
    "with open(CONFIG_PATH, 'r') as stream:\n",
    "    CONFIG = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tP7AkAj8MAsZ"
   },
   "outputs": [],
   "source": [
    "def spec_augment(spec: np.ndarray, num_mask=2, freq_masking_max_percentage=0.05, time_masking_max_percentage=0.1):\n",
    "    spec = spec.copy()\n",
    "    for i in range(num_mask):\n",
    "        num_freqs, num_frames = spec.shape\n",
    "        freq_percentage = random.uniform(0.0, freq_masking_max_percentage)\n",
    "        time_percentage = random.uniform(0.0, time_masking_max_percentage)\n",
    "        \n",
    "        num_freqs_to_mask = int(freq_percentage * num_freqs)\n",
    "        num_frames_to_mask = int(time_percentage * num_frames)\n",
    "        \n",
    "        t0 = int(np.random.uniform(low=0.0, high=num_frames - num_frames_to_mask))\n",
    "        f0 = int(np.random.uniform(low=0.0, high=num_freqs - num_freqs_to_mask))\n",
    "        \n",
    "        spec[:, t0:t0 + num_frames_to_mask] = 0      \n",
    "        spec[f0:f0 + num_freqs_to_mask, :] = 0 \n",
    "        \n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "G43KN85lp4D0"
   },
   "outputs": [],
   "source": [
    "def uniform_len(mel, input_len):\n",
    "    mel_len = mel.shape[-1]\n",
    "    if mel_len > input_len:\n",
    "        diff = mel_len - input_len\n",
    "        start = np.random.randint(diff)\n",
    "        end = start + input_len\n",
    "        mel = mel[:, start: end]\n",
    "    elif mel_len < input_len:\n",
    "        diff = input_len - mel_len\n",
    "        offset = np.random.randint(diff)\n",
    "        offset_right = diff - offset\n",
    "        mel = np.pad(\n",
    "            mel,\n",
    "            ((0, 0), (offset, offset_right)),\n",
    "            \"symmetric\",  # constant\n",
    "        )\n",
    "    return mel\n",
    "\n",
    "\n",
    "class TorqueDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data, mel_logs, labels=None, transform=None):\n",
    "        \"\"\"Init Dataset\"\"\"\n",
    "        self.mel_logs = mel_logs\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.input_len = CONFIG['mel']['mel_len']\n",
    "        self.mode = 'test' if self.labels is None else 'train'\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Length\"\"\"\n",
    "        return len(self.mel_logs)\n",
    "\n",
    "    def add_frequency_encoding(self, x):\n",
    "        d, h, w = x.shape\n",
    "        vertical = np.linspace(-1, 1, h).reshape(1, -1, 1)\n",
    "        vertical = np.repeat(vertical, w, axis=2)\n",
    "        x = np.concatenate([x, vertical], axis=0)\n",
    "        return x.astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generates one sample of data\"\"\"\n",
    "        table_data = self.data[index]\n",
    "\n",
    "        label = None\n",
    "        if self.mode == 'train':\n",
    "            label = self.labels[[index]]\n",
    "\n",
    "        mel_data = uniform_len(self.mel_logs[index], self.input_len)\n",
    "        if self.transform and self.mode == 'train':\n",
    "            mel_data = self.transform(mel_data)\n",
    "\n",
    "        mel_data = np.expand_dims(mel_data, axis=0)\n",
    "        mel_data = self.add_frequency_encoding(mel_data)\n",
    "        return mel_data, table_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XdOpMc7Rp-AZ"
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark= True\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    \"\"\"Fix random seeds\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2vJdztTNqiv0"
   },
   "outputs": [],
   "source": [
    "def get_mobilenet_model(out_features, pretrained_mn3_path=\"\", pretrained_path=\"\", channels=2):\n",
    "    \"\"\"Load MobilenetV3 model with specified in and out channels\"\"\"\n",
    "    # model = mobilenetv3_small().to(DEVICE)\n",
    "    model = mobilenetv3_large() #.to(DEVICE)\n",
    "    if pretrained_mn3_path and not pretrained_path:\n",
    "        model.load_state_dict(torch.load(pretrained_mn3_path))\n",
    "\n",
    "    if channels == 1:\n",
    "        model.features[0][0].weight.data = torch.sum(\n",
    "            model.features[0][0].weight.data, dim=1, keepdim=True\n",
    "        )\n",
    "    elif channels == 2:\n",
    "        model.features[0][0].weight.data = model.features[0][0].weight.data[:, :2]\n",
    "    model.features[0][0].in_channels = channels\n",
    "\n",
    "    if pretrained_path:\n",
    "        model.load_state_dict(torch.load(pretrained_path))\n",
    "    return model\n",
    "\n",
    "def pretrained_model(model_name):\n",
    "    model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n",
    "    model.layer0[0].weight.data = torch.sum(\n",
    "        model.layer0[0].weight.data, dim=1, keepdim=True\n",
    "    )\n",
    "    model.layer0[0].in_channels = 1\n",
    "    model.avg_pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "    return model\n",
    "\n",
    "def efficientnet_model(model_name):\n",
    "    model = EfficientNet.from_pretrained(model_name)\n",
    "    model._conv_stem.weight.data = torch.sum(\n",
    "        model._conv_stem.weight.data, dim=1, keepdim=True\n",
    "    )\n",
    "    model._conv_stem.in_channels = 1\n",
    "    # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "    return model\n",
    "\n",
    "class TorqueModel(nn.Module):\n",
    "    def __init__(self, out_features_conv, out_features_dence, mid_features, pretrained_mn3_path=\"\", pretrained_path=\"\"):\n",
    "        super(TorqueModel, self).__init__()\n",
    "        self.mnet = get_mobilenet_model(out_features_conv, pretrained_mn3_path, pretrained_path)\n",
    "        # self.mnet = pretrained_model('se_resnext50_32x4d')\n",
    "        # self.mnet = efficientnet_model('efficientnet-b0')\n",
    "        self.fc1 = nn.Linear(out_features_conv + out_features_dence, mid_features)\n",
    "        self.fc2 = nn.Linear(mid_features, mid_features)\n",
    "        self.fc3 = nn.Linear(mid_features, 1)\n",
    "\n",
    "    def forward(self, image, data):\n",
    "        x1 = self.mnet(image)\n",
    "        x2 = data\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def process_epoch(model, criterion, optimizer, loader):\n",
    "    \"\"\"Calc one epoch\"\"\"\n",
    "    losses = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.set_grad_enabled(model.training):\n",
    "        for local_batch, local_data, local_labels in loader:\n",
    "            local_batch, local_data, local_labels = \\\n",
    "                local_batch.to(DEVICE), local_data.to(DEVICE), local_labels.to(DEVICE)\n",
    "\n",
    "            # optimizer.zero_grad()\n",
    "            for param in model.parameters():\n",
    "                param.grad = None\n",
    "            outputs = model(local_batch, local_data)\n",
    "\n",
    "            loss = criterion(outputs, local_labels)\n",
    "            if model.training:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            losses.append(loss)\n",
    "            y_true.append(local_labels.detach().cpu().numpy())\n",
    "            y_pred.append(outputs.data.detach().cpu().numpy())\n",
    "    loss_train = np.array(losses).astype(np.float32).mean()\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    rmse_train = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    return loss_train, rmse_train, y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pT95qJpQqlu9"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, train_loader, test_loader, n_fold, n_freeze=10):\n",
    "    \"\"\"Training loop\"\"\"\n",
    "    logs = {'loss_train': [], 'loss_val': [], 'mse_train': [], 'mse_val': []}\n",
    "    best_true = None\n",
    "    best_pred = None\n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # More accurate work with new layers\n",
    "        scheduler.step()\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        loss_train, mse_train, _, _ = \\\n",
    "            process_epoch(model, criterion, optimizer, train_loader)\n",
    "        logs['loss_train'].append(loss_train)\n",
    "        logs['mse_train'].append(mse_train)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        loss_val, mse_val, y_true, y_pred = \\\n",
    "            process_epoch(model, criterion, optimizer, test_loader)\n",
    "        logs['loss_val'].append(loss_val)\n",
    "        logs['mse_val'].append(mse_val)\n",
    "        print(\n",
    "            f\"Epoch #{epoch + 1}. \"\n",
    "            f\"Time: {(time.time() - start_time):.1f}s. \"\n",
    "            f\"Train loss: {loss_train:.3f}, train mse: {mse_train:.5f}. \"\n",
    "            f\"Val loss: {loss_val:.3f}, val mse: {mse_val:.5f}\"\n",
    "        )\n",
    "        if mse_val <= np.min(logs['mse_val']):\n",
    "            if CONFIG['save_model']:\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(\n",
    "                        CONFIG['model_dir'],\n",
    "                        f\"work_{CONFIG['experiment_name']}_fold{n_fold}.pt\"\n",
    "                    )\n",
    "                )\n",
    "            best_true = y_true\n",
    "            best_pred = y_pred\n",
    "    return best_true, best_pred\n",
    "\n",
    "\n",
    "def run_training():\n",
    "    start_time = time.time()\n",
    "\n",
    "    with open(CONFIG['data_path'], 'rb') as f:\n",
    "        (data, mel_logs, target) = pickle.load(f)\n",
    "\n",
    "    folds = KFold(\n",
    "        n_splits=CONFIG['n_folds'],\n",
    "        shuffle=True,\n",
    "        random_state=CONFIG['fold_seed']\n",
    "    )\n",
    "    splits = list(folds.split(mel_logs))\n",
    "\n",
    "    total_rmse = list()\n",
    "\n",
    "    for n_fold, (train_idx, val_idx) in enumerate(splits):\n",
    "        print(f\"Start #{n_fold + 1} fold\")\n",
    "        train_dataset = TorqueDataset(\n",
    "            data[train_idx],\n",
    "            [mel_logs[i] for i in train_idx],\n",
    "            target[train_idx],\n",
    "            transform=spec_augment\n",
    "        )\n",
    "        val_dataset = TorqueDataset(\n",
    "            data[val_idx],\n",
    "            [mel_logs[i] for i in val_idx],\n",
    "            target[val_idx]\n",
    "        )\n",
    "        train_loader = DataLoader(train_dataset, **CONFIG['loader_params'])\n",
    "        val_loader = DataLoader(val_dataset, **CONFIG['loader_params'])\n",
    "\n",
    "        model = TorqueModel(\n",
    "            CONFIG['model_params']['out_features_conv'],\n",
    "            CONFIG['model_params']['out_features_dence'],\n",
    "            CONFIG['model_params']['mid_features'],\n",
    "            CONFIG['pretrained_path']\n",
    "        )\n",
    "        model = model.to(DEVICE)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), CONFIG['lr'])\n",
    "\n",
    "        # CONFIG['scheduler_params']['max_lr'] *= CONFIG['lr']\n",
    "        # CONFIG['scheduler_params']['min_lr'] *= CONFIG['lr']\n",
    "        scheduler = CosineAnnealingWarmupRestarts(optimizer, **CONFIG['scheduler_params'])\n",
    "\n",
    "        best_true, best_pred = \\\n",
    "            train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, n_fold)\n",
    "\n",
    "        rmse = mean_squared_error(best_true, best_pred, squared=False)\n",
    "        print(f\"Training done. Best rmse: {rmse}\")\n",
    "        total_rmse.append(rmse)\n",
    "    print(f\"Total time: {(time.time() - start_time) / 60}m\")\n",
    "    print(f\"Total rmse: {np.mean(total_rmse)} +- {np.std(total_rmse)}\")\n",
    "    print(total_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "m1aNynqjtHPj"
   },
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QiLbhqtFtEMt"
   },
   "outputs": [],
   "source": [
    "CONFIG['loader_params'] = {'batch_size': 16, 'shuffle': True, \n",
    "                           'num_workers': 4, 'pin_memory':True}\n",
    "CONFIG['lr'] = 0.0001   # 0.0001\n",
    "\n",
    "# CONFIG['slow_epochs']\n",
    "CONFIG['num_epochs'] = 40\n",
    "\n",
    "CONFIG['pretrained_path'] = './pretrained/mobilenetv3-large-1cd25616.pth'\n",
    "\n",
    "CONFIG['scheduler_params'] = {'first_cycle_steps':20,  # 20\n",
    "                            'cycle_mult':1.0,\n",
    "                            'max_lr':CONFIG['lr'] * 6,  # 6\n",
    "                            'min_lr':CONFIG['lr'] / 8,  # 8\n",
    "                            'warmup_steps':5,  # 5\n",
    "                            'gamma':0.9}  # 0.9\n",
    "\n",
    "CONFIG['experiment_name'] = 'origin_2ch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_jM8j3o0mMK7",
    "outputId": "fb8fa508-118d-473f-8515-3f3dab7f9f35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start #1 fold\n",
      "Epoch #1. Time: 20.9s. Train loss: 2929.647, train mse: 54.22674. Val loss: 1090.466, val mse: 32.63964\n",
      "Epoch #2. Time: 14.0s. Train loss: 971.740, train mse: 31.11654. Val loss: 887.059, val mse: 29.88103\n",
      "Epoch #3. Time: 14.0s. Train loss: 912.981, train mse: 30.16027. Val loss: 1083.188, val mse: 33.16329\n",
      "Epoch #4. Time: 13.9s. Train loss: 965.752, train mse: 31.07615. Val loss: 1008.643, val mse: 32.28299\n",
      "Epoch #5. Time: 14.0s. Train loss: 904.582, train mse: 30.07695. Val loss: 873.146, val mse: 29.32389\n",
      "Epoch #6. Time: 14.0s. Train loss: 841.574, train mse: 29.04198. Val loss: 934.299, val mse: 30.15234\n",
      "Epoch #7. Time: 14.0s. Train loss: 793.759, train mse: 28.08131. Val loss: 733.768, val mse: 27.25793\n",
      "Epoch #8. Time: 14.0s. Train loss: 782.973, train mse: 28.03034. Val loss: 804.326, val mse: 28.62308\n",
      "Epoch #9. Time: 13.9s. Train loss: 746.624, train mse: 27.35686. Val loss: 837.138, val mse: 29.11954\n",
      "Epoch #10. Time: 14.0s. Train loss: 632.159, train mse: 25.17341. Val loss: 686.399, val mse: 26.50324\n",
      "Epoch #11. Time: 14.0s. Train loss: 595.562, train mse: 24.40961. Val loss: 699.730, val mse: 26.65395\n",
      "Epoch #12. Time: 14.0s. Train loss: 569.817, train mse: 23.85991. Val loss: 749.307, val mse: 27.61689\n",
      "Epoch #13. Time: 14.0s. Train loss: 494.774, train mse: 22.17361. Val loss: 712.553, val mse: 27.02029\n",
      "Epoch #14. Time: 13.9s. Train loss: 439.024, train mse: 20.93177. Val loss: 737.035, val mse: 27.17697\n",
      "Epoch #15. Time: 14.0s. Train loss: 415.583, train mse: 20.23920. Val loss: 800.302, val mse: 28.55849\n",
      "Epoch #16. Time: 13.9s. Train loss: 397.219, train mse: 19.94641. Val loss: 855.441, val mse: 29.50568\n",
      "Epoch #17. Time: 14.0s. Train loss: 362.593, train mse: 19.05276. Val loss: 723.833, val mse: 26.48260\n",
      "Epoch #18. Time: 14.0s. Train loss: 312.810, train mse: 17.70048. Val loss: 656.830, val mse: 25.60400\n",
      "Epoch #19. Time: 14.1s. Train loss: 313.636, train mse: 17.72860. Val loss: 644.542, val mse: 25.56877\n",
      "Epoch #20. Time: 14.0s. Train loss: 309.366, train mse: 17.58252. Val loss: 675.660, val mse: 25.81398\n",
      "Epoch #21. Time: 13.9s. Train loss: 324.092, train mse: 17.98822. Val loss: 788.404, val mse: 27.01038\n",
      "Epoch #22. Time: 13.9s. Train loss: 350.672, train mse: 18.75208. Val loss: 651.527, val mse: 25.76733\n",
      "Epoch #23. Time: 14.0s. Train loss: 469.476, train mse: 21.69791. Val loss: 789.604, val mse: 28.41317\n",
      "Epoch #24. Time: 13.9s. Train loss: 468.823, train mse: 21.64888. Val loss: 716.934, val mse: 26.88557\n",
      "Epoch #25. Time: 14.0s. Train loss: 523.088, train mse: 22.85798. Val loss: 746.784, val mse: 27.22514\n",
      "Epoch #26. Time: 13.9s. Train loss: 474.068, train mse: 21.75104. Val loss: 801.072, val mse: 28.64733\n",
      "Epoch #27. Time: 14.0s. Train loss: 489.008, train mse: 22.09121. Val loss: 1007.434, val mse: 30.99348\n",
      "Epoch #28. Time: 14.0s. Train loss: 459.906, train mse: 21.46165. Val loss: 640.430, val mse: 25.66183\n",
      "Epoch #29. Time: 13.9s. Train loss: 365.689, train mse: 19.13113. Val loss: 777.908, val mse: 27.77245\n",
      "Epoch #30. Time: 14.0s. Train loss: 309.554, train mse: 17.59478. Val loss: 696.428, val mse: 26.28319\n",
      "Epoch #31. Time: 14.0s. Train loss: 310.656, train mse: 17.64234. Val loss: 718.441, val mse: 26.78930\n",
      "Epoch #32. Time: 14.0s. Train loss: 244.109, train mse: 15.61883. Val loss: 653.459, val mse: 25.62013\n",
      "Epoch #33. Time: 13.9s. Train loss: 232.420, train mse: 15.28161. Val loss: 669.016, val mse: 25.48335\n",
      "Epoch #34. Time: 14.0s. Train loss: 228.763, train mse: 15.13358. Val loss: 601.172, val mse: 24.79545\n",
      "Epoch #35. Time: 14.0s. Train loss: 192.592, train mse: 13.86271. Val loss: 591.634, val mse: 24.49242\n",
      "Epoch #36. Time: 14.0s. Train loss: 180.339, train mse: 13.40232. Val loss: 590.562, val mse: 24.61207\n",
      "Epoch #37. Time: 14.0s. Train loss: 181.775, train mse: 13.50583. Val loss: 648.094, val mse: 25.26197\n",
      "Epoch #38. Time: 14.0s. Train loss: 147.088, train mse: 12.15171. Val loss: 582.060, val mse: 24.33509\n",
      "Epoch #39. Time: 14.0s. Train loss: 157.001, train mse: 12.53366. Val loss: 605.620, val mse: 24.93430\n",
      "Epoch #40. Time: 14.0s. Train loss: 154.979, train mse: 12.39086. Val loss: 603.611, val mse: 24.85946\n",
      "Training done. Best rmse: 24.335094451904297\n",
      "Start #2 fold\n",
      "Epoch #1. Time: 14.0s. Train loss: 2894.842, train mse: 53.86208. Val loss: 1458.390, val mse: 37.50805\n",
      "Epoch #2. Time: 14.2s. Train loss: 971.666, train mse: 31.22498. Val loss: 748.908, val mse: 27.03250\n",
      "Epoch #3. Time: 14.0s. Train loss: 928.221, train mse: 30.49999. Val loss: 747.828, val mse: 27.14049\n",
      "Epoch #4. Time: 14.1s. Train loss: 955.913, train mse: 30.96177. Val loss: 838.841, val mse: 28.41197\n",
      "Epoch #5. Time: 14.0s. Train loss: 962.734, train mse: 31.03188. Val loss: 687.281, val mse: 26.07941\n",
      "Epoch #6. Time: 14.0s. Train loss: 824.269, train mse: 28.74849. Val loss: 656.517, val mse: 25.36360\n",
      "Epoch #7. Time: 14.0s. Train loss: 780.107, train mse: 27.99548. Val loss: 986.206, val mse: 31.55470\n",
      "Epoch #8. Time: 13.9s. Train loss: 749.018, train mse: 27.38405. Val loss: 706.342, val mse: 26.55847\n",
      "Epoch #9. Time: 13.9s. Train loss: 703.782, train mse: 26.44867. Val loss: 613.619, val mse: 25.04777\n",
      "Epoch #10. Time: 14.0s. Train loss: 637.774, train mse: 25.24917. Val loss: 580.042, val mse: 24.34879\n",
      "Epoch #11. Time: 14.0s. Train loss: 563.941, train mse: 23.76215. Val loss: 544.099, val mse: 23.51049\n",
      "Epoch #12. Time: 14.0s. Train loss: 508.746, train mse: 22.51846. Val loss: 616.469, val mse: 24.81713\n",
      "Epoch #13. Time: 14.0s. Train loss: 513.842, train mse: 22.69846. Val loss: 501.292, val mse: 22.47664\n",
      "Epoch #14. Time: 14.0s. Train loss: 419.724, train mse: 20.46544. Val loss: 580.023, val mse: 23.94989\n",
      "Epoch #15. Time: 14.0s. Train loss: 374.320, train mse: 19.39188. Val loss: 443.674, val mse: 21.28910\n",
      "Epoch #16. Time: 14.0s. Train loss: 339.546, train mse: 18.44125. Val loss: 506.277, val mse: 22.45331\n",
      "Epoch #17. Time: 14.0s. Train loss: 327.427, train mse: 17.98608. Val loss: 437.205, val mse: 20.39607\n",
      "Epoch #18. Time: 14.1s. Train loss: 301.001, train mse: 17.37763. Val loss: 454.766, val mse: 21.56388\n",
      "Epoch #19. Time: 14.0s. Train loss: 281.261, train mse: 16.56672. Val loss: 486.657, val mse: 22.36078\n",
      "Epoch #20. Time: 14.0s. Train loss: 259.091, train mse: 16.09820. Val loss: 504.557, val mse: 22.30280\n",
      "Epoch #21. Time: 14.0s. Train loss: 322.368, train mse: 17.97101. Val loss: 448.080, val mse: 21.39223\n",
      "Epoch #22. Time: 14.0s. Train loss: 296.064, train mse: 17.16789. Val loss: 535.700, val mse: 23.22853\n",
      "Epoch #23. Time: 14.0s. Train loss: 376.027, train mse: 19.37635. Val loss: 559.471, val mse: 23.75104\n",
      "Epoch #24. Time: 14.0s. Train loss: 469.882, train mse: 21.63506. Val loss: 795.204, val mse: 28.71237\n",
      "Epoch #25. Time: 14.0s. Train loss: 521.912, train mse: 22.80830. Val loss: 599.402, val mse: 24.57256\n",
      "Epoch #26. Time: 14.0s. Train loss: 498.050, train mse: 22.30472. Val loss: 546.864, val mse: 23.46632\n",
      "Epoch #27. Time: 14.0s. Train loss: 546.203, train mse: 23.39810. Val loss: 862.312, val mse: 29.44167\n",
      "Epoch #28. Time: 14.0s. Train loss: 360.298, train mse: 18.91618. Val loss: 638.819, val mse: 25.13417\n",
      "Epoch #29. Time: 14.0s. Train loss: 359.896, train mse: 18.95156. Val loss: 524.078, val mse: 22.89822\n",
      "Epoch #30. Time: 14.0s. Train loss: 370.284, train mse: 19.26914. Val loss: 459.274, val mse: 21.32529\n",
      "Epoch #31. Time: 14.0s. Train loss: 295.122, train mse: 17.20012. Val loss: 436.076, val mse: 21.20178\n",
      "Epoch #32. Time: 14.0s. Train loss: 225.157, train mse: 14.99600. Val loss: 409.646, val mse: 20.38870\n",
      "Epoch #33. Time: 14.0s. Train loss: 222.932, train mse: 14.94580. Val loss: 502.933, val mse: 22.31212\n",
      "Epoch #34. Time: 14.0s. Train loss: 190.848, train mse: 13.80595. Val loss: 433.484, val mse: 21.04605\n",
      "Epoch #35. Time: 14.0s. Train loss: 165.515, train mse: 12.86876. Val loss: 470.726, val mse: 21.39153\n",
      "Epoch #36. Time: 14.0s. Train loss: 172.376, train mse: 13.11371. Val loss: 426.061, val mse: 20.51583\n",
      "Epoch #37. Time: 14.0s. Train loss: 146.563, train mse: 12.12017. Val loss: 418.766, val mse: 20.44040\n",
      "Epoch #38. Time: 14.0s. Train loss: 139.263, train mse: 11.82073. Val loss: 390.758, val mse: 19.89864\n",
      "Epoch #39. Time: 14.0s. Train loss: 142.051, train mse: 11.91277. Val loss: 433.795, val mse: 20.45687\n",
      "Epoch #40. Time: 14.0s. Train loss: 132.774, train mse: 11.50573. Val loss: 386.945, val mse: 19.81775\n",
      "Training done. Best rmse: 19.8177547454834\n",
      "Start #3 fold\n",
      "Epoch #1. Time: 14.0s. Train loss: 2909.983, train mse: 54.06784. Val loss: 954.227, val mse: 30.68765\n",
      "Epoch #2. Time: 14.0s. Train loss: 967.357, train mse: 31.11779. Val loss: 961.599, val mse: 31.12389\n",
      "Epoch #3. Time: 14.1s. Train loss: 921.844, train mse: 30.39555. Val loss: 1041.418, val mse: 31.79880\n",
      "Epoch #4. Time: 13.9s. Train loss: 979.836, train mse: 31.28580. Val loss: 589.260, val mse: 24.35039\n",
      "Epoch #5. Time: 14.0s. Train loss: 962.854, train mse: 31.02033. Val loss: 1039.604, val mse: 32.24485\n",
      "Epoch #6. Time: 14.0s. Train loss: 909.748, train mse: 30.15136. Val loss: 759.205, val mse: 27.79614\n",
      "Epoch #7. Time: 14.0s. Train loss: 857.548, train mse: 29.25698. Val loss: 669.191, val mse: 26.27967\n",
      "Epoch #8. Time: 14.0s. Train loss: 755.939, train mse: 27.48113. Val loss: 756.220, val mse: 27.66566\n",
      "Epoch #9. Time: 14.0s. Train loss: 750.899, train mse: 27.35065. Val loss: 592.740, val mse: 24.65743\n",
      "Epoch #10. Time: 14.0s. Train loss: 630.320, train mse: 25.11850. Val loss: 510.418, val mse: 22.86588\n",
      "Epoch #11. Time: 14.0s. Train loss: 575.511, train mse: 23.97559. Val loss: 527.410, val mse: 22.92697\n",
      "Epoch #12. Time: 14.0s. Train loss: 526.105, train mse: 22.96266. Val loss: 507.213, val mse: 22.69553\n",
      "Epoch #13. Time: 14.0s. Train loss: 509.269, train mse: 22.53036. Val loss: 467.155, val mse: 21.51765\n",
      "Epoch #14. Time: 14.0s. Train loss: 435.921, train mse: 20.84550. Val loss: 429.548, val mse: 21.01323\n",
      "Epoch #15. Time: 14.0s. Train loss: 413.606, train mse: 20.37329. Val loss: 474.613, val mse: 21.77530\n",
      "Epoch #16. Time: 14.0s. Train loss: 363.748, train mse: 18.97617. Val loss: 463.106, val mse: 21.17548\n",
      "Epoch #17. Time: 14.0s. Train loss: 365.898, train mse: 19.09199. Val loss: 431.615, val mse: 20.84371\n",
      "Epoch #18. Time: 14.0s. Train loss: 368.547, train mse: 19.22071. Val loss: 457.735, val mse: 21.56042\n",
      "Epoch #19. Time: 14.0s. Train loss: 334.524, train mse: 18.27035. Val loss: 443.825, val mse: 21.28250\n",
      "Epoch #20. Time: 14.0s. Train loss: 301.908, train mse: 17.39986. Val loss: 439.105, val mse: 20.91183\n",
      "Epoch #21. Time: 14.0s. Train loss: 340.533, train mse: 18.48377. Val loss: 428.697, val mse: 21.01071\n",
      "Epoch #22. Time: 13.9s. Train loss: 362.946, train mse: 19.02643. Val loss: 449.761, val mse: 20.91579\n",
      "Epoch #23. Time: 14.0s. Train loss: 420.546, train mse: 20.54634. Val loss: 474.551, val mse: 21.76297\n",
      "Epoch #24. Time: 14.0s. Train loss: 417.485, train mse: 20.35868. Val loss: 469.451, val mse: 21.82483\n",
      "Epoch #25. Time: 14.0s. Train loss: 467.360, train mse: 21.58022. Val loss: 599.247, val mse: 24.54494\n",
      "Epoch #26. Time: 14.0s. Train loss: 507.847, train mse: 22.44495. Val loss: 460.855, val mse: 21.40386\n",
      "Epoch #27. Time: 14.0s. Train loss: 415.831, train mse: 20.40990. Val loss: 423.501, val mse: 20.76744\n",
      "Epoch #28. Time: 14.0s. Train loss: 425.206, train mse: 20.57229. Val loss: 525.923, val mse: 23.21856\n",
      "Epoch #29. Time: 14.0s. Train loss: 342.378, train mse: 18.53458. Val loss: 399.596, val mse: 19.93440\n",
      "Epoch #30. Time: 14.1s. Train loss: 293.771, train mse: 17.15707. Val loss: 457.650, val mse: 21.62273\n",
      "Epoch #31. Time: 14.0s. Train loss: 321.078, train mse: 17.91042. Val loss: 509.851, val mse: 22.71009\n",
      "Epoch #32. Time: 14.0s. Train loss: 254.040, train mse: 15.92102. Val loss: 547.111, val mse: 23.39233\n",
      "Epoch #33. Time: 14.0s. Train loss: 229.529, train mse: 15.12283. Val loss: 469.263, val mse: 21.83630\n",
      "Epoch #34. Time: 14.1s. Train loss: 221.504, train mse: 14.89150. Val loss: 370.397, val mse: 18.99011\n",
      "Epoch #35. Time: 14.1s. Train loss: 189.477, train mse: 13.75686. Val loss: 539.583, val mse: 23.35315\n",
      "Epoch #36. Time: 14.0s. Train loss: 167.721, train mse: 12.90655. Val loss: 429.597, val mse: 20.72597\n",
      "Epoch #37. Time: 13.9s. Train loss: 163.820, train mse: 12.81432. Val loss: 394.976, val mse: 19.90872\n",
      "Epoch #38. Time: 14.0s. Train loss: 140.968, train mse: 11.89613. Val loss: 401.835, val mse: 20.25308\n",
      "Epoch #39. Time: 14.0s. Train loss: 140.243, train mse: 11.83518. Val loss: 401.560, val mse: 20.20353\n",
      "Epoch #40. Time: 14.0s. Train loss: 147.582, train mse: 12.12599. Val loss: 408.151, val mse: 20.23751\n",
      "Training done. Best rmse: 18.990114212036133\n",
      "Start #4 fold\n",
      "Epoch #1. Time: 14.0s. Train loss: 2941.585, train mse: 54.37533. Val loss: 1127.258, val mse: 32.60964\n",
      "Epoch #2. Time: 14.0s. Train loss: 963.847, train mse: 31.09114. Val loss: 913.537, val mse: 29.75354\n",
      "Epoch #3. Time: 14.0s. Train loss: 907.981, train mse: 30.14959. Val loss: 1352.713, val mse: 36.60865\n",
      "Epoch #4. Time: 14.0s. Train loss: 868.521, train mse: 29.47136. Val loss: 774.803, val mse: 27.65222\n",
      "Epoch #5. Time: 14.0s. Train loss: 882.049, train mse: 29.61156. Val loss: 1402.905, val mse: 36.70545\n",
      "Epoch #6. Time: 14.0s. Train loss: 791.168, train mse: 28.09037. Val loss: 653.052, val mse: 25.56086\n",
      "Epoch #7. Time: 14.0s. Train loss: 707.120, train mse: 26.54454. Val loss: 733.418, val mse: 26.85515\n",
      "Epoch #8. Time: 14.0s. Train loss: 721.605, train mse: 26.90664. Val loss: 656.456, val mse: 25.93966\n",
      "Epoch #9. Time: 14.0s. Train loss: 645.522, train mse: 25.47592. Val loss: 607.322, val mse: 24.66137\n",
      "Epoch #10. Time: 14.0s. Train loss: 574.218, train mse: 23.99624. Val loss: 629.957, val mse: 25.08654\n",
      "Epoch #11. Time: 14.0s. Train loss: 574.225, train mse: 24.00346. Val loss: 751.163, val mse: 26.73502\n",
      "Epoch #12. Time: 14.0s. Train loss: 516.001, train mse: 22.72329. Val loss: 691.763, val mse: 26.52325\n",
      "Epoch #13. Time: 14.0s. Train loss: 477.741, train mse: 21.84091. Val loss: 508.872, val mse: 22.47554\n",
      "Epoch #14. Time: 14.0s. Train loss: 437.317, train mse: 20.90858. Val loss: 583.684, val mse: 24.27041\n",
      "Epoch #15. Time: 14.0s. Train loss: 369.405, train mse: 19.24934. Val loss: 615.054, val mse: 25.11680\n",
      "Epoch #16. Time: 14.0s. Train loss: 349.991, train mse: 18.67839. Val loss: 589.704, val mse: 23.58459\n",
      "Epoch #17. Time: 14.0s. Train loss: 315.755, train mse: 17.77721. Val loss: 588.139, val mse: 23.57258\n",
      "Epoch #18. Time: 14.0s. Train loss: 292.736, train mse: 17.11345. Val loss: 585.574, val mse: 24.43456\n",
      "Epoch #19. Time: 13.9s. Train loss: 302.772, train mse: 17.43936. Val loss: 571.887, val mse: 24.04987\n",
      "Epoch #20. Time: 14.0s. Train loss: 288.788, train mse: 16.98115. Val loss: 570.976, val mse: 24.16137\n",
      "Epoch #21. Time: 14.0s. Train loss: 313.083, train mse: 17.68473. Val loss: 555.509, val mse: 23.68180\n",
      "Epoch #22. Time: 14.0s. Train loss: 313.219, train mse: 17.62929. Val loss: 551.488, val mse: 23.58589\n",
      "Epoch #23. Time: 14.0s. Train loss: 415.675, train mse: 20.33291. Val loss: 879.017, val mse: 29.51604\n",
      "Epoch #24. Time: 14.0s. Train loss: 465.153, train mse: 21.58805. Val loss: 581.974, val mse: 24.32184\n",
      "Epoch #25. Time: 14.0s. Train loss: 491.915, train mse: 22.17337. Val loss: 489.525, val mse: 22.55724\n",
      "Epoch #26. Time: 14.0s. Train loss: 429.281, train mse: 20.57529. Val loss: 578.465, val mse: 23.96512\n",
      "Epoch #27. Time: 14.0s. Train loss: 455.230, train mse: 21.34661. Val loss: 849.278, val mse: 29.59519\n",
      "Epoch #28. Time: 14.0s. Train loss: 375.365, train mse: 19.42118. Val loss: 573.685, val mse: 23.76964\n",
      "Epoch #29. Time: 14.0s. Train loss: 320.382, train mse: 17.85724. Val loss: 625.518, val mse: 24.83796\n",
      "Epoch #30. Time: 14.0s. Train loss: 389.122, train mse: 19.75090. Val loss: 643.902, val mse: 25.63495\n",
      "Epoch #31. Time: 14.0s. Train loss: 295.678, train mse: 17.17439. Val loss: 661.070, val mse: 25.45602\n",
      "Epoch #32. Time: 14.0s. Train loss: 267.116, train mse: 16.27721. Val loss: 485.198, val mse: 21.89608\n",
      "Epoch #33. Time: 14.0s. Train loss: 254.037, train mse: 15.95084. Val loss: 585.171, val mse: 23.95928\n",
      "Epoch #34. Time: 14.0s. Train loss: 208.215, train mse: 14.40232. Val loss: 542.665, val mse: 23.52679\n",
      "Epoch #35. Time: 14.0s. Train loss: 198.650, train mse: 14.12854. Val loss: 523.703, val mse: 22.85425\n",
      "Epoch #36. Time: 14.0s. Train loss: 183.353, train mse: 13.53212. Val loss: 514.094, val mse: 22.26460\n",
      "Epoch #37. Time: 14.0s. Train loss: 166.152, train mse: 12.90284. Val loss: 490.820, val mse: 22.52153\n",
      "Epoch #38. Time: 13.9s. Train loss: 159.277, train mse: 12.56093. Val loss: 518.899, val mse: 23.05871\n",
      "Epoch #39. Time: 14.0s. Train loss: 150.536, train mse: 12.28117. Val loss: 504.018, val mse: 22.79021\n",
      "Epoch #40. Time: 14.0s. Train loss: 143.248, train mse: 11.98427. Val loss: 492.116, val mse: 21.82631\n",
      "Training done. Best rmse: 21.82630729675293\n",
      "Start #5 fold\n",
      "Epoch #1. Time: 14.0s. Train loss: 2855.123, train mse: 53.50973. Val loss: 1204.180, val mse: 34.36071\n",
      "Epoch #2. Time: 14.0s. Train loss: 900.896, train mse: 30.01978. Val loss: 808.584, val mse: 28.66284\n",
      "Epoch #3. Time: 14.0s. Train loss: 886.113, train mse: 29.76312. Val loss: 809.223, val mse: 28.75433\n",
      "Epoch #4. Time: 14.0s. Train loss: 927.893, train mse: 30.52957. Val loss: 903.657, val mse: 30.33059\n",
      "Epoch #5. Time: 14.0s. Train loss: 833.718, train mse: 28.86435. Val loss: 1665.452, val mse: 40.42297\n",
      "Epoch #6. Time: 14.0s. Train loss: 868.730, train mse: 29.46259. Val loss: 725.181, val mse: 27.16990\n",
      "Epoch #7. Time: 14.0s. Train loss: 786.299, train mse: 28.04363. Val loss: 1210.208, val mse: 34.70877\n",
      "Epoch #8. Time: 14.0s. Train loss: 860.153, train mse: 29.31049. Val loss: 596.258, val mse: 24.63325\n",
      "Epoch #9. Time: 14.0s. Train loss: 689.401, train mse: 26.29772. Val loss: 556.214, val mse: 23.48471\n",
      "Epoch #10. Time: 14.0s. Train loss: 636.473, train mse: 25.21611. Val loss: 620.910, val mse: 24.53594\n",
      "Epoch #11. Time: 13.9s. Train loss: 583.710, train mse: 24.17638. Val loss: 500.595, val mse: 22.66731\n",
      "Epoch #12. Time: 14.0s. Train loss: 483.137, train mse: 21.99783. Val loss: 579.771, val mse: 24.17168\n",
      "Epoch #13. Time: 14.0s. Train loss: 450.156, train mse: 21.21363. Val loss: 500.558, val mse: 22.41853\n",
      "Epoch #14. Time: 14.0s. Train loss: 459.728, train mse: 21.40457. Val loss: 442.458, val mse: 21.14414\n",
      "Epoch #15. Time: 14.0s. Train loss: 395.782, train mse: 19.91360. Val loss: 502.493, val mse: 22.07155\n",
      "Epoch #16. Time: 14.0s. Train loss: 358.351, train mse: 18.93655. Val loss: 396.679, val mse: 19.96848\n",
      "Epoch #17. Time: 14.0s. Train loss: 329.499, train mse: 18.16183. Val loss: 441.164, val mse: 20.94806\n",
      "Epoch #18. Time: 14.0s. Train loss: 350.694, train mse: 18.76264. Val loss: 410.124, val mse: 20.52244\n",
      "Epoch #19. Time: 14.0s. Train loss: 315.164, train mse: 17.77127. Val loss: 413.511, val mse: 20.11257\n",
      "Epoch #20. Time: 14.0s. Train loss: 295.658, train mse: 17.15783. Val loss: 439.077, val mse: 20.45756\n",
      "Epoch #21. Time: 14.0s. Train loss: 312.052, train mse: 17.68350. Val loss: 444.482, val mse: 20.80155\n",
      "Epoch #22. Time: 14.0s. Train loss: 363.335, train mse: 19.07814. Val loss: 452.066, val mse: 20.88160\n",
      "Epoch #23. Time: 14.0s. Train loss: 343.770, train mse: 18.53152. Val loss: 464.173, val mse: 21.66250\n",
      "Epoch #24. Time: 14.0s. Train loss: 410.010, train mse: 20.25268. Val loss: 498.097, val mse: 22.57300\n",
      "Epoch #25. Time: 14.0s. Train loss: 434.689, train mse: 20.80946. Val loss: 535.521, val mse: 23.21676\n",
      "Epoch #26. Time: 14.0s. Train loss: 454.170, train mse: 21.28891. Val loss: 554.615, val mse: 23.21673\n",
      "Epoch #27. Time: 14.0s. Train loss: 405.424, train mse: 20.10318. Val loss: 653.991, val mse: 25.33261\n",
      "Epoch #28. Time: 14.0s. Train loss: 435.935, train mse: 20.87853. Val loss: 495.702, val mse: 22.10547\n",
      "Epoch #29. Time: 14.0s. Train loss: 360.555, train mse: 18.95304. Val loss: 520.261, val mse: 23.03185\n",
      "Epoch #30. Time: 14.0s. Train loss: 338.416, train mse: 18.38704. Val loss: 576.587, val mse: 24.01688\n",
      "Epoch #31. Time: 14.0s. Train loss: 329.855, train mse: 18.17547. Val loss: 701.513, val mse: 26.63496\n",
      "Epoch #32. Time: 14.0s. Train loss: 271.765, train mse: 16.49361. Val loss: 481.002, val mse: 22.03838\n",
      "Epoch #33. Time: 14.0s. Train loss: 217.422, train mse: 14.72224. Val loss: 530.776, val mse: 23.35331\n",
      "Epoch #34. Time: 14.0s. Train loss: 212.961, train mse: 14.54823. Val loss: 448.373, val mse: 21.42611\n",
      "Epoch #35. Time: 14.0s. Train loss: 193.836, train mse: 13.90808. Val loss: 546.132, val mse: 23.19519\n",
      "Epoch #36. Time: 14.0s. Train loss: 165.949, train mse: 12.84995. Val loss: 470.675, val mse: 21.93109\n",
      "Epoch #37. Time: 14.0s. Train loss: 154.175, train mse: 12.42201. Val loss: 459.050, val mse: 21.37717\n",
      "Epoch #38. Time: 14.0s. Train loss: 150.796, train mse: 12.26842. Val loss: 460.843, val mse: 21.41968\n",
      "Epoch #39. Time: 14.0s. Train loss: 152.668, train mse: 12.32562. Val loss: 442.233, val mse: 21.35803\n",
      "Epoch #40. Time: 14.0s. Train loss: 126.595, train mse: 11.26889. Val loss: 498.882, val mse: 22.29474\n",
      "Training done. Best rmse: 19.96848487854004\n",
      "Start #6 fold\n",
      "Epoch #1. Time: 14.0s. Train loss: 2964.478, train mse: 54.57451. Val loss: 924.605, val mse: 30.50996\n",
      "Epoch #2. Time: 14.0s. Train loss: 976.136, train mse: 31.23206. Val loss: 723.841, val mse: 27.25596\n",
      "Epoch #3. Time: 14.0s. Train loss: 900.672, train mse: 30.07247. Val loss: 940.456, val mse: 30.84095\n",
      "Epoch #4. Time: 14.0s. Train loss: 882.390, train mse: 29.73263. Val loss: 705.098, val mse: 26.83163\n",
      "Epoch #5. Time: 14.0s. Train loss: 829.138, train mse: 28.72596. Val loss: 691.526, val mse: 26.49784\n",
      "Epoch #6. Time: 14.0s. Train loss: 822.399, train mse: 28.69541. Val loss: 1116.670, val mse: 33.46815\n",
      "Epoch #7. Time: 14.0s. Train loss: 794.368, train mse: 28.05841. Val loss: 746.047, val mse: 27.47024\n",
      "Epoch #8. Time: 14.0s. Train loss: 691.564, train mse: 26.27983. Val loss: 718.148, val mse: 27.07963\n",
      "Epoch #9. Time: 14.0s. Train loss: 658.533, train mse: 25.55954. Val loss: 634.956, val mse: 25.04725\n",
      "Epoch #10. Time: 14.0s. Train loss: 565.545, train mse: 23.67949. Val loss: 545.987, val mse: 23.75635\n",
      "Epoch #11. Time: 14.0s. Train loss: 640.330, train mse: 25.33278. Val loss: 682.836, val mse: 26.29716\n",
      "Epoch #12. Time: 14.0s. Train loss: 539.695, train mse: 23.22624. Val loss: 528.469, val mse: 23.24545\n",
      "Epoch #13. Time: 14.0s. Train loss: 445.103, train mse: 21.08081. Val loss: 614.438, val mse: 24.36613\n",
      "Epoch #14. Time: 14.0s. Train loss: 403.678, train mse: 20.05440. Val loss: 499.973, val mse: 22.58337\n",
      "Epoch #15. Time: 14.0s. Train loss: 379.332, train mse: 19.49210. Val loss: 535.935, val mse: 23.43497\n",
      "Epoch #16. Time: 14.0s. Train loss: 341.953, train mse: 18.49335. Val loss: 556.262, val mse: 23.19241\n",
      "Epoch #17. Time: 14.0s. Train loss: 298.594, train mse: 17.30610. Val loss: 569.751, val mse: 23.71718\n",
      "Epoch #18. Time: 14.0s. Train loss: 297.581, train mse: 17.23886. Val loss: 495.809, val mse: 22.28356\n",
      "Epoch #19. Time: 14.0s. Train loss: 291.599, train mse: 17.03647. Val loss: 525.088, val mse: 22.77496\n",
      "Epoch #20. Time: 14.0s. Train loss: 282.514, train mse: 16.80235. Val loss: 522.940, val mse: 23.18075\n",
      "Epoch #21. Time: 14.0s. Train loss: 323.108, train mse: 17.99358. Val loss: 593.361, val mse: 24.51283\n",
      "Epoch #22. Time: 14.0s. Train loss: 344.987, train mse: 18.50436. Val loss: 735.787, val mse: 27.31399\n",
      "Epoch #23. Time: 14.0s. Train loss: 361.841, train mse: 19.03990. Val loss: 521.303, val mse: 22.66460\n",
      "Epoch #24. Time: 14.0s. Train loss: 432.079, train mse: 20.79623. Val loss: 699.709, val mse: 26.88761\n",
      "Epoch #25. Time: 14.0s. Train loss: 480.174, train mse: 21.95578. Val loss: 554.371, val mse: 23.80234\n",
      "Epoch #26. Time: 14.0s. Train loss: 409.780, train mse: 20.23609. Val loss: 553.812, val mse: 23.89448\n",
      "Epoch #27. Time: 14.0s. Train loss: 375.301, train mse: 19.27722. Val loss: 609.838, val mse: 25.10293\n",
      "Epoch #28. Time: 13.9s. Train loss: 348.340, train mse: 18.68557. Val loss: 618.476, val mse: 25.28964\n",
      "Epoch #29. Time: 14.0s. Train loss: 390.489, train mse: 19.75274. Val loss: 514.039, val mse: 23.14114\n",
      "Epoch #30. Time: 14.0s. Train loss: 377.890, train mse: 19.45819. Val loss: 545.757, val mse: 23.70925\n",
      "Epoch #31. Time: 14.0s. Train loss: 302.364, train mse: 17.35738. Val loss: 688.717, val mse: 26.59597\n",
      "Epoch #32. Time: 14.0s. Train loss: 257.921, train mse: 16.04729. Val loss: 588.315, val mse: 24.39911\n",
      "Epoch #33. Time: 14.0s. Train loss: 198.916, train mse: 14.13575. Val loss: 592.108, val mse: 24.43214\n",
      "Epoch #34. Time: 14.0s. Train loss: 191.524, train mse: 13.87174. Val loss: 589.411, val mse: 24.45432\n",
      "Epoch #35. Time: 14.0s. Train loss: 171.321, train mse: 13.10619. Val loss: 600.601, val mse: 24.13566\n",
      "Epoch #36. Time: 14.0s. Train loss: 166.934, train mse: 12.93857. Val loss: 583.016, val mse: 24.31289\n",
      "Epoch #37. Time: 14.0s. Train loss: 151.307, train mse: 12.31808. Val loss: 563.811, val mse: 23.55894\n",
      "Epoch #38. Time: 14.0s. Train loss: 134.006, train mse: 11.57271. Val loss: 542.094, val mse: 23.55382\n",
      "Epoch #39. Time: 14.0s. Train loss: 139.118, train mse: 11.73406. Val loss: 537.630, val mse: 22.75112\n",
      "Epoch #40. Time: 14.0s. Train loss: 132.865, train mse: 11.52798. Val loss: 579.449, val mse: 23.51130\n",
      "Training done. Best rmse: 22.2835636138916\n",
      "Start #7 fold\n",
      "Epoch #1. Time: 14.0s. Train loss: 2939.949, train mse: 54.34793. Val loss: 974.735, val mse: 31.30944\n",
      "Epoch #2. Time: 14.0s. Train loss: 965.024, train mse: 31.05604. Val loss: 810.598, val mse: 28.61322\n",
      "Epoch #3. Time: 14.0s. Train loss: 883.381, train mse: 29.75386. Val loss: 1350.057, val mse: 36.91317\n",
      "Epoch #4. Time: 14.0s. Train loss: 886.164, train mse: 29.81988. Val loss: 748.930, val mse: 27.37439\n",
      "Epoch #5. Time: 14.0s. Train loss: 835.030, train mse: 28.95019. Val loss: 862.829, val mse: 29.27217\n",
      "Epoch #6. Time: 14.0s. Train loss: 823.359, train mse: 28.71221. Val loss: 668.758, val mse: 25.82201\n",
      "Epoch #7. Time: 14.0s. Train loss: 829.745, train mse: 28.87099. Val loss: 636.393, val mse: 25.52223\n",
      "Epoch #8. Time: 14.0s. Train loss: 722.355, train mse: 26.83330. Val loss: 709.083, val mse: 26.38713\n",
      "Epoch #9. Time: 14.0s. Train loss: 665.160, train mse: 25.73620. Val loss: 834.436, val mse: 29.16282\n",
      "Epoch #10. Time: 14.0s. Train loss: 598.989, train mse: 24.46854. Val loss: 596.552, val mse: 24.19920\n",
      "Epoch #11. Time: 14.0s. Train loss: 560.035, train mse: 23.69221. Val loss: 517.776, val mse: 22.45265\n",
      "Epoch #12. Time: 14.1s. Train loss: 523.580, train mse: 22.87702. Val loss: 552.931, val mse: 23.47927\n",
      "Epoch #13. Time: 14.0s. Train loss: 487.295, train mse: 22.07995. Val loss: 461.275, val mse: 21.66984\n",
      "Epoch #14. Time: 14.0s. Train loss: 442.616, train mse: 21.05820. Val loss: 485.942, val mse: 22.31203\n",
      "Epoch #15. Time: 14.0s. Train loss: 393.392, train mse: 19.78577. Val loss: 425.988, val mse: 20.91730\n",
      "Epoch #16. Time: 14.1s. Train loss: 368.594, train mse: 19.19785. Val loss: 470.731, val mse: 21.66032\n",
      "Epoch #17. Time: 14.0s. Train loss: 323.822, train mse: 18.01149. Val loss: 461.483, val mse: 21.61110\n",
      "Epoch #18. Time: 14.0s. Train loss: 327.871, train mse: 18.12767. Val loss: 473.098, val mse: 21.20436\n",
      "Epoch #19. Time: 14.0s. Train loss: 291.423, train mse: 17.10692. Val loss: 397.147, val mse: 20.30313\n",
      "Epoch #20. Time: 14.0s. Train loss: 313.263, train mse: 17.72815. Val loss: 417.938, val mse: 20.54235\n",
      "Epoch #21. Time: 14.0s. Train loss: 303.092, train mse: 17.29878. Val loss: 471.769, val mse: 21.78003\n",
      "Epoch #22. Time: 14.0s. Train loss: 353.388, train mse: 18.80991. Val loss: 497.463, val mse: 22.64828\n",
      "Epoch #23. Time: 14.0s. Train loss: 385.428, train mse: 19.59068. Val loss: 942.144, val mse: 30.55027\n",
      "Epoch #24. Time: 14.0s. Train loss: 397.278, train mse: 19.89926. Val loss: 441.928, val mse: 21.15397\n",
      "Epoch #25. Time: 14.0s. Train loss: 493.857, train mse: 22.22593. Val loss: 895.898, val mse: 29.85810\n",
      "Epoch #26. Time: 14.0s. Train loss: 436.453, train mse: 20.89853. Val loss: 692.096, val mse: 26.34746\n",
      "Epoch #27. Time: 14.0s. Train loss: 382.432, train mse: 19.57498. Val loss: 539.450, val mse: 23.51527\n",
      "Epoch #28. Time: 14.0s. Train loss: 356.045, train mse: 18.90802. Val loss: 537.291, val mse: 23.33528\n",
      "Epoch #29. Time: 14.0s. Train loss: 322.319, train mse: 17.95941. Val loss: 489.430, val mse: 22.12929\n",
      "Epoch #30. Time: 14.0s. Train loss: 308.065, train mse: 17.57813. Val loss: 443.662, val mse: 21.33686\n",
      "Epoch #31. Time: 14.0s. Train loss: 302.193, train mse: 17.35850. Val loss: 438.270, val mse: 21.08728\n",
      "Epoch #32. Time: 13.9s. Train loss: 267.784, train mse: 16.37338. Val loss: 472.678, val mse: 21.49096\n",
      "Epoch #33. Time: 14.0s. Train loss: 222.299, train mse: 14.92636. Val loss: 416.680, val mse: 20.69780\n",
      "Epoch #34. Time: 14.0s. Train loss: 199.897, train mse: 14.14746. Val loss: 448.527, val mse: 21.44426\n",
      "Epoch #35. Time: 13.9s. Train loss: 183.651, train mse: 13.57333. Val loss: 506.147, val mse: 22.54700\n",
      "Epoch #36. Time: 14.0s. Train loss: 175.178, train mse: 13.10698. Val loss: 492.211, val mse: 22.14442\n",
      "Epoch #37. Time: 14.0s. Train loss: 145.979, train mse: 12.11042. Val loss: 469.186, val mse: 21.58335\n",
      "Epoch #38. Time: 14.0s. Train loss: 133.713, train mse: 11.54916. Val loss: 462.971, val mse: 21.65938\n",
      "Epoch #39. Time: 13.9s. Train loss: 155.953, train mse: 12.49862. Val loss: 506.769, val mse: 22.78273\n",
      "Epoch #40. Time: 14.0s. Train loss: 138.422, train mse: 11.78135. Val loss: 439.742, val mse: 21.02669\n",
      "Training done. Best rmse: 20.303125381469727\n",
      "Start #8 fold\n",
      "Epoch #1. Time: 13.9s. Train loss: 2880.351, train mse: 53.78491. Val loss: 832.417, val mse: 28.71881\n",
      "Epoch #2. Time: 14.0s. Train loss: 950.982, train mse: 30.88608. Val loss: 666.909, val mse: 26.12980\n",
      "Epoch #3. Time: 14.0s. Train loss: 918.573, train mse: 30.20998. Val loss: 1123.134, val mse: 33.17516\n",
      "Epoch #4. Time: 14.0s. Train loss: 952.367, train mse: 30.89268. Val loss: 768.062, val mse: 27.62026\n",
      "Epoch #5. Time: 14.0s. Train loss: 954.709, train mse: 30.88575. Val loss: 760.886, val mse: 27.50702\n",
      "Epoch #6. Time: 14.0s. Train loss: 829.163, train mse: 28.77017. Val loss: 848.775, val mse: 29.03040\n",
      "Epoch #7. Time: 14.0s. Train loss: 790.589, train mse: 28.09156. Val loss: 551.707, val mse: 23.67098\n",
      "Epoch #8. Time: 14.0s. Train loss: 716.325, train mse: 26.81624. Val loss: 516.681, val mse: 23.11540\n",
      "Epoch #9. Time: 13.9s. Train loss: 679.198, train mse: 26.09469. Val loss: 479.440, val mse: 22.02290\n",
      "Epoch #10. Time: 13.9s. Train loss: 626.048, train mse: 25.03589. Val loss: 474.156, val mse: 21.97591\n",
      "Epoch #11. Time: 13.9s. Train loss: 550.557, train mse: 23.45345. Val loss: 428.283, val mse: 20.73333\n",
      "Epoch #12. Time: 14.0s. Train loss: 547.829, train mse: 23.43024. Val loss: 432.086, val mse: 21.05949\n",
      "Epoch #13. Time: 14.0s. Train loss: 462.439, train mse: 21.50986. Val loss: 610.233, val mse: 24.77285\n",
      "Epoch #14. Time: 14.0s. Train loss: 445.258, train mse: 21.08466. Val loss: 495.772, val mse: 21.93893\n",
      "Epoch #15. Time: 13.9s. Train loss: 386.787, train mse: 19.65519. Val loss: 481.575, val mse: 22.11787\n",
      "Epoch #16. Time: 13.9s. Train loss: 323.184, train mse: 18.01444. Val loss: 479.694, val mse: 21.99719\n",
      "Epoch #17. Time: 14.0s. Train loss: 348.183, train mse: 18.65314. Val loss: 504.088, val mse: 22.29194\n",
      "Epoch #18. Time: 14.0s. Train loss: 324.741, train mse: 18.04844. Val loss: 486.954, val mse: 21.91050\n",
      "Epoch #19. Time: 13.9s. Train loss: 277.026, train mse: 16.67521. Val loss: 429.960, val mse: 20.78820\n",
      "Epoch #20. Time: 14.0s. Train loss: 289.597, train mse: 17.03263. Val loss: 485.525, val mse: 21.99735\n",
      "Epoch #21. Time: 14.0s. Train loss: 313.035, train mse: 17.71581. Val loss: 477.827, val mse: 22.14001\n",
      "Epoch #22. Time: 14.0s. Train loss: 341.597, train mse: 18.42132. Val loss: 491.777, val mse: 22.33880\n",
      "Epoch #23. Time: 14.0s. Train loss: 388.455, train mse: 19.67207. Val loss: 453.331, val mse: 21.56264\n",
      "Epoch #24. Time: 14.0s. Train loss: 445.179, train mse: 21.13508. Val loss: 512.617, val mse: 22.68414\n",
      "Epoch #25. Time: 14.0s. Train loss: 500.302, train mse: 22.35651. Val loss: 523.482, val mse: 22.98089\n",
      "Epoch #26. Time: 14.0s. Train loss: 464.882, train mse: 21.54008. Val loss: 525.820, val mse: 22.99862\n",
      "Epoch #27. Time: 14.0s. Train loss: 450.682, train mse: 21.03371. Val loss: 491.414, val mse: 22.44469\n",
      "Epoch #28. Time: 14.0s. Train loss: 381.281, train mse: 19.53108. Val loss: 488.338, val mse: 22.22722\n",
      "Epoch #29. Time: 14.0s. Train loss: 349.486, train mse: 18.71680. Val loss: 622.701, val mse: 25.02974\n",
      "Epoch #30. Time: 14.0s. Train loss: 304.534, train mse: 17.45782. Val loss: 538.288, val mse: 22.90396\n",
      "Epoch #31. Time: 14.0s. Train loss: 310.374, train mse: 17.62617. Val loss: 458.667, val mse: 21.48025\n",
      "Epoch #32. Time: 14.0s. Train loss: 253.711, train mse: 15.79839. Val loss: 608.706, val mse: 24.65043\n",
      "Epoch #33. Time: 14.0s. Train loss: 251.479, train mse: 15.85656. Val loss: 431.465, val mse: 20.88885\n",
      "Epoch #34. Time: 13.9s. Train loss: 202.797, train mse: 14.22154. Val loss: 423.066, val mse: 20.23451\n",
      "Epoch #35. Time: 14.0s. Train loss: 207.862, train mse: 14.37758. Val loss: 387.368, val mse: 20.02837\n",
      "Epoch #36. Time: 14.0s. Train loss: 177.947, train mse: 13.34629. Val loss: 468.007, val mse: 21.75485\n",
      "Epoch #37. Time: 14.0s. Train loss: 151.853, train mse: 12.29892. Val loss: 377.543, val mse: 19.69085\n",
      "Epoch #38. Time: 14.0s. Train loss: 160.391, train mse: 12.62770. Val loss: 419.681, val mse: 19.92550\n",
      "Epoch #39. Time: 14.0s. Train loss: 152.043, train mse: 12.34380. Val loss: 393.409, val mse: 20.17049\n",
      "Epoch #40. Time: 14.0s. Train loss: 139.627, train mse: 11.83408. Val loss: 394.491, val mse: 19.93367\n",
      "Training done. Best rmse: 19.690853118896484\n",
      "Start #9 fold\n",
      "Epoch #1. Time: 14.0s. Train loss: 2760.989, train mse: 52.69695. Val loss: 1078.477, val mse: 33.12315\n",
      "Epoch #2. Time: 14.0s. Train loss: 919.357, train mse: 30.30286. Val loss: 1128.056, val mse: 34.18431\n",
      "Epoch #3. Time: 14.0s. Train loss: 1001.282, train mse: 31.56997. Val loss: 1022.268, val mse: 32.18498\n",
      "Epoch #4. Time: 14.0s. Train loss: 940.183, train mse: 30.62929. Val loss: 973.879, val mse: 31.69403\n",
      "Epoch #5. Time: 14.0s. Train loss: 896.672, train mse: 29.94073. Val loss: 1020.701, val mse: 32.08388\n",
      "Epoch #6. Time: 14.0s. Train loss: 826.728, train mse: 28.76810. Val loss: 798.194, val mse: 28.64935\n",
      "Epoch #7. Time: 13.9s. Train loss: 729.116, train mse: 26.93817. Val loss: 946.311, val mse: 30.52251\n",
      "Epoch #8. Time: 14.0s. Train loss: 669.440, train mse: 25.82305. Val loss: 750.043, val mse: 27.66049\n",
      "Epoch #9. Time: 14.0s. Train loss: 658.068, train mse: 25.55186. Val loss: 901.836, val mse: 30.43455\n",
      "Epoch #10. Time: 14.0s. Train loss: 556.355, train mse: 23.61100. Val loss: 733.234, val mse: 26.05256\n",
      "Epoch #11. Time: 14.0s. Train loss: 574.368, train mse: 23.95510. Val loss: 691.438, val mse: 26.46432\n",
      "Epoch #12. Time: 14.0s. Train loss: 541.729, train mse: 23.32382. Val loss: 583.152, val mse: 24.31857\n",
      "Epoch #13. Time: 14.0s. Train loss: 463.551, train mse: 21.51063. Val loss: 684.978, val mse: 26.22548\n",
      "Epoch #14. Time: 14.0s. Train loss: 444.200, train mse: 21.00520. Val loss: 648.536, val mse: 25.50941\n",
      "Epoch #15. Time: 14.0s. Train loss: 372.140, train mse: 19.26906. Val loss: 703.720, val mse: 25.98528\n",
      "Epoch #16. Time: 14.0s. Train loss: 343.866, train mse: 18.57475. Val loss: 666.033, val mse: 26.19577\n",
      "Epoch #17. Time: 14.0s. Train loss: 324.156, train mse: 18.00643. Val loss: 689.035, val mse: 26.28638\n",
      "Epoch #18. Time: 14.0s. Train loss: 309.378, train mse: 17.52731. Val loss: 679.729, val mse: 26.30634\n",
      "Epoch #19. Time: 14.0s. Train loss: 303.956, train mse: 17.45051. Val loss: 635.660, val mse: 25.30470\n",
      "Epoch #20. Time: 14.0s. Train loss: 311.413, train mse: 17.67454. Val loss: 674.180, val mse: 25.72618\n",
      "Epoch #21. Time: 14.0s. Train loss: 313.588, train mse: 17.61248. Val loss: 676.838, val mse: 26.35157\n",
      "Epoch #22. Time: 14.0s. Train loss: 391.692, train mse: 19.77521. Val loss: 681.542, val mse: 26.64821\n",
      "Epoch #23. Time: 14.0s. Train loss: 368.086, train mse: 19.19471. Val loss: 667.979, val mse: 25.97552\n",
      "Epoch #24. Time: 14.0s. Train loss: 413.621, train mse: 20.36163. Val loss: 688.510, val mse: 26.42157\n",
      "Epoch #25. Time: 14.0s. Train loss: 439.966, train mse: 20.92694. Val loss: 782.405, val mse: 28.05300\n",
      "Epoch #26. Time: 14.0s. Train loss: 371.016, train mse: 19.29182. Val loss: 762.458, val mse: 27.78885\n",
      "Epoch #27. Time: 14.0s. Train loss: 403.732, train mse: 20.11881. Val loss: 710.873, val mse: 26.63523\n",
      "Epoch #28. Time: 14.0s. Train loss: 382.954, train mse: 19.56628. Val loss: 886.627, val mse: 29.89112\n",
      "Epoch #29. Time: 14.0s. Train loss: 380.203, train mse: 19.51605. Val loss: 777.709, val mse: 27.37571\n",
      "Epoch #30. Time: 14.0s. Train loss: 306.500, train mse: 17.50887. Val loss: 703.255, val mse: 26.34077\n",
      "Epoch #31. Time: 14.0s. Train loss: 290.334, train mse: 17.05126. Val loss: 693.492, val mse: 26.38640\n",
      "Epoch #32. Time: 14.0s. Train loss: 237.528, train mse: 15.41265. Val loss: 739.216, val mse: 26.67635\n",
      "Epoch #33. Time: 14.0s. Train loss: 223.463, train mse: 14.94033. Val loss: 586.233, val mse: 24.33276\n",
      "Epoch #34. Time: 14.0s. Train loss: 200.048, train mse: 14.16596. Val loss: 644.187, val mse: 25.38586\n",
      "Epoch #35. Time: 14.0s. Train loss: 163.722, train mse: 12.80674. Val loss: 730.332, val mse: 27.20036\n",
      "Epoch #36. Time: 14.0s. Train loss: 171.204, train mse: 13.09015. Val loss: 712.513, val mse: 26.86768\n",
      "Epoch #37. Time: 14.0s. Train loss: 147.815, train mse: 12.17877. Val loss: 666.171, val mse: 26.15909\n",
      "Epoch #38. Time: 14.0s. Train loss: 125.758, train mse: 11.23044. Val loss: 652.621, val mse: 25.67088\n",
      "Epoch #39. Time: 14.0s. Train loss: 139.320, train mse: 11.81331. Val loss: 661.962, val mse: 26.08223\n",
      "Epoch #40. Time: 14.0s. Train loss: 127.594, train mse: 11.25629. Val loss: 716.954, val mse: 26.12111\n",
      "Training done. Best rmse: 24.318565368652344\n",
      "Start #10 fold\n",
      "Epoch #1. Time: 15.3s. Train loss: 2899.372, train mse: 53.94423. Val loss: 995.643, val mse: 31.54610\n",
      "Epoch #2. Time: 14.0s. Train loss: 919.391, train mse: 30.34043. Val loss: 889.871, val mse: 30.10671\n",
      "Epoch #3. Time: 14.0s. Train loss: 936.430, train mse: 30.63953. Val loss: 835.601, val mse: 29.27060\n",
      "Epoch #4. Time: 14.0s. Train loss: 907.043, train mse: 30.11031. Val loss: 810.192, val mse: 28.96456\n",
      "Epoch #5. Time: 14.1s. Train loss: 932.929, train mse: 30.57326. Val loss: 743.597, val mse: 26.84644\n",
      "Epoch #6. Time: 14.0s. Train loss: 786.037, train mse: 28.05664. Val loss: 667.086, val mse: 25.75777\n",
      "Epoch #7. Time: 14.0s. Train loss: 705.098, train mse: 26.57139. Val loss: 1006.607, val mse: 32.40791\n",
      "Epoch #8. Time: 14.0s. Train loss: 725.863, train mse: 26.98064. Val loss: 893.083, val mse: 29.86614\n",
      "Epoch #9. Time: 14.0s. Train loss: 644.896, train mse: 25.42624. Val loss: 653.883, val mse: 25.93662\n",
      "Epoch #10. Time: 14.0s. Train loss: 603.776, train mse: 24.43894. Val loss: 767.231, val mse: 28.03994\n",
      "Epoch #11. Time: 14.0s. Train loss: 537.791, train mse: 23.18903. Val loss: 638.237, val mse: 25.36617\n",
      "Epoch #12. Time: 14.1s. Train loss: 497.287, train mse: 22.30472. Val loss: 1181.997, val mse: 34.47967\n",
      "Epoch #13. Time: 14.1s. Train loss: 459.715, train mse: 21.46156. Val loss: 776.091, val mse: 28.25852\n",
      "Epoch #14. Time: 14.0s. Train loss: 444.044, train mse: 21.09057. Val loss: 601.951, val mse: 24.35397\n",
      "Epoch #15. Time: 14.0s. Train loss: 365.485, train mse: 19.15670. Val loss: 632.971, val mse: 25.25829\n",
      "Epoch #16. Time: 14.0s. Train loss: 326.947, train mse: 18.09893. Val loss: 674.065, val mse: 25.81889\n",
      "Epoch #17. Time: 14.0s. Train loss: 287.364, train mse: 16.94071. Val loss: 655.570, val mse: 25.50517\n",
      "Epoch #18. Time: 14.1s. Train loss: 278.230, train mse: 16.64845. Val loss: 684.909, val mse: 26.50947\n",
      "Epoch #19. Time: 14.0s. Train loss: 260.285, train mse: 16.14278. Val loss: 671.552, val mse: 26.02990\n",
      "Epoch #20. Time: 14.1s. Train loss: 277.019, train mse: 16.65605. Val loss: 637.050, val mse: 25.53844\n",
      "Epoch #21. Time: 14.1s. Train loss: 292.434, train mse: 17.12539. Val loss: 680.857, val mse: 25.84169\n",
      "Epoch #22. Time: 14.1s. Train loss: 342.272, train mse: 18.51813. Val loss: 836.359, val mse: 28.69078\n",
      "Epoch #23. Time: 14.0s. Train loss: 366.375, train mse: 19.12601. Val loss: 730.006, val mse: 26.72979\n",
      "Epoch #24. Time: 14.0s. Train loss: 381.109, train mse: 19.53897. Val loss: 646.573, val mse: 25.49347\n",
      "Epoch #25. Time: 14.1s. Train loss: 531.415, train mse: 23.07725. Val loss: 663.490, val mse: 26.03555\n",
      "Epoch #26. Time: 14.0s. Train loss: 490.667, train mse: 22.14309. Val loss: 614.193, val mse: 24.85898\n",
      "Epoch #27. Time: 14.1s. Train loss: 451.223, train mse: 21.14100. Val loss: 767.937, val mse: 28.16635\n",
      "Epoch #28. Time: 14.0s. Train loss: 353.612, train mse: 18.81912. Val loss: 679.282, val mse: 26.02320\n",
      "Epoch #29. Time: 14.1s. Train loss: 328.246, train mse: 18.13430. Val loss: 605.313, val mse: 25.09528\n",
      "Epoch #30. Time: 14.1s. Train loss: 289.979, train mse: 16.98897. Val loss: 636.856, val mse: 25.23136\n",
      "Epoch #31. Time: 14.0s. Train loss: 254.366, train mse: 15.97259. Val loss: 610.935, val mse: 25.09251\n",
      "Epoch #32. Time: 14.0s. Train loss: 255.928, train mse: 16.02291. Val loss: 582.022, val mse: 23.97509\n",
      "Epoch #33. Time: 14.1s. Train loss: 226.201, train mse: 15.02667. Val loss: 590.039, val mse: 24.32088\n",
      "Epoch #34. Time: 14.1s. Train loss: 172.393, train mse: 13.14713. Val loss: 603.084, val mse: 24.65356\n",
      "Epoch #35. Time: 14.0s. Train loss: 170.240, train mse: 13.03548. Val loss: 598.633, val mse: 24.79191\n",
      "Epoch #36. Time: 14.1s. Train loss: 154.687, train mse: 12.44727. Val loss: 572.804, val mse: 23.38701\n",
      "Epoch #37. Time: 14.1s. Train loss: 148.642, train mse: 12.19721. Val loss: 549.154, val mse: 23.23640\n",
      "Epoch #38. Time: 14.1s. Train loss: 138.009, train mse: 11.74168. Val loss: 613.853, val mse: 25.21432\n",
      "Epoch #39. Time: 14.0s. Train loss: 144.111, train mse: 12.00621. Val loss: 601.645, val mse: 24.08520\n",
      "Epoch #40. Time: 14.0s. Train loss: 131.141, train mse: 11.44440. Val loss: 588.656, val mse: 23.97558\n",
      "Training done. Best rmse: 23.23639678955078\n",
      "Total time: 94.74885720411936m\n",
      "Total rmse: 21.477025985717773 +- 1.894540786743164\n",
      "[24.335094, 19.817755, 18.990114, 21.826307, 19.968485, 22.283564, 20.303125, 19.690853, 24.318565, 23.236397]\n"
     ]
    }
   ],
   "source": [
    "run_training() \n",
    "\n",
    "# Total time: 91.80613117615381m\n",
    "# Total rmse: 21.496326446533203 +- 1.9405601024627686\n",
    "# [23.648375, 20.050655, 19.122704, 22.243158, 19.753935, 21.988363, 20.367603, 19.324064, 24.734041, 23.730392]\n",
    "\n",
    "# \n",
    "# Total time: 94.74885720411936m\n",
    "# Total rmse: 21.477025985717773 +- 1.894540786743164\n",
    "# [24.335094, 19.817755, 18.990114, 21.826307, 19.968485, 22.283564, 20.303125, 19.690853, 24.318565, 23.236397]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6zkGHqsmN4S"
   },
   "outputs": [],
   "source": [
    "# model = pretrainedmodels.__dict__['se_resnext50_32x4d'](num_classes=1000, pretrained='imagenet')\n",
    "# model = EfficientNet.from_pretrained('efficientnet-b4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiV4c3cnQ68B"
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "# del model\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-D96Z-0RBNu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Exp1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
